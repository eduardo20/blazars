{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMxXxCxWFJG74Yx0+HdSdNG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "bae6925c343846c3a4edf182dc73bb28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "state": {
            "_view_name": "VBoxView",
            "_dom_classes": [
              "widget-interact"
            ],
            "_model_name": "VBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a31a2d8448e94b7c8af38e1a34baccf1",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_cd738612030b41cf920bf1b7b3b11d60",
              "IPY_MODEL_54d674fed1bf4666b3d367cd541a84c6",
              "IPY_MODEL_99c1c7bba875420a852f307325f4a38b",
              "IPY_MODEL_756cda88dbe74bbc99c21a86518c16e1",
              "IPY_MODEL_88e2788a8403443fbd01e53b5fa0452c",
              "IPY_MODEL_bb344409745f4a82a61d5eea9cf7e067",
              "IPY_MODEL_a95e7d0bb69747f3baabf227c14bc73e",
              "IPY_MODEL_9092ed29f0dd4980a285e6b56c9cd9a6"
            ]
          }
        },
        "a31a2d8448e94b7c8af38e1a34baccf1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cd738612030b41cf920bf1b7b3b11d60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextModel",
          "state": {
            "_view_name": "TextView",
            "style": "IPY_MODEL_36ae2da4cc9148648b83f59ccb438154",
            "_dom_classes": [],
            "description": "Learning_init",
            "_model_name": "TextModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "0.001",
            "_view_count": null,
            "disabled": false,
            "_view_module_version": "1.5.0",
            "continuous_update": true,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3385a4a8a8be4d45b1b4f9477c79dab1"
          }
        },
        "54d674fed1bf4666b3d367cd541a84c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "state": {
            "_options_labels": [
              "0.2",
              "0.25",
              "0.3",
              "0.35",
              "0.4"
            ],
            "_view_name": "DropdownView",
            "style": "IPY_MODEL_d38626b3dbc14e79a6cb16f8dfafade4",
            "_dom_classes": [],
            "description": "size_subset",
            "_model_name": "DropdownModel",
            "index": 2,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "disabled": false,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c908f34ab8d94d74820eeac346b3a088"
          }
        },
        "99c1c7bba875420a852f307325f4a38b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "state": {
            "_options_labels": [
              "identity",
              "logistic",
              "tanh",
              "relu"
            ],
            "_view_name": "DropdownView",
            "style": "IPY_MODEL_c869f1a4ad03422f953044b64a61c6b8",
            "_dom_classes": [],
            "description": "activation",
            "_model_name": "DropdownModel",
            "index": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "disabled": false,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_67805c8f36474fa992270a6e1a0cfdb0"
          }
        },
        "756cda88dbe74bbc99c21a86518c16e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "state": {
            "_options_labels": [
              "constant",
              "invscaling",
              "adaptive"
            ],
            "_view_name": "DropdownView",
            "style": "IPY_MODEL_66b8617af6a447abb06de65088e92ac1",
            "_dom_classes": [],
            "description": "Learning_rate",
            "_model_name": "DropdownModel",
            "index": 0,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "disabled": false,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_02bd7e86877b4e87a3c74aa9e4b2a6a9"
          }
        },
        "88e2788a8403443fbd01e53b5fa0452c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntSliderModel",
          "state": {
            "_view_name": "IntSliderView",
            "style": "IPY_MODEL_8d8c1808c74e450eb0146ac77927b404",
            "_dom_classes": [],
            "description": "max_iter",
            "step": 100,
            "_model_name": "IntSliderModel",
            "orientation": "horizontal",
            "max": 3000,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 800,
            "_view_count": null,
            "disabled": false,
            "_view_module_version": "1.5.0",
            "min": 200,
            "continuous_update": true,
            "readout_format": "d",
            "description_tooltip": null,
            "readout": true,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_dbcbb3b044584dae81ecb555bed1bdfb"
          }
        },
        "bb344409745f4a82a61d5eea9cf7e067": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "state": {
            "_view_name": "CheckboxView",
            "style": "IPY_MODEL_84a28b955235454b9bebe75c559755c2",
            "_dom_classes": [],
            "description": "Save_model",
            "_model_name": "CheckboxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": true,
            "_view_count": null,
            "disabled": false,
            "_view_module_version": "1.5.0",
            "indent": true,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_457adfd114864f1a828a16d57616da37"
          }
        },
        "a95e7d0bb69747f3baabf227c14bc73e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "state": {
            "_view_name": "ButtonView",
            "style": "IPY_MODEL_8b28f59c23284928bf7859689bd813b6",
            "_dom_classes": [],
            "description": "Run Interact",
            "_model_name": "ButtonModel",
            "button_style": "",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "tooltip": "",
            "_view_count": null,
            "disabled": false,
            "_view_module_version": "1.5.0",
            "layout": "IPY_MODEL_b1df82bcd0a14dcc92d88d068c22a58f",
            "_model_module": "@jupyter-widgets/controls",
            "icon": ""
          }
        },
        "9092ed29f0dd4980a285e6b56c9cd9a6": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "state": {
            "_view_name": "OutputView",
            "msg_id": "",
            "_dom_classes": [],
            "_model_name": "OutputModel",
            "outputs": [
              {
                "output_type": "stream",
                "metadata": {
                  "tags": []
                },
                "text": "Entrenando modelo\n",
                "stream": "stdout"
              },
              {
                "output_type": "stream",
                "metadata": {
                  "tags": []
                },
                "text": "test score:  0.9011627906976745\ntrain score:  0.9\nCross validation score with std desv\nPrediciendo escore con des est.\n",
                "stream": "stdout"
              },
              {
                "output_type": "stream",
                "metadata": {
                  "tags": []
                },
                "text": "Accuracy: 0.90 (+/- 0.07)\nPrediciendo resultados\n[[125  21]\n [ 13 185]]\n Salvando modelo\n",
                "stream": "stdout"
              },
              {
                "output_type": "display_data",
                "metadata": {
                  "tags": []
                },
                "text/plain": "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n              hidden_layer_sizes=(45, 90, 2), learning_rate='constant',\n              learning_rate_init=0.001, max_fun=15000, max_iter=800,\n              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n              power_t=0.5, random_state=21, shuffle=True, solver='sgd',\n              tol=1e-09, validation_fraction=0.1, verbose=10, warm_start=False)"
              }
            ],
            "_view_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_view_count": null,
            "_view_module_version": "1.0.0",
            "layout": "IPY_MODEL_03473443a81a4189a25ab9d0db7a3ca3",
            "_model_module": "@jupyter-widgets/output"
          }
        },
        "36ae2da4cc9148648b83f59ccb438154": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3385a4a8a8be4d45b1b4f9477c79dab1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d38626b3dbc14e79a6cb16f8dfafade4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c908f34ab8d94d74820eeac346b3a088": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c869f1a4ad03422f953044b64a61c6b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "67805c8f36474fa992270a6e1a0cfdb0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "66b8617af6a447abb06de65088e92ac1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "02bd7e86877b4e87a3c74aa9e4b2a6a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8d8c1808c74e450eb0146ac77927b404": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "SliderStyleModel",
          "state": {
            "_view_name": "StyleView",
            "handle_color": null,
            "_model_name": "SliderStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "dbcbb3b044584dae81ecb555bed1bdfb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "84a28b955235454b9bebe75c559755c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "457adfd114864f1a828a16d57616da37": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8b28f59c23284928bf7859689bd813b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ButtonStyleModel",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "button_color": null,
            "font_weight": "",
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b1df82bcd0a14dcc92d88d068c22a58f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eduardo20/blazars/blob/master/NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKWDq38gFwME",
        "colab_type": "text"
      },
      "source": [
        "#Idea juntar varios arboles\n",
        "https://scikit-learn.org/stable/auto_examples/ensemble/plot_feature_transformation.html#sphx-glr-auto-examples-ensemble-plot-feature-transformation-py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5ak0sgakinD",
        "colab_type": "code",
        "outputId": "c3eefa16-ce27-4ec7-a555-ed2e89356d84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N62iAS1d9iOh",
        "colab_type": "code",
        "outputId": "6bcb5e54-a8b9-4a36-a6db-94960f83a97a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        }
      },
      "source": [
        "!pip install ipynb\n",
        "#!pip install ipywidgets==7.4"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ipynb\n",
            "  Downloading https://files.pythonhosted.org/packages/31/42/4c0bbb66390e3a68e04ebf134c8d074a00c18b5882293f8ace5f7497fbf0/ipynb-0.5.1-py3-none-any.whl\n",
            "Installing collected packages: ipynb\n",
            "Successfully installed ipynb-0.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgVM59piktcE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import sys\n",
        "import os\n",
        "sys.path.append('/content/drive/My Drive/')\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib as plt\n",
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import ipynb.fs.full.my_functions_c as mf\n",
        "#import warnings\n",
        "#warnings.filterwarnings('ignore')\n",
        "#import ipynb.fs.full.my_functions\n",
        "#from ipynb.fs.full.my_functions import Snippet_188_a, Snippet_191,Snippet_192,Snippet_190, ROC_\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.tree import plot_tree\n",
        "from sklearn import tree as tr_\n",
        "import pydotplus\n",
        "import collections\n",
        "\n",
        "import ipywidgets as widgets\n",
        "#import load_clean\n",
        "\n",
        "#%run my_functions_c.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsvC6Zo_z4yi",
        "colab_type": "code",
        "outputId": "93c738dd-9f68-4def-ae6c-c145a7f81fa3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "%ls\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mdrive\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uq9fHtyaixLd",
        "colab_type": "text"
      },
      "source": [
        "# Lista de caracteristicas posibles del modelo, de entre ellas se prueban disitintas selecciones:\n",
        "features_=['ra', 'dec', 'flux_1_100_gev', 'spectral_index', 'spectral_index_error',\n",
        "          'detection_significance', 'lii', 'bii', 'pivot_energy', 'flux_density',\n",
        "          'flux_density_error', 'energy_flux', 'energy_flux_error',\n",
        "          'curve_significance', 'spectrum_type', 'powerlaw_index',\n",
        "          'flux_100_300_mev', 'flux_100_300_mev_pos_err',\n",
        "          'flux_100_300_mev_neg_err', 'nufnu_100_300_mev', 'sqrt_ts_100_300_mev',\n",
        "          'flux_0p3_1_gev', 'flux_0p3_1_gev_pos_err', 'flux_0p3_1_gev_neg_err',\n",
        "          'nufnu_0p3_1_gev', 'sqrt_ts_0p3_1_gev', 'flux_1_3_gev',\n",
        "          'flux_1_3_gev_pos_err', 'flux_1_3_gev_neg_err', 'nufnu_1_3_gev',\n",
        "          'sqrt_ts_1_3_gev', 'flux_3_10_gev', 'nufnu_3_10_gev',\n",
        "          'sqrt_ts_3_10_gev', 'flux_10_100_gev', 'nufnu_10_100_gev',\n",
        "          'sqrt_ts_10_100_gev', 'variability_index', 'significance_peak',\n",
        "          'flux_peak', 'flux_peak_error', 'time_peak', 'time_peak_interval',\n",
        "          'source_type', 'analysis_flags', 'HR12', 'HR23', 'HR34', 'hard_slope',\n",
        "          'soft_slope', 'P_E_lg', 'TS_', 'sig_', 'gamm_log', 'Ts_log', 'sig_log',\n",
        "          'F100_log']\n",
        "\n",
        "  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLFUQcsOkQRq",
        "colab_type": "text"
      },
      "source": [
        "# Trabajamos con 3 separciones poblacionales \n",
        "\n",
        "1.   pop1\n",
        "2.   pop2\n",
        "3.   full\n",
        "## La separaciÃ³n de estas poblaciones se puede encontrar en:\n",
        "https://colab.research.google.com/drive/1D4dCevS0IlVaqjwkLjBT4RaQilZmDatu#scrollTo=vuzPV4sqrstv\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1XMpEdNkte8",
        "colab_type": "code",
        "outputId": "6edfc8c0-1d68-4ff8-fbd9-f51e2fde1629",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        }
      },
      "source": [
        "features_=[ 'flux_1_100_gev', 'spectral_index', \n",
        "          'detection_significance', 'pivot_energy', 'flux_density',\n",
        "          'energy_flux',\n",
        "          'curve_significance', 'spectrum_type', 'powerlaw_index',\n",
        "          'flux_100_300_mev', 'nufnu_100_300_mev', 'sqrt_ts_100_300_mev',\n",
        "          'flux_0p3_1_gev', 'flux_0p3_1_gev_neg_err',\n",
        "          'nufnu_0p3_1_gev', 'sqrt_ts_0p3_1_gev', 'flux_1_3_gev',\n",
        "          'nufnu_1_3_gev','sqrt_ts_1_3_gev', 'flux_3_10_gev', 'nufnu_3_10_gev',\n",
        "          'sqrt_ts_3_10_gev', 'flux_10_100_gev', 'nufnu_10_100_gev',\n",
        "          'sqrt_ts_10_100_gev', 'variability_index', 'significance_peak',\n",
        "          'flux_peak', 'flux_peak_error', 'time_peak', 'time_peak_interval',\n",
        "          'source_type', 'analysis_flags', 'HR12', 'HR23', 'HR34', 'hard_slope',\n",
        "          'soft_slope', 'P_E_lg', 'TS_', 'sig_', 'gamm_log', 'Ts_log', 'sig_log',\n",
        "          'F100_log']\n",
        "     \n",
        "#----------------------------------------------------------------------#\n",
        "# Carga el dataframe de la poblacion, lo devuelve limpio, separadas las caracteristicas (X sin normalizar, X_std normalizadas) de las etiquetas (y_) mÃ¡s una copia completa del dataframe df0\n",
        "# Sintaxis:\n",
        "# X,y_,df0,X_std=load_clean(poblacion(int),features del modelo[lista]) \n",
        "X, y_, df0, X_std=mf.load_clean(3,features_)\n",
        "#----------------------------------------------------------------------#\n",
        "\n",
        "print (\"nÂº de caracterÃ­sticas del modelo :\",len(features_))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cargando full\n",
            "(1717, 57)\n",
            "nÂº de caracterÃ­sticas del modelo : 45\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYEO_uHo6mzq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "   \n",
        "def reduce_features (features_,df):\n",
        "  from sklearn.preprocessing import StandardScaler\n",
        "  new_df=df[features_]\n",
        "  #df.drop(['source_type'], axis=1, inplace=True)\n",
        "  sc = StandardScaler()\n",
        "  sc.fit(X)\n",
        "  #sc.fit(y)\n",
        "  X_std = sc.transform(X)\n",
        "  return new_df,X_std\n",
        "#X,X_std=reduce_features(features_,X) \n",
        "#df_UNK=pd.read_excel('/content/drive/My Drive/tree_UNK.xlsx')\n",
        "#df0.head(20)\n",
        "#!pip install eli5\n",
        "#from notebook.services.config import ConfigManager\n",
        "#cm = ConfigManager().update('notebook', {'limit_output': 10})\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZC3fFn1jKr6C",
        "colab_type": "code",
        "outputId": "03760b66-a465-4397-a9c6-aa3df6b34cdc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326,
          "referenced_widgets": [
            "bae6925c343846c3a4edf182dc73bb28",
            "a31a2d8448e94b7c8af38e1a34baccf1",
            "cd738612030b41cf920bf1b7b3b11d60",
            "54d674fed1bf4666b3d367cd541a84c6",
            "99c1c7bba875420a852f307325f4a38b",
            "756cda88dbe74bbc99c21a86518c16e1",
            "88e2788a8403443fbd01e53b5fa0452c",
            "bb344409745f4a82a61d5eea9cf7e067",
            "a95e7d0bb69747f3baabf227c14bc73e",
            "9092ed29f0dd4980a285e6b56c9cd9a6",
            "36ae2da4cc9148648b83f59ccb438154",
            "3385a4a8a8be4d45b1b4f9477c79dab1",
            "d38626b3dbc14e79a6cb16f8dfafade4",
            "c908f34ab8d94d74820eeac346b3a088",
            "c869f1a4ad03422f953044b64a61c6b8",
            "67805c8f36474fa992270a6e1a0cfdb0",
            "66b8617af6a447abb06de65088e92ac1",
            "02bd7e86877b4e87a3c74aa9e4b2a6a9",
            "8d8c1808c74e450eb0146ac77927b404",
            "dbcbb3b044584dae81ecb555bed1bdfb",
            "84a28b955235454b9bebe75c559755c2",
            "457adfd114864f1a828a16d57616da37",
            "8b28f59c23284928bf7859689bd813b6",
            "b1df82bcd0a14dcc92d88d068c22a58f",
            "03473443a81a4189a25ab9d0db7a3ca3"
          ]
        }
      },
      "source": [
        "#%%capture salida\n",
        "from notebook.services.config import ConfigManager\n",
        "cm = ConfigManager().update('notebook', {'limit_output': 10})\n",
        "from ipywidgets import  interact, widget  \n",
        "from ipywidgets import  interact_manual\n",
        "#from ipywidgets  import TwoByTwoLayout, AppLayout, GridspecLayout\n",
        "from ipywidgets import interactive,interact, HBox, Layout,VBox\n",
        "from IPython.utils import io\n",
        "\n",
        "\n",
        "from ipywidgets import IntSlider\n",
        "net=len(features_)\n",
        "#import eli5\n",
        "#from eli5.sklearn import PermutationImportance\n",
        "#size_subset = [0.2,0.25,0.3,0.35,0.4]\n",
        "#@interact\n",
        "@widgets.interact_manual(\n",
        "    color=['blue', 'red', 'green'], lw=(1., 10.))\n",
        "\n",
        "\n",
        "\n",
        "def echo(Learning_init='0.001', size_subset=[0.2, 0.25, 0.3,0.35, 0.4],activation=['identity', 'logistic', 'tanh', 'relu'],Learning_rate=['constant', 'invscaling', 'adaptive'],max_iter=IntSlider(min=200, max=3000, step=100),Save_model=False):\n",
        "\n",
        "  # hay que devolver X_std normalizado con el numero de caracteriscitcas reducidas usando reduce_features()\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X_std, y_, test_size=size_subset, random_state=27)\n",
        "  tree = MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
        "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
        "              hidden_layer_sizes=(net, 2*net, 2), learning_rate=Learning_rate,\n",
        "              learning_rate_init=np.float(Learning_init), max_fun=15000, max_iter=max_iter,\n",
        "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
        "              power_t=0.5, random_state=21, shuffle=True, solver='sgd',\n",
        "              tol=1e-09, validation_fraction=0.1, verbose=10, warm_start=False)\n",
        "  \n",
        "  #suprimir salida en pantalla del entrenamiento\n",
        "  print('Entrenando modelo')\n",
        "  with io.capture_output() as captured:    \n",
        "    \n",
        "    clf =tree.fit(X_train, y_train);\n",
        "  echo.widget.layout.flex_flow='row wrap'\n",
        "  # widget=interactive(echo,Save='Salvar archivo como...', size_subset=[0.2,0.25,0.3,0.35,0.4],loss=['entropy','gini'],split=100)\n",
        "  #controls = HBox(widget.children[:-1], layout = Layout(flex_flow='row wrap'))\n",
        "  #output = widget.children[-1]\n",
        "  #display(VBox([controls, output]))  \n",
        "\n",
        "\n",
        "  print(\"test score: \",tree.score(X_test, y_test))\n",
        "  print(\"train score: \",tree.score(X_train, y_train))\n",
        "  print('Cross validation score with std desv')\n",
        "  #suprimir salida en pantalla del entrenamiento\n",
        "  print('Prediciendo escore con des est.')\n",
        "  with io.capture_output() as captured:    \n",
        "    a,b=mf.Snippet_192(tree, X_test, y_test)\n",
        "  #ab=np.str(a)  \n",
        "  #suprimir salida en pantalla del entrenamiento\n",
        "  print(\"Accuracy: %0.2f (+/- %0.2f)\" % (a , b*2) )\n",
        "  print('Prediciendo resultados')\n",
        "  with io.capture_output() as captured: \n",
        "    y_pred = tree.predict(X_test);\n",
        "  print(confusion_matrix(y_test, y_pred))\n",
        "  #plt.figure(figsize=(30,15))\n",
        "  #plt.suptitle(\"Decision surface of a decision tree using paired features\")\n",
        "  #plt.legend(loc='lower right', borderpad=0, handletextpad=0)\n",
        "  #plt.axis(\"tight\")\n",
        " \n",
        "  if Save_model == True :\n",
        "    print(' Salvando modelo')\n",
        "    import pickle\n",
        "\n",
        "      #from google.colab import files\n",
        "      #files.download('your typical text file or what ever.txt')\n",
        "      # Create your model here (same as above)\n",
        "      #\n",
        "\n",
        "      # Save to file in the current working directory\n",
        "    model=tree\n",
        "    pkl_filename = \"/content/drive/My Drive/NN-pickle_model.pkl\"\n",
        "    with open(pkl_filename, 'wb') as file:\n",
        "          pickle.dump(model, file)\n",
        "\n",
        "\n",
        "  return tree\n",
        "  \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bae6925c343846c3a4edf182dc73bb28",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "interactive(children=(Text(value='0.001', description='Learning_init'), Dropdown(description='size_subset', opâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wjJUZ3cSyqX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#salida.outputs.reverse\n",
        "#salida.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWd5J5Mtktng",
        "colab_type": "code",
        "outputId": "30d6372d-93d3-4446-bae7-d1b4aabef990",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        }
      },
      "source": [
        "import pickle\n",
        "\n",
        "# Load from file\n",
        "pkl_filename = \"/content/drive/My Drive/NN-pickle_model.pkl\"\n",
        "with open(pkl_filename, 'rb') as file:\n",
        "    pickle_model = pickle.load(file)\n",
        "#df_UNK=pd.read_excel('/content/drive/My Drive/tree_UNK.xlsx')\n",
        "#Calculate the accuracy score and predict target values from recovered model\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_std, y_, test_size=0.3)\n",
        "#suprimir salida en pantalla del entrenamiento\n",
        "print('Usando modelo recuperado sobre X_test')\n",
        "with io.capture_output() as captured: \n",
        "  score = pickle_model.score(X_test, y_test)\n",
        "print('ok')\n",
        "print(\"Test score: {0:.2f} %\".format(100 * score))\n",
        "print('Prediciendo escore con des est.')\n",
        "with io.capture_output() as captured:    \n",
        "  a,b=mf.Snippet_192(pickle_model, X_test, y_test)\n",
        "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (a , b*2) )\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Usando modelo recuperado sobre X_test\n",
            "ok\n",
            "Test score: 90.41 %\n",
            "Prediciendo escore con des est.\n",
            "Accuracy: 0.87 (+/- 0.10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyKjd8iQktqv",
        "colab_type": "code",
        "outputId": "aa983c09-3e96-43f4-8036-c85154066963",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        }
      },
      "source": [
        "#mf.Snippet_192(pickle_model, X_test, y_test)\n",
        "# featrues -> NO EN ESTE CLASIFICADOR mf.Snippet_191(pickle_model, X_test, y_test)\n",
        "mf.Snippet_188(X_test, y_test,pickle_model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MLPClassifier\n",
            "********************Hoe to evaluate model with learning curves********************\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAGoCAYAAAATsnHAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRESkSv8PKvyPU3O3AEQAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgzMcUiQlI85",
        "colab_type": "code",
        "outputId": "4766ad34-aaf0-4ce1-fe1b-38f7d7211589",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "mf.Snippet_190(pickle_model, X_test, y_test, 5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*********************Hoe to visualise cross validation scores*********************\n",
            "Iteration 1, loss = 0.70522853\n",
            "Iteration 2, loss = 0.70457449\n",
            "Iteration 3, loss = 0.70373510\n",
            "Iteration 4, loss = 0.70267917\n",
            "Iteration 5, loss = 0.70145789\n",
            "Iteration 6, loss = 0.70014887\n",
            "Iteration 7, loss = 0.69881322\n",
            "Iteration 8, loss = 0.69745888\n",
            "Iteration 9, loss = 0.69609249\n",
            "Iteration 10, loss = 0.69495030\n",
            "Iteration 11, loss = 0.69380223\n",
            "Iteration 12, loss = 0.69277799\n",
            "Iteration 13, loss = 0.69180747\n",
            "Iteration 14, loss = 0.69090740\n",
            "Iteration 15, loss = 0.69017277\n",
            "Iteration 16, loss = 0.68959279\n",
            "Iteration 17, loss = 0.68915852\n",
            "Iteration 18, loss = 0.68876299\n",
            "Iteration 19, loss = 0.68844306\n",
            "Iteration 20, loss = 0.68817335\n",
            "Iteration 21, loss = 0.68791453\n",
            "Iteration 22, loss = 0.68767091\n",
            "Iteration 23, loss = 0.68747026\n",
            "Iteration 24, loss = 0.68735766\n",
            "Iteration 25, loss = 0.68726460\n",
            "Iteration 26, loss = 0.68718033\n",
            "Iteration 27, loss = 0.68710411\n",
            "Iteration 28, loss = 0.68703148\n",
            "Iteration 29, loss = 0.68695938\n",
            "Iteration 30, loss = 0.68688315\n",
            "Iteration 31, loss = 0.68680312\n",
            "Iteration 32, loss = 0.68671961\n",
            "Iteration 33, loss = 0.68663334\n",
            "Iteration 34, loss = 0.68654395\n",
            "Iteration 35, loss = 0.68645123\n",
            "Iteration 36, loss = 0.68635543\n",
            "Iteration 37, loss = 0.68625680\n",
            "Iteration 38, loss = 0.68615592\n",
            "Iteration 39, loss = 0.68605381\n",
            "Iteration 40, loss = 0.68594983\n",
            "Iteration 41, loss = 0.68584410\n",
            "Iteration 42, loss = 0.68573674\n",
            "Iteration 43, loss = 0.68562781\n",
            "Iteration 44, loss = 0.68551739\n",
            "Iteration 45, loss = 0.68540549\n",
            "Iteration 46, loss = 0.68529218\n",
            "Iteration 47, loss = 0.68517753\n",
            "Iteration 48, loss = 0.68506161\n",
            "Iteration 49, loss = 0.68494443\n",
            "Iteration 50, loss = 0.68482604\n",
            "Iteration 51, loss = 0.68470646\n",
            "Iteration 52, loss = 0.68458572\n",
            "Iteration 53, loss = 0.68446381\n",
            "Iteration 54, loss = 0.68434072\n",
            "Iteration 55, loss = 0.68421647\n",
            "Iteration 56, loss = 0.68409107\n",
            "Iteration 57, loss = 0.68396453\n",
            "Iteration 58, loss = 0.68383685\n",
            "Iteration 59, loss = 0.68370496\n",
            "Iteration 60, loss = 0.68357000\n",
            "Iteration 61, loss = 0.68343323\n",
            "Iteration 62, loss = 0.68329818\n",
            "Iteration 63, loss = 0.68316321\n",
            "Iteration 64, loss = 0.68302709\n",
            "Iteration 65, loss = 0.68288986\n",
            "Iteration 66, loss = 0.68275145\n",
            "Iteration 67, loss = 0.68261212\n",
            "Iteration 68, loss = 0.68247505\n",
            "Iteration 69, loss = 0.68233843\n",
            "Iteration 70, loss = 0.68220079\n",
            "Iteration 71, loss = 0.68206208\n",
            "Iteration 72, loss = 0.68192229\n",
            "Iteration 73, loss = 0.68178133\n",
            "Iteration 74, loss = 0.68163908\n",
            "Iteration 75, loss = 0.68149544\n",
            "Iteration 76, loss = 0.68134970\n",
            "Iteration 77, loss = 0.68120112\n",
            "Iteration 78, loss = 0.68105117\n",
            "Iteration 79, loss = 0.68089965\n",
            "Iteration 80, loss = 0.68074649\n",
            "Iteration 81, loss = 0.68059165\n",
            "Iteration 82, loss = 0.68043509\n",
            "Iteration 83, loss = 0.68027687\n",
            "Iteration 84, loss = 0.68011694\n",
            "Iteration 85, loss = 0.67995527\n",
            "Iteration 86, loss = 0.67979207\n",
            "Iteration 87, loss = 0.67962818\n",
            "Iteration 88, loss = 0.67946224\n",
            "Iteration 89, loss = 0.67929452\n",
            "Iteration 90, loss = 0.67912479\n",
            "Iteration 91, loss = 0.67895344\n",
            "Iteration 92, loss = 0.67878012\n",
            "Iteration 93, loss = 0.67860479\n",
            "Iteration 94, loss = 0.67842744\n",
            "Iteration 95, loss = 0.67824868\n",
            "Iteration 96, loss = 0.67806804\n",
            "Iteration 97, loss = 0.67788536\n",
            "Iteration 98, loss = 0.67770063\n",
            "Iteration 99, loss = 0.67751385\n",
            "Iteration 100, loss = 0.67732494\n",
            "Iteration 101, loss = 0.67713389\n",
            "Iteration 102, loss = 0.67694068\n",
            "Iteration 103, loss = 0.67674534\n",
            "Iteration 104, loss = 0.67654784\n",
            "Iteration 105, loss = 0.67634852\n",
            "Iteration 106, loss = 0.67614650\n",
            "Iteration 107, loss = 0.67594217\n",
            "Iteration 108, loss = 0.67573553\n",
            "Iteration 109, loss = 0.67552646\n",
            "Iteration 110, loss = 0.67531499\n",
            "Iteration 111, loss = 0.67510129\n",
            "Iteration 112, loss = 0.67488563\n",
            "Iteration 113, loss = 0.67466742\n",
            "Iteration 114, loss = 0.67444714\n",
            "Iteration 115, loss = 0.67422448\n",
            "Iteration 116, loss = 0.67399941\n",
            "Iteration 117, loss = 0.67377155\n",
            "Iteration 118, loss = 0.67354106\n",
            "Iteration 119, loss = 0.67330790\n",
            "Iteration 120, loss = 0.67307209\n",
            "Iteration 121, loss = 0.67283332\n",
            "Iteration 122, loss = 0.67259191\n",
            "Iteration 123, loss = 0.67234769\n",
            "Iteration 124, loss = 0.67210098\n",
            "Iteration 125, loss = 0.67185141\n",
            "Iteration 126, loss = 0.67159899\n",
            "Iteration 127, loss = 0.67134370\n",
            "Iteration 128, loss = 0.67108560\n",
            "Iteration 129, loss = 0.67082457\n",
            "Iteration 130, loss = 0.67056061\n",
            "Iteration 131, loss = 0.67029358\n",
            "Iteration 132, loss = 0.67002344\n",
            "Iteration 133, loss = 0.66975014\n",
            "Iteration 134, loss = 0.66947382\n",
            "Iteration 135, loss = 0.66919419\n",
            "Iteration 136, loss = 0.66891129\n",
            "Iteration 137, loss = 0.66862555\n",
            "Iteration 138, loss = 0.66833686\n",
            "Iteration 139, loss = 0.66804487\n",
            "Iteration 140, loss = 0.66774964\n",
            "Iteration 141, loss = 0.66745089\n",
            "Iteration 142, loss = 0.66714872\n",
            "Iteration 143, loss = 0.66684317\n",
            "Iteration 144, loss = 0.66653391\n",
            "Iteration 145, loss = 0.66622099\n",
            "Iteration 146, loss = 0.66590435\n",
            "Iteration 147, loss = 0.66558398\n",
            "Iteration 148, loss = 0.66525994\n",
            "Iteration 149, loss = 0.66493213\n",
            "Iteration 150, loss = 0.66460063\n",
            "Iteration 151, loss = 0.66426717\n",
            "Iteration 152, loss = 0.66393146\n",
            "Iteration 153, loss = 0.66359208\n",
            "Iteration 154, loss = 0.66324923\n",
            "Iteration 155, loss = 0.66290263\n",
            "Iteration 156, loss = 0.66255228\n",
            "Iteration 157, loss = 0.66219796\n",
            "Iteration 158, loss = 0.66183875\n",
            "Iteration 159, loss = 0.66147457\n",
            "Iteration 160, loss = 0.66110635\n",
            "Iteration 161, loss = 0.66073322\n",
            "Iteration 162, loss = 0.66035570\n",
            "Iteration 163, loss = 0.65997365\n",
            "Iteration 164, loss = 0.65958706\n",
            "Iteration 165, loss = 0.65919641\n",
            "Iteration 166, loss = 0.65880088\n",
            "Iteration 167, loss = 0.65840079\n",
            "Iteration 168, loss = 0.65799596\n",
            "Iteration 169, loss = 0.65758660\n",
            "Iteration 170, loss = 0.65717241\n",
            "Iteration 171, loss = 0.65675369\n",
            "Iteration 172, loss = 0.65633031\n",
            "Iteration 173, loss = 0.65590211\n",
            "Iteration 174, loss = 0.65546896\n",
            "Iteration 175, loss = 0.65503090\n",
            "Iteration 176, loss = 0.65458785\n",
            "Iteration 177, loss = 0.65414022\n",
            "Iteration 178, loss = 0.65368848\n",
            "Iteration 179, loss = 0.65323198\n",
            "Iteration 180, loss = 0.65277072\n",
            "Iteration 181, loss = 0.65230437\n",
            "Iteration 182, loss = 0.65183278\n",
            "Iteration 183, loss = 0.65135607\n",
            "Iteration 184, loss = 0.65087444\n",
            "Iteration 185, loss = 0.65038777\n",
            "Iteration 186, loss = 0.64989595\n",
            "Iteration 187, loss = 0.64939893\n",
            "Iteration 188, loss = 0.64889668\n",
            "Iteration 189, loss = 0.64838920\n",
            "Iteration 190, loss = 0.64787497\n",
            "Iteration 191, loss = 0.64735363\n",
            "Iteration 192, loss = 0.64682681\n",
            "Iteration 193, loss = 0.64629433\n",
            "Iteration 194, loss = 0.64575621\n",
            "Iteration 195, loss = 0.64521231\n",
            "Iteration 196, loss = 0.64466273\n",
            "Iteration 197, loss = 0.64410749\n",
            "Iteration 198, loss = 0.64354645\n",
            "Iteration 199, loss = 0.64297975\n",
            "Iteration 200, loss = 0.64240794\n",
            "Iteration 201, loss = 0.64183034\n",
            "Iteration 202, loss = 0.64124682\n",
            "Iteration 203, loss = 0.64065744\n",
            "Iteration 204, loss = 0.64006214\n",
            "Iteration 205, loss = 0.63946101\n",
            "Iteration 206, loss = 0.63885445\n",
            "Iteration 207, loss = 0.63824217\n",
            "Iteration 208, loss = 0.63762361\n",
            "Iteration 209, loss = 0.63699919\n",
            "Iteration 210, loss = 0.63636893\n",
            "Iteration 211, loss = 0.63573279\n",
            "Iteration 212, loss = 0.63509061\n",
            "Iteration 213, loss = 0.63444265\n",
            "Iteration 214, loss = 0.63379178\n",
            "Iteration 215, loss = 0.63313707\n",
            "Iteration 216, loss = 0.63247585\n",
            "Iteration 217, loss = 0.63180827\n",
            "Iteration 218, loss = 0.63113471\n",
            "Iteration 219, loss = 0.63045531\n",
            "Iteration 220, loss = 0.62977002\n",
            "Iteration 221, loss = 0.62907892\n",
            "Iteration 222, loss = 0.62838245\n",
            "Iteration 223, loss = 0.62768041\n",
            "Iteration 224, loss = 0.62696991\n",
            "Iteration 225, loss = 0.62625274\n",
            "Iteration 226, loss = 0.62553081\n",
            "Iteration 227, loss = 0.62480302\n",
            "Iteration 228, loss = 0.62406734\n",
            "Iteration 229, loss = 0.62332151\n",
            "Iteration 230, loss = 0.62256258\n",
            "Iteration 231, loss = 0.62179597\n",
            "Iteration 232, loss = 0.62101945\n",
            "Iteration 233, loss = 0.62023017\n",
            "Iteration 234, loss = 0.61943307\n",
            "Iteration 235, loss = 0.61862973\n",
            "Iteration 236, loss = 0.61782133\n",
            "Iteration 237, loss = 0.61700783\n",
            "Iteration 238, loss = 0.61618584\n",
            "Iteration 239, loss = 0.61535485\n",
            "Iteration 240, loss = 0.61451780\n",
            "Iteration 241, loss = 0.61367447\n",
            "Iteration 242, loss = 0.61282412\n",
            "Iteration 243, loss = 0.61196711\n",
            "Iteration 244, loss = 0.61110191\n",
            "Iteration 245, loss = 0.61022199\n",
            "Iteration 246, loss = 0.60932683\n",
            "Iteration 247, loss = 0.60842029\n",
            "Iteration 248, loss = 0.60750606\n",
            "Iteration 249, loss = 0.60657535\n",
            "Iteration 250, loss = 0.60563158\n",
            "Iteration 251, loss = 0.60467534\n",
            "Iteration 252, loss = 0.60369724\n",
            "Iteration 253, loss = 0.60270877\n",
            "Iteration 254, loss = 0.60170998\n",
            "Iteration 255, loss = 0.60070260\n",
            "Iteration 256, loss = 0.59968722\n",
            "Iteration 257, loss = 0.59866139\n",
            "Iteration 258, loss = 0.59762225\n",
            "Iteration 259, loss = 0.59656983\n",
            "Iteration 260, loss = 0.59550010\n",
            "Iteration 261, loss = 0.59442149\n",
            "Iteration 262, loss = 0.59334390\n",
            "Iteration 263, loss = 0.59225417\n",
            "Iteration 264, loss = 0.59116244\n",
            "Iteration 265, loss = 0.59007241\n",
            "Iteration 266, loss = 0.58896884\n",
            "Iteration 267, loss = 0.58785267\n",
            "Iteration 268, loss = 0.58673407\n",
            "Iteration 269, loss = 0.58560995\n",
            "Iteration 270, loss = 0.58448088\n",
            "Iteration 271, loss = 0.58334605\n",
            "Iteration 272, loss = 0.58219818\n",
            "Iteration 273, loss = 0.58103410\n",
            "Iteration 274, loss = 0.57985221\n",
            "Iteration 275, loss = 0.57864822\n",
            "Iteration 276, loss = 0.57743741\n",
            "Iteration 277, loss = 0.57622009\n",
            "Iteration 278, loss = 0.57497888\n",
            "Iteration 279, loss = 0.57371629\n",
            "Iteration 280, loss = 0.57242395\n",
            "Iteration 281, loss = 0.57110950\n",
            "Iteration 282, loss = 0.56976728\n",
            "Iteration 283, loss = 0.56841667\n",
            "Iteration 284, loss = 0.56703964\n",
            "Iteration 285, loss = 0.56561415\n",
            "Iteration 286, loss = 0.56417928\n",
            "Iteration 287, loss = 0.56273551\n",
            "Iteration 288, loss = 0.56128179\n",
            "Iteration 289, loss = 0.55979282\n",
            "Iteration 290, loss = 0.55831290\n",
            "Iteration 291, loss = 0.55680981\n",
            "Iteration 292, loss = 0.55528941\n",
            "Iteration 293, loss = 0.55377853\n",
            "Iteration 294, loss = 0.55225609\n",
            "Iteration 295, loss = 0.55072534\n",
            "Iteration 296, loss = 0.54920636\n",
            "Iteration 297, loss = 0.54768017\n",
            "Iteration 298, loss = 0.54612703\n",
            "Iteration 299, loss = 0.54454174\n",
            "Iteration 300, loss = 0.54289412\n",
            "Iteration 301, loss = 0.54120615\n",
            "Iteration 302, loss = 0.53949211\n",
            "Iteration 303, loss = 0.53773011\n",
            "Iteration 304, loss = 0.53594453\n",
            "Iteration 305, loss = 0.53415170\n",
            "Iteration 306, loss = 0.53234721\n",
            "Iteration 307, loss = 0.53052034\n",
            "Iteration 308, loss = 0.52865212\n",
            "Iteration 309, loss = 0.52667609\n",
            "Iteration 310, loss = 0.52464647\n",
            "Iteration 311, loss = 0.52255165\n",
            "Iteration 312, loss = 0.52039836\n",
            "Iteration 313, loss = 0.51821162\n",
            "Iteration 314, loss = 0.51601560\n",
            "Iteration 315, loss = 0.51383029\n",
            "Iteration 316, loss = 0.51154518\n",
            "Iteration 317, loss = 0.50912810\n",
            "Iteration 318, loss = 0.50667084\n",
            "Iteration 319, loss = 0.50411673\n",
            "Iteration 320, loss = 0.50136782\n",
            "Iteration 321, loss = 0.49857387\n",
            "Iteration 322, loss = 0.49574448\n",
            "Iteration 323, loss = 0.49291518\n",
            "Iteration 324, loss = 0.49010128\n",
            "Iteration 325, loss = 0.48730410\n",
            "Iteration 326, loss = 0.48452481\n",
            "Iteration 327, loss = 0.48176485\n",
            "Iteration 328, loss = 0.47898316\n",
            "Iteration 329, loss = 0.47622691\n",
            "Iteration 330, loss = 0.47349694\n",
            "Iteration 331, loss = 0.47080097\n",
            "Iteration 332, loss = 0.46811039\n",
            "Iteration 333, loss = 0.46543003\n",
            "Iteration 334, loss = 0.46278026\n",
            "Iteration 335, loss = 0.46016474\n",
            "Iteration 336, loss = 0.45758585\n",
            "Iteration 337, loss = 0.45503792\n",
            "Iteration 338, loss = 0.45246425\n",
            "Iteration 339, loss = 0.44992053\n",
            "Iteration 340, loss = 0.44741369\n",
            "Iteration 341, loss = 0.44495626\n",
            "Iteration 342, loss = 0.44249172\n",
            "Iteration 343, loss = 0.44004340\n",
            "Iteration 344, loss = 0.43763555\n",
            "Iteration 345, loss = 0.43525713\n",
            "Iteration 346, loss = 0.43290841\n",
            "Iteration 347, loss = 0.43058816\n",
            "Iteration 348, loss = 0.42829645\n",
            "Iteration 349, loss = 0.42603220\n",
            "Iteration 350, loss = 0.42379784\n",
            "Iteration 351, loss = 0.42159222\n",
            "Iteration 352, loss = 0.41941329\n",
            "Iteration 353, loss = 0.41726257\n",
            "Iteration 354, loss = 0.41514159\n",
            "Iteration 355, loss = 0.41304906\n",
            "Iteration 356, loss = 0.41098777\n",
            "Iteration 357, loss = 0.40895437\n",
            "Iteration 358, loss = 0.40694783\n",
            "Iteration 359, loss = 0.40496963\n",
            "Iteration 360, loss = 0.40301692\n",
            "Iteration 361, loss = 0.40108984\n",
            "Iteration 362, loss = 0.39918809\n",
            "Iteration 363, loss = 0.39731043\n",
            "Iteration 364, loss = 0.39545644\n",
            "Iteration 365, loss = 0.39362838\n",
            "Iteration 366, loss = 0.39182765\n",
            "Iteration 367, loss = 0.39005116\n",
            "Iteration 368, loss = 0.38829785\n",
            "Iteration 369, loss = 0.38657185\n",
            "Iteration 370, loss = 0.38486992\n",
            "Iteration 371, loss = 0.38319147\n",
            "Iteration 372, loss = 0.38153663\n",
            "Iteration 373, loss = 0.37990506\n",
            "Iteration 374, loss = 0.37829382\n",
            "Iteration 375, loss = 0.37670128\n",
            "Iteration 376, loss = 0.37513025\n",
            "Iteration 377, loss = 0.37358083\n",
            "Iteration 378, loss = 0.37205075\n",
            "Iteration 379, loss = 0.37053977\n",
            "Iteration 380, loss = 0.36904782\n",
            "Iteration 381, loss = 0.36757481\n",
            "Iteration 382, loss = 0.36612112\n",
            "Iteration 383, loss = 0.36468550\n",
            "Iteration 384, loss = 0.36326822\n",
            "Iteration 385, loss = 0.36186889\n",
            "Iteration 386, loss = 0.36048612\n",
            "Iteration 387, loss = 0.35912067\n",
            "Iteration 388, loss = 0.35777295\n",
            "Iteration 389, loss = 0.35644178\n",
            "Iteration 390, loss = 0.35512769\n",
            "Iteration 391, loss = 0.35382993\n",
            "Iteration 392, loss = 0.35254878\n",
            "Iteration 393, loss = 0.35128372\n",
            "Iteration 394, loss = 0.35003507\n",
            "Iteration 395, loss = 0.34880278\n",
            "Iteration 396, loss = 0.34758569\n",
            "Iteration 397, loss = 0.34638293\n",
            "Iteration 398, loss = 0.34519599\n",
            "Iteration 399, loss = 0.34402299\n",
            "Iteration 400, loss = 0.34286369\n",
            "Iteration 401, loss = 0.34171841\n",
            "Iteration 402, loss = 0.34058664\n",
            "Iteration 403, loss = 0.33946872\n",
            "Iteration 404, loss = 0.33836478\n",
            "Iteration 405, loss = 0.33727495\n",
            "Iteration 406, loss = 0.33619926\n",
            "Iteration 407, loss = 0.33513654\n",
            "Iteration 408, loss = 0.33408656\n",
            "Iteration 409, loss = 0.33304873\n",
            "Iteration 410, loss = 0.33202306\n",
            "Iteration 411, loss = 0.33100934\n",
            "Iteration 412, loss = 0.33000726\n",
            "Iteration 413, loss = 0.32901639\n",
            "Iteration 414, loss = 0.32803599\n",
            "Iteration 415, loss = 0.32706632\n",
            "Iteration 416, loss = 0.32610924\n",
            "Iteration 417, loss = 0.32516340\n",
            "Iteration 418, loss = 0.32422817\n",
            "Iteration 419, loss = 0.32330372\n",
            "Iteration 420, loss = 0.32238969\n",
            "Iteration 421, loss = 0.32148542\n",
            "Iteration 422, loss = 0.32059108\n",
            "Iteration 423, loss = 0.31970619\n",
            "Iteration 424, loss = 0.31883128\n",
            "Iteration 425, loss = 0.31796857\n",
            "Iteration 426, loss = 0.31711638\n",
            "Iteration 427, loss = 0.31627268\n",
            "Iteration 428, loss = 0.31543876\n",
            "Iteration 429, loss = 0.31461537\n",
            "Iteration 430, loss = 0.31380255\n",
            "Iteration 431, loss = 0.31299904\n",
            "Iteration 432, loss = 0.31220462\n",
            "Iteration 433, loss = 0.31141877\n",
            "Iteration 434, loss = 0.31064135\n",
            "Iteration 435, loss = 0.30987256\n",
            "Iteration 436, loss = 0.30911184\n",
            "Iteration 437, loss = 0.30835926\n",
            "Iteration 438, loss = 0.30761428\n",
            "Iteration 439, loss = 0.30687701\n",
            "Iteration 440, loss = 0.30614766\n",
            "Iteration 441, loss = 0.30542606\n",
            "Iteration 442, loss = 0.30471217\n",
            "Iteration 443, loss = 0.30400609\n",
            "Iteration 444, loss = 0.30330679\n",
            "Iteration 445, loss = 0.30261458\n",
            "Iteration 446, loss = 0.30192945\n",
            "Iteration 447, loss = 0.30125229\n",
            "Iteration 448, loss = 0.30058300\n",
            "Iteration 449, loss = 0.29992087\n",
            "Iteration 450, loss = 0.29926532\n",
            "Iteration 451, loss = 0.29861643\n",
            "Iteration 452, loss = 0.29797410\n",
            "Iteration 453, loss = 0.29733833\n",
            "Iteration 454, loss = 0.29670899\n",
            "Iteration 455, loss = 0.29608638\n",
            "Iteration 456, loss = 0.29547030\n",
            "Iteration 457, loss = 0.29485976\n",
            "Iteration 458, loss = 0.29425515\n",
            "Iteration 459, loss = 0.29365693\n",
            "Iteration 460, loss = 0.29306548\n",
            "Iteration 461, loss = 0.29248005\n",
            "Iteration 462, loss = 0.29189983\n",
            "Iteration 463, loss = 0.29132399\n",
            "Iteration 464, loss = 0.29075324\n",
            "Iteration 465, loss = 0.29018809\n",
            "Iteration 466, loss = 0.28962876\n",
            "Iteration 467, loss = 0.28907502\n",
            "Iteration 468, loss = 0.28852678\n",
            "Iteration 469, loss = 0.28798382\n",
            "Iteration 470, loss = 0.28744584\n",
            "Iteration 471, loss = 0.28691296\n",
            "Iteration 472, loss = 0.28638534\n",
            "Iteration 473, loss = 0.28586304\n",
            "Iteration 474, loss = 0.28534557\n",
            "Iteration 475, loss = 0.28483309\n",
            "Iteration 476, loss = 0.28432553\n",
            "Iteration 477, loss = 0.28382323\n",
            "Iteration 478, loss = 0.28332608\n",
            "Iteration 479, loss = 0.28283384\n",
            "Iteration 480, loss = 0.28234617\n",
            "Iteration 481, loss = 0.28186306\n",
            "Iteration 482, loss = 0.28138449\n",
            "Iteration 483, loss = 0.28091031\n",
            "Iteration 484, loss = 0.28044061\n",
            "Iteration 485, loss = 0.27997632\n",
            "Iteration 486, loss = 0.27951676\n",
            "Iteration 487, loss = 0.27906148\n",
            "Iteration 488, loss = 0.27861042\n",
            "Iteration 489, loss = 0.27816328\n",
            "Iteration 490, loss = 0.27772020\n",
            "Iteration 491, loss = 0.27728057\n",
            "Iteration 492, loss = 0.27684479\n",
            "Iteration 493, loss = 0.27641285\n",
            "Iteration 494, loss = 0.27598476\n",
            "Iteration 495, loss = 0.27556046\n",
            "Iteration 496, loss = 0.27513994\n",
            "Iteration 497, loss = 0.27472317\n",
            "Iteration 498, loss = 0.27430995\n",
            "Iteration 499, loss = 0.27389896\n",
            "Iteration 500, loss = 0.27349115\n",
            "Iteration 501, loss = 0.27308637\n",
            "Iteration 502, loss = 0.27268573\n",
            "Iteration 503, loss = 0.27228902\n",
            "Iteration 504, loss = 0.27189584\n",
            "Iteration 505, loss = 0.27150573\n",
            "Iteration 506, loss = 0.27111876\n",
            "Iteration 507, loss = 0.27073490\n",
            "Iteration 508, loss = 0.27035623\n",
            "Iteration 509, loss = 0.26998422\n",
            "Iteration 510, loss = 0.26961487\n",
            "Iteration 511, loss = 0.26924886\n",
            "Iteration 512, loss = 0.26888590\n",
            "Iteration 513, loss = 0.26852573\n",
            "Iteration 514, loss = 0.26816827\n",
            "Iteration 515, loss = 0.26781380\n",
            "Iteration 516, loss = 0.26746226\n",
            "Iteration 517, loss = 0.26711358\n",
            "Iteration 518, loss = 0.26676765\n",
            "Iteration 519, loss = 0.26642497\n",
            "Iteration 520, loss = 0.26608529\n",
            "Iteration 521, loss = 0.26574831\n",
            "Iteration 522, loss = 0.26541384\n",
            "Iteration 523, loss = 0.26508186\n",
            "Iteration 524, loss = 0.26475249\n",
            "Iteration 525, loss = 0.26442575\n",
            "Iteration 526, loss = 0.26410165\n",
            "Iteration 527, loss = 0.26378021\n",
            "Iteration 528, loss = 0.26346133\n",
            "Iteration 529, loss = 0.26314523\n",
            "Iteration 530, loss = 0.26283249\n",
            "Iteration 531, loss = 0.26252231\n",
            "Iteration 532, loss = 0.26221458\n",
            "Iteration 533, loss = 0.26190899\n",
            "Iteration 534, loss = 0.26160496\n",
            "Iteration 535, loss = 0.26130728\n",
            "Iteration 536, loss = 0.26101314\n",
            "Iteration 537, loss = 0.26072142\n",
            "Iteration 538, loss = 0.26043205\n",
            "Iteration 539, loss = 0.26014528\n",
            "Iteration 540, loss = 0.25986076\n",
            "Iteration 541, loss = 0.25957898\n",
            "Iteration 542, loss = 0.25930000\n",
            "Iteration 543, loss = 0.25902331\n",
            "Iteration 544, loss = 0.25874882\n",
            "Iteration 545, loss = 0.25847655\n",
            "Iteration 546, loss = 0.25820649\n",
            "Iteration 547, loss = 0.25793842\n",
            "Iteration 548, loss = 0.25767254\n",
            "Iteration 549, loss = 0.25740877\n",
            "Iteration 550, loss = 0.25714695\n",
            "Iteration 551, loss = 0.25688707\n",
            "Iteration 552, loss = 0.25662915\n",
            "Iteration 553, loss = 0.25637325\n",
            "Iteration 554, loss = 0.25611914\n",
            "Iteration 555, loss = 0.25586690\n",
            "Iteration 556, loss = 0.25561650\n",
            "Iteration 557, loss = 0.25536800\n",
            "Iteration 558, loss = 0.25512142\n",
            "Iteration 559, loss = 0.25487667\n",
            "Iteration 560, loss = 0.25463385\n",
            "Iteration 561, loss = 0.25439280\n",
            "Iteration 562, loss = 0.25415350\n",
            "Iteration 563, loss = 0.25391589\n",
            "Iteration 564, loss = 0.25367992\n",
            "Iteration 565, loss = 0.25344606\n",
            "Iteration 566, loss = 0.25321404\n",
            "Iteration 567, loss = 0.25298365\n",
            "Iteration 568, loss = 0.25275488\n",
            "Iteration 569, loss = 0.25252772\n",
            "Iteration 570, loss = 0.25230226\n",
            "Iteration 571, loss = 0.25207847\n",
            "Iteration 572, loss = 0.25185631\n",
            "Iteration 573, loss = 0.25163569\n",
            "Iteration 574, loss = 0.25141675\n",
            "Iteration 575, loss = 0.25119939\n",
            "Iteration 576, loss = 0.25098351\n",
            "Iteration 577, loss = 0.25076905\n",
            "Iteration 578, loss = 0.25055605\n",
            "Iteration 579, loss = 0.25034439\n",
            "Iteration 580, loss = 0.25013429\n",
            "Iteration 581, loss = 0.24992577\n",
            "Iteration 582, loss = 0.24971864\n",
            "Iteration 583, loss = 0.24951289\n",
            "Iteration 584, loss = 0.24930828\n",
            "Iteration 585, loss = 0.24910478\n",
            "Iteration 586, loss = 0.24890253\n",
            "Iteration 587, loss = 0.24870153\n",
            "Iteration 588, loss = 0.24850142\n",
            "Iteration 589, loss = 0.24830237\n",
            "Iteration 590, loss = 0.24810451\n",
            "Iteration 591, loss = 0.24790755\n",
            "Iteration 592, loss = 0.24771154\n",
            "Iteration 593, loss = 0.24751657\n",
            "Iteration 594, loss = 0.24732277\n",
            "Iteration 595, loss = 0.24713003\n",
            "Iteration 596, loss = 0.24693845\n",
            "Iteration 597, loss = 0.24674861\n",
            "Iteration 598, loss = 0.24655978\n",
            "Iteration 599, loss = 0.24637200\n",
            "Iteration 600, loss = 0.24618527\n",
            "Iteration 601, loss = 0.24599951\n",
            "Iteration 602, loss = 0.24581479\n",
            "Iteration 603, loss = 0.24563106\n",
            "Iteration 604, loss = 0.24544842\n",
            "Iteration 605, loss = 0.24526696\n",
            "Iteration 606, loss = 0.24508627\n",
            "Iteration 607, loss = 0.24490640\n",
            "Iteration 608, loss = 0.24472757\n",
            "Iteration 609, loss = 0.24455028\n",
            "Iteration 610, loss = 0.24437396\n",
            "Iteration 611, loss = 0.24419863\n",
            "Iteration 612, loss = 0.24402459\n",
            "Iteration 613, loss = 0.24385207\n",
            "Iteration 614, loss = 0.24368064\n",
            "Iteration 615, loss = 0.24351020\n",
            "Iteration 616, loss = 0.24334062\n",
            "Iteration 617, loss = 0.24317215\n",
            "Iteration 618, loss = 0.24300478\n",
            "Iteration 619, loss = 0.24283843\n",
            "Iteration 620, loss = 0.24267323\n",
            "Iteration 621, loss = 0.24250892\n",
            "Iteration 622, loss = 0.24234543\n",
            "Iteration 623, loss = 0.24218294\n",
            "Iteration 624, loss = 0.24202111\n",
            "Iteration 625, loss = 0.24185998\n",
            "Iteration 626, loss = 0.24169958\n",
            "Iteration 627, loss = 0.24153996\n",
            "Iteration 628, loss = 0.24138099\n",
            "Iteration 629, loss = 0.24122288\n",
            "Iteration 630, loss = 0.24106543\n",
            "Iteration 631, loss = 0.24090871\n",
            "Iteration 632, loss = 0.24075273\n",
            "Iteration 633, loss = 0.24059745\n",
            "Iteration 634, loss = 0.24044296\n",
            "Iteration 635, loss = 0.24028940\n",
            "Iteration 636, loss = 0.24013648\n",
            "Iteration 637, loss = 0.23998430\n",
            "Iteration 638, loss = 0.23983287\n",
            "Iteration 639, loss = 0.23968235\n",
            "Iteration 640, loss = 0.23953250\n",
            "Iteration 641, loss = 0.23938399\n",
            "Iteration 642, loss = 0.23923661\n",
            "Iteration 643, loss = 0.23908985\n",
            "Iteration 644, loss = 0.23894385\n",
            "Iteration 645, loss = 0.23879845\n",
            "Iteration 646, loss = 0.23865375\n",
            "Iteration 647, loss = 0.23850976\n",
            "Iteration 648, loss = 0.23836646\n",
            "Iteration 649, loss = 0.23822419\n",
            "Iteration 650, loss = 0.23808271\n",
            "Iteration 651, loss = 0.23794183\n",
            "Iteration 652, loss = 0.23780160\n",
            "Iteration 653, loss = 0.23766192\n",
            "Iteration 654, loss = 0.23752280\n",
            "Iteration 655, loss = 0.23738440\n",
            "Iteration 656, loss = 0.23724677\n",
            "Iteration 657, loss = 0.23710974\n",
            "Iteration 658, loss = 0.23697328\n",
            "Iteration 659, loss = 0.23683740\n",
            "Iteration 660, loss = 0.23670247\n",
            "Iteration 661, loss = 0.23656821\n",
            "Iteration 662, loss = 0.23643453\n",
            "Iteration 663, loss = 0.23630150\n",
            "Iteration 664, loss = 0.23616908\n",
            "Iteration 665, loss = 0.23603716\n",
            "Iteration 666, loss = 0.23590582\n",
            "Iteration 667, loss = 0.23577495\n",
            "Iteration 668, loss = 0.23564463\n",
            "Iteration 669, loss = 0.23551509\n",
            "Iteration 670, loss = 0.23538600\n",
            "Iteration 671, loss = 0.23525744\n",
            "Iteration 672, loss = 0.23513002\n",
            "Iteration 673, loss = 0.23500345\n",
            "Iteration 674, loss = 0.23487785\n",
            "Iteration 675, loss = 0.23475269\n",
            "Iteration 676, loss = 0.23462811\n",
            "Iteration 677, loss = 0.23450420\n",
            "Iteration 678, loss = 0.23438076\n",
            "Iteration 679, loss = 0.23425790\n",
            "Iteration 680, loss = 0.23413568\n",
            "Iteration 681, loss = 0.23401412\n",
            "Iteration 682, loss = 0.23389368\n",
            "Iteration 683, loss = 0.23377369\n",
            "Iteration 684, loss = 0.23365419\n",
            "Iteration 685, loss = 0.23353509\n",
            "Iteration 686, loss = 0.23341660\n",
            "Iteration 687, loss = 0.23329886\n",
            "Iteration 688, loss = 0.23318174\n",
            "Iteration 689, loss = 0.23306492\n",
            "Iteration 690, loss = 0.23294851\n",
            "Iteration 691, loss = 0.23283242\n",
            "Iteration 692, loss = 0.23271675\n",
            "Iteration 693, loss = 0.23260159\n",
            "Iteration 694, loss = 0.23248665\n",
            "Iteration 695, loss = 0.23237227\n",
            "Iteration 696, loss = 0.23225841\n",
            "Iteration 697, loss = 0.23214495\n",
            "Iteration 698, loss = 0.23203193\n",
            "Iteration 699, loss = 0.23191937\n",
            "Iteration 700, loss = 0.23180703\n",
            "Iteration 701, loss = 0.23169532\n",
            "Iteration 702, loss = 0.23158407\n",
            "Iteration 703, loss = 0.23147310\n",
            "Iteration 704, loss = 0.23136268\n",
            "Iteration 705, loss = 0.23125290\n",
            "Iteration 706, loss = 0.23114388\n",
            "Iteration 707, loss = 0.23103506\n",
            "Iteration 708, loss = 0.23092683\n",
            "Iteration 709, loss = 0.23081898\n",
            "Iteration 710, loss = 0.23071154\n",
            "Iteration 711, loss = 0.23060439\n",
            "Iteration 712, loss = 0.23049782\n",
            "Iteration 713, loss = 0.23039158\n",
            "Iteration 714, loss = 0.23028573\n",
            "Iteration 715, loss = 0.23018020\n",
            "Iteration 716, loss = 0.23007496\n",
            "Iteration 717, loss = 0.22997006\n",
            "Iteration 718, loss = 0.22986544\n",
            "Iteration 719, loss = 0.22976115\n",
            "Iteration 720, loss = 0.22965720\n",
            "Iteration 721, loss = 0.22955364\n",
            "Iteration 722, loss = 0.22945042\n",
            "Iteration 723, loss = 0.22934727\n",
            "Iteration 724, loss = 0.22924447\n",
            "Iteration 725, loss = 0.22914197\n",
            "Iteration 726, loss = 0.22903977\n",
            "Iteration 727, loss = 0.22893781\n",
            "Iteration 728, loss = 0.22883613\n",
            "Iteration 729, loss = 0.22873491\n",
            "Iteration 730, loss = 0.22863381\n",
            "Iteration 731, loss = 0.22853301\n",
            "Iteration 732, loss = 0.22843256\n",
            "Iteration 733, loss = 0.22833231\n",
            "Iteration 734, loss = 0.22823262\n",
            "Iteration 735, loss = 0.22813321\n",
            "Iteration 736, loss = 0.22803421\n",
            "Iteration 737, loss = 0.22793535\n",
            "Iteration 738, loss = 0.22783687\n",
            "Iteration 739, loss = 0.22773852\n",
            "Iteration 740, loss = 0.22764054\n",
            "Iteration 741, loss = 0.22754293\n",
            "Iteration 742, loss = 0.22744583\n",
            "Iteration 743, loss = 0.22734913\n",
            "Iteration 744, loss = 0.22725266\n",
            "Iteration 745, loss = 0.22715652\n",
            "Iteration 746, loss = 0.22706057\n",
            "Iteration 747, loss = 0.22696492\n",
            "Iteration 748, loss = 0.22686958\n",
            "Iteration 749, loss = 0.22677431\n",
            "Iteration 750, loss = 0.22667931\n",
            "Iteration 751, loss = 0.22658474\n",
            "Iteration 752, loss = 0.22649015\n",
            "Iteration 753, loss = 0.22639592\n",
            "Iteration 754, loss = 0.22630179\n",
            "Iteration 755, loss = 0.22620793\n",
            "Iteration 756, loss = 0.22611423\n",
            "Iteration 757, loss = 0.22602077\n",
            "Iteration 758, loss = 0.22592755\n",
            "Iteration 759, loss = 0.22583470\n",
            "Iteration 760, loss = 0.22574211\n",
            "Iteration 761, loss = 0.22564951\n",
            "Iteration 762, loss = 0.22555720\n",
            "Iteration 763, loss = 0.22546522\n",
            "Iteration 764, loss = 0.22537364\n",
            "Iteration 765, loss = 0.22528206\n",
            "Iteration 766, loss = 0.22519092\n",
            "Iteration 767, loss = 0.22509987\n",
            "Iteration 768, loss = 0.22500906\n",
            "Iteration 769, loss = 0.22491855\n",
            "Iteration 770, loss = 0.22482831\n",
            "Iteration 771, loss = 0.22473814\n",
            "Iteration 772, loss = 0.22464817\n",
            "Iteration 773, loss = 0.22455829\n",
            "Iteration 774, loss = 0.22446866\n",
            "Iteration 775, loss = 0.22437928\n",
            "Iteration 776, loss = 0.22429010\n",
            "Iteration 777, loss = 0.22420118\n",
            "Iteration 778, loss = 0.22411223\n",
            "Iteration 779, loss = 0.22402358\n",
            "Iteration 780, loss = 0.22393503\n",
            "Iteration 781, loss = 0.22384663\n",
            "Iteration 782, loss = 0.22375834\n",
            "Iteration 783, loss = 0.22367025\n",
            "Iteration 784, loss = 0.22358216\n",
            "Iteration 785, loss = 0.22349426\n",
            "Iteration 786, loss = 0.22340647\n",
            "Iteration 787, loss = 0.22331893\n",
            "Iteration 788, loss = 0.22323145\n",
            "Iteration 789, loss = 0.22314427\n",
            "Iteration 790, loss = 0.22305732\n",
            "Iteration 791, loss = 0.22297033\n",
            "Iteration 792, loss = 0.22288369\n",
            "Iteration 793, loss = 0.22279671\n",
            "Iteration 794, loss = 0.22270977\n",
            "Iteration 795, loss = 0.22262282\n",
            "Iteration 796, loss = 0.22253599\n",
            "Iteration 797, loss = 0.22244910\n",
            "Iteration 798, loss = 0.22236237\n",
            "Iteration 799, loss = 0.22227558\n",
            "Iteration 800, loss = 0.22218895\n",
            "Iteration 1, loss = 0.70355192\n",
            "Iteration 2, loss = 0.70302702\n",
            "Iteration 3, loss = 0.70233593\n",
            "Iteration 4, loss = 0.70149048\n",
            "Iteration 5, loss = 0.70050242\n",
            "Iteration 6, loss = 0.69948174\n",
            "Iteration 7, loss = 0.69840238\n",
            "Iteration 8, loss = 0.69735680\n",
            "Iteration 9, loss = 0.69630344\n",
            "Iteration 10, loss = 0.69523106\n",
            "Iteration 11, loss = 0.69429400\n",
            "Iteration 12, loss = 0.69333284\n",
            "Iteration 13, loss = 0.69246348\n",
            "Iteration 14, loss = 0.69167630\n",
            "Iteration 15, loss = 0.69093899\n",
            "Iteration 16, loss = 0.69031276\n",
            "Iteration 17, loss = 0.68979665\n",
            "Iteration 18, loss = 0.68942123\n",
            "Iteration 19, loss = 0.68910580\n",
            "Iteration 20, loss = 0.68886062\n",
            "Iteration 21, loss = 0.68867284\n",
            "Iteration 22, loss = 0.68849692\n",
            "Iteration 23, loss = 0.68834948\n",
            "Iteration 24, loss = 0.68820050\n",
            "Iteration 25, loss = 0.68807504\n",
            "Iteration 26, loss = 0.68795467\n",
            "Iteration 27, loss = 0.68784483\n",
            "Iteration 28, loss = 0.68774161\n",
            "Iteration 29, loss = 0.68764457\n",
            "Iteration 30, loss = 0.68755151\n",
            "Iteration 31, loss = 0.68747437\n",
            "Iteration 32, loss = 0.68739925\n",
            "Iteration 33, loss = 0.68731730\n",
            "Iteration 34, loss = 0.68723358\n",
            "Iteration 35, loss = 0.68715149\n",
            "Iteration 36, loss = 0.68706762\n",
            "Iteration 37, loss = 0.68698122\n",
            "Iteration 38, loss = 0.68689250\n",
            "Iteration 39, loss = 0.68680160\n",
            "Iteration 40, loss = 0.68670870\n",
            "Iteration 41, loss = 0.68661399\n",
            "Iteration 42, loss = 0.68651760\n",
            "Iteration 43, loss = 0.68641967\n",
            "Iteration 44, loss = 0.68632031\n",
            "Iteration 45, loss = 0.68621962\n",
            "Iteration 46, loss = 0.68611766\n",
            "Iteration 47, loss = 0.68601390\n",
            "Iteration 48, loss = 0.68590577\n",
            "Iteration 49, loss = 0.68579593\n",
            "Iteration 50, loss = 0.68568452\n",
            "Iteration 51, loss = 0.68557306\n",
            "Iteration 52, loss = 0.68545981\n",
            "Iteration 53, loss = 0.68534542\n",
            "Iteration 54, loss = 0.68522970\n",
            "Iteration 55, loss = 0.68511279\n",
            "Iteration 56, loss = 0.68499483\n",
            "Iteration 57, loss = 0.68487628\n",
            "Iteration 58, loss = 0.68475976\n",
            "Iteration 59, loss = 0.68464419\n",
            "Iteration 60, loss = 0.68452976\n",
            "Iteration 61, loss = 0.68441647\n",
            "Iteration 62, loss = 0.68430247\n",
            "Iteration 63, loss = 0.68418776\n",
            "Iteration 64, loss = 0.68407231\n",
            "Iteration 65, loss = 0.68395612\n",
            "Iteration 66, loss = 0.68383919\n",
            "Iteration 67, loss = 0.68372188\n",
            "Iteration 68, loss = 0.68360452\n",
            "Iteration 69, loss = 0.68348607\n",
            "Iteration 70, loss = 0.68336656\n",
            "Iteration 71, loss = 0.68324598\n",
            "Iteration 72, loss = 0.68312434\n",
            "Iteration 73, loss = 0.68300164\n",
            "Iteration 74, loss = 0.68287798\n",
            "Iteration 75, loss = 0.68275332\n",
            "Iteration 76, loss = 0.68262863\n",
            "Iteration 77, loss = 0.68250307\n",
            "Iteration 78, loss = 0.68237475\n",
            "Iteration 79, loss = 0.68224452\n",
            "Iteration 80, loss = 0.68211302\n",
            "Iteration 81, loss = 0.68198031\n",
            "Iteration 82, loss = 0.68184640\n",
            "Iteration 83, loss = 0.68171131\n",
            "Iteration 84, loss = 0.68157502\n",
            "Iteration 85, loss = 0.68143737\n",
            "Iteration 86, loss = 0.68129854\n",
            "Iteration 87, loss = 0.68115854\n",
            "Iteration 88, loss = 0.68101547\n",
            "Iteration 89, loss = 0.68086725\n",
            "Iteration 90, loss = 0.68071594\n",
            "Iteration 91, loss = 0.68056286\n",
            "Iteration 92, loss = 0.68040799\n",
            "Iteration 93, loss = 0.68025154\n",
            "Iteration 94, loss = 0.68009337\n",
            "Iteration 95, loss = 0.67993358\n",
            "Iteration 96, loss = 0.67977218\n",
            "Iteration 97, loss = 0.67960934\n",
            "Iteration 98, loss = 0.67944496\n",
            "Iteration 99, loss = 0.67927921\n",
            "Iteration 100, loss = 0.67910919\n",
            "Iteration 101, loss = 0.67893455\n",
            "Iteration 102, loss = 0.67875659\n",
            "Iteration 103, loss = 0.67857007\n",
            "Iteration 104, loss = 0.67837778\n",
            "Iteration 105, loss = 0.67818241\n",
            "Iteration 106, loss = 0.67798422\n",
            "Iteration 107, loss = 0.67778320\n",
            "Iteration 108, loss = 0.67757983\n",
            "Iteration 109, loss = 0.67737408\n",
            "Iteration 110, loss = 0.67716597\n",
            "Iteration 111, loss = 0.67695554\n",
            "Iteration 112, loss = 0.67674202\n",
            "Iteration 113, loss = 0.67652120\n",
            "Iteration 114, loss = 0.67630464\n",
            "Iteration 115, loss = 0.67608136\n",
            "Iteration 116, loss = 0.67585318\n",
            "Iteration 117, loss = 0.67561555\n",
            "Iteration 118, loss = 0.67536752\n",
            "Iteration 119, loss = 0.67511476\n",
            "Iteration 120, loss = 0.67486055\n",
            "Iteration 121, loss = 0.67460487\n",
            "Iteration 122, loss = 0.67434389\n",
            "Iteration 123, loss = 0.67407320\n",
            "Iteration 124, loss = 0.67379144\n",
            "Iteration 125, loss = 0.67350353\n",
            "Iteration 126, loss = 0.67321411\n",
            "Iteration 127, loss = 0.67293182\n",
            "Iteration 128, loss = 0.67264725\n",
            "Iteration 129, loss = 0.67236301\n",
            "Iteration 130, loss = 0.67207690\n",
            "Iteration 131, loss = 0.67179422\n",
            "Iteration 132, loss = 0.67151046\n",
            "Iteration 133, loss = 0.67122194\n",
            "Iteration 134, loss = 0.67093184\n",
            "Iteration 135, loss = 0.67064006\n",
            "Iteration 136, loss = 0.67034104\n",
            "Iteration 137, loss = 0.67003720\n",
            "Iteration 138, loss = 0.66972189\n",
            "Iteration 139, loss = 0.66939434\n",
            "Iteration 140, loss = 0.66905722\n",
            "Iteration 141, loss = 0.66871042\n",
            "Iteration 142, loss = 0.66835562\n",
            "Iteration 143, loss = 0.66799602\n",
            "Iteration 144, loss = 0.66763877\n",
            "Iteration 145, loss = 0.66727399\n",
            "Iteration 146, loss = 0.66689392\n",
            "Iteration 147, loss = 0.66649813\n",
            "Iteration 148, loss = 0.66608691\n",
            "Iteration 149, loss = 0.66565335\n",
            "Iteration 150, loss = 0.66521357\n",
            "Iteration 151, loss = 0.66476461\n",
            "Iteration 152, loss = 0.66431088\n",
            "Iteration 153, loss = 0.66384735\n",
            "Iteration 154, loss = 0.66336899\n",
            "Iteration 155, loss = 0.66287067\n",
            "Iteration 156, loss = 0.66236522\n",
            "Iteration 157, loss = 0.66186774\n",
            "Iteration 158, loss = 0.66137034\n",
            "Iteration 159, loss = 0.66086895\n",
            "Iteration 160, loss = 0.66036272\n",
            "Iteration 161, loss = 0.65983480\n",
            "Iteration 162, loss = 0.65928352\n",
            "Iteration 163, loss = 0.65870663\n",
            "Iteration 164, loss = 0.65811043\n",
            "Iteration 165, loss = 0.65749764\n",
            "Iteration 166, loss = 0.65685999\n",
            "Iteration 167, loss = 0.65617103\n",
            "Iteration 168, loss = 0.65546597\n",
            "Iteration 169, loss = 0.65475025\n",
            "Iteration 170, loss = 0.65401573\n",
            "Iteration 171, loss = 0.65324475\n",
            "Iteration 172, loss = 0.65245795\n",
            "Iteration 173, loss = 0.65166031\n",
            "Iteration 174, loss = 0.65084112\n",
            "Iteration 175, loss = 0.64998737\n",
            "Iteration 176, loss = 0.64910202\n",
            "Iteration 177, loss = 0.64813551\n",
            "Iteration 178, loss = 0.64714081\n",
            "Iteration 179, loss = 0.64611923\n",
            "Iteration 180, loss = 0.64506439\n",
            "Iteration 181, loss = 0.64399047\n",
            "Iteration 182, loss = 0.64287909\n",
            "Iteration 183, loss = 0.64172909\n",
            "Iteration 184, loss = 0.64053696\n",
            "Iteration 185, loss = 0.63928974\n",
            "Iteration 186, loss = 0.63799361\n",
            "Iteration 187, loss = 0.63665167\n",
            "Iteration 188, loss = 0.63530519\n",
            "Iteration 189, loss = 0.63395864\n",
            "Iteration 190, loss = 0.63258304\n",
            "Iteration 191, loss = 0.63116038\n",
            "Iteration 192, loss = 0.62971857\n",
            "Iteration 193, loss = 0.62822957\n",
            "Iteration 194, loss = 0.62672421\n",
            "Iteration 195, loss = 0.62516205\n",
            "Iteration 196, loss = 0.62356884\n",
            "Iteration 197, loss = 0.62197426\n",
            "Iteration 198, loss = 0.62038805\n",
            "Iteration 199, loss = 0.61879671\n",
            "Iteration 200, loss = 0.61720273\n",
            "Iteration 201, loss = 0.61558722\n",
            "Iteration 202, loss = 0.61389025\n",
            "Iteration 203, loss = 0.61216347\n",
            "Iteration 204, loss = 0.61039588\n",
            "Iteration 205, loss = 0.60848675\n",
            "Iteration 206, loss = 0.60651758\n",
            "Iteration 207, loss = 0.60452709\n",
            "Iteration 208, loss = 0.60248913\n",
            "Iteration 209, loss = 0.60044113\n",
            "Iteration 210, loss = 0.59839871\n",
            "Iteration 211, loss = 0.59635729\n",
            "Iteration 212, loss = 0.59428021\n",
            "Iteration 213, loss = 0.59219767\n",
            "Iteration 214, loss = 0.59007695\n",
            "Iteration 215, loss = 0.58795380\n",
            "Iteration 216, loss = 0.58579752\n",
            "Iteration 217, loss = 0.58364218\n",
            "Iteration 218, loss = 0.58149166\n",
            "Iteration 219, loss = 0.57935561\n",
            "Iteration 220, loss = 0.57725915\n",
            "Iteration 221, loss = 0.57519429\n",
            "Iteration 222, loss = 0.57312948\n",
            "Iteration 223, loss = 0.57103332\n",
            "Iteration 224, loss = 0.56893988\n",
            "Iteration 225, loss = 0.56684900\n",
            "Iteration 226, loss = 0.56479046\n",
            "Iteration 227, loss = 0.56273047\n",
            "Iteration 228, loss = 0.56066955\n",
            "Iteration 229, loss = 0.55861668\n",
            "Iteration 230, loss = 0.55657319\n",
            "Iteration 231, loss = 0.55453620\n",
            "Iteration 232, loss = 0.55250520\n",
            "Iteration 233, loss = 0.55048938\n",
            "Iteration 234, loss = 0.54848715\n",
            "Iteration 235, loss = 0.54650030\n",
            "Iteration 236, loss = 0.54453821\n",
            "Iteration 237, loss = 0.54258828\n",
            "Iteration 238, loss = 0.54064636\n",
            "Iteration 239, loss = 0.53871108\n",
            "Iteration 240, loss = 0.53678440\n",
            "Iteration 241, loss = 0.53488590\n",
            "Iteration 242, loss = 0.53300261\n",
            "Iteration 243, loss = 0.53113822\n",
            "Iteration 244, loss = 0.52928065\n",
            "Iteration 245, loss = 0.52743036\n",
            "Iteration 246, loss = 0.52558636\n",
            "Iteration 247, loss = 0.52375054\n",
            "Iteration 248, loss = 0.52192774\n",
            "Iteration 249, loss = 0.52011203\n",
            "Iteration 250, loss = 0.51830520\n",
            "Iteration 251, loss = 0.51650674\n",
            "Iteration 252, loss = 0.51471404\n",
            "Iteration 253, loss = 0.51292795\n",
            "Iteration 254, loss = 0.51115126\n",
            "Iteration 255, loss = 0.50938419\n",
            "Iteration 256, loss = 0.50762523\n",
            "Iteration 257, loss = 0.50587446\n",
            "Iteration 258, loss = 0.50413030\n",
            "Iteration 259, loss = 0.50239288\n",
            "Iteration 260, loss = 0.50066220\n",
            "Iteration 261, loss = 0.49894023\n",
            "Iteration 262, loss = 0.49722572\n",
            "Iteration 263, loss = 0.49551738\n",
            "Iteration 264, loss = 0.49381445\n",
            "Iteration 265, loss = 0.49211660\n",
            "Iteration 266, loss = 0.49042316\n",
            "Iteration 267, loss = 0.48873621\n",
            "Iteration 268, loss = 0.48705555\n",
            "Iteration 269, loss = 0.48538086\n",
            "Iteration 270, loss = 0.48371231\n",
            "Iteration 271, loss = 0.48205032\n",
            "Iteration 272, loss = 0.48039321\n",
            "Iteration 273, loss = 0.47874239\n",
            "Iteration 274, loss = 0.47709835\n",
            "Iteration 275, loss = 0.47546120\n",
            "Iteration 276, loss = 0.47382911\n",
            "Iteration 277, loss = 0.47220264\n",
            "Iteration 278, loss = 0.47058257\n",
            "Iteration 279, loss = 0.46896834\n",
            "Iteration 280, loss = 0.46736038\n",
            "Iteration 281, loss = 0.46575964\n",
            "Iteration 282, loss = 0.46416591\n",
            "Iteration 283, loss = 0.46257934\n",
            "Iteration 284, loss = 0.46100022\n",
            "Iteration 285, loss = 0.45942805\n",
            "Iteration 286, loss = 0.45786281\n",
            "Iteration 287, loss = 0.45630396\n",
            "Iteration 288, loss = 0.45475284\n",
            "Iteration 289, loss = 0.45320945\n",
            "Iteration 290, loss = 0.45167281\n",
            "Iteration 291, loss = 0.45014377\n",
            "Iteration 292, loss = 0.44862324\n",
            "Iteration 293, loss = 0.44711097\n",
            "Iteration 294, loss = 0.44560608\n",
            "Iteration 295, loss = 0.44410855\n",
            "Iteration 296, loss = 0.44261813\n",
            "Iteration 297, loss = 0.44113563\n",
            "Iteration 298, loss = 0.43965961\n",
            "Iteration 299, loss = 0.43819123\n",
            "Iteration 300, loss = 0.43673071\n",
            "Iteration 301, loss = 0.43527877\n",
            "Iteration 302, loss = 0.43383455\n",
            "Iteration 303, loss = 0.43239836\n",
            "Iteration 304, loss = 0.43097115\n",
            "Iteration 305, loss = 0.42955403\n",
            "Iteration 306, loss = 0.42814422\n",
            "Iteration 307, loss = 0.42674364\n",
            "Iteration 308, loss = 0.42535098\n",
            "Iteration 309, loss = 0.42396578\n",
            "Iteration 310, loss = 0.42258920\n",
            "Iteration 311, loss = 0.42122042\n",
            "Iteration 312, loss = 0.41985996\n",
            "Iteration 313, loss = 0.41850851\n",
            "Iteration 314, loss = 0.41716564\n",
            "Iteration 315, loss = 0.41583192\n",
            "Iteration 316, loss = 0.41450740\n",
            "Iteration 317, loss = 0.41319180\n",
            "Iteration 318, loss = 0.41188522\n",
            "Iteration 319, loss = 0.41058742\n",
            "Iteration 320, loss = 0.40929930\n",
            "Iteration 321, loss = 0.40802299\n",
            "Iteration 322, loss = 0.40675562\n",
            "Iteration 323, loss = 0.40549706\n",
            "Iteration 324, loss = 0.40424700\n",
            "Iteration 325, loss = 0.40300547\n",
            "Iteration 326, loss = 0.40177273\n",
            "Iteration 327, loss = 0.40054836\n",
            "Iteration 328, loss = 0.39933255\n",
            "Iteration 329, loss = 0.39812555\n",
            "Iteration 330, loss = 0.39692742\n",
            "Iteration 331, loss = 0.39573737\n",
            "Iteration 332, loss = 0.39455540\n",
            "Iteration 333, loss = 0.39338161\n",
            "Iteration 334, loss = 0.39221557\n",
            "Iteration 335, loss = 0.39105792\n",
            "Iteration 336, loss = 0.38990843\n",
            "Iteration 337, loss = 0.38876807\n",
            "Iteration 338, loss = 0.38763647\n",
            "Iteration 339, loss = 0.38651400\n",
            "Iteration 340, loss = 0.38540058\n",
            "Iteration 341, loss = 0.38429649\n",
            "Iteration 342, loss = 0.38320113\n",
            "Iteration 343, loss = 0.38211507\n",
            "Iteration 344, loss = 0.38103781\n",
            "Iteration 345, loss = 0.37996915\n",
            "Iteration 346, loss = 0.37890856\n",
            "Iteration 347, loss = 0.37785600\n",
            "Iteration 348, loss = 0.37681114\n",
            "Iteration 349, loss = 0.37577358\n",
            "Iteration 350, loss = 0.37474412\n",
            "Iteration 351, loss = 0.37372236\n",
            "Iteration 352, loss = 0.37270802\n",
            "Iteration 353, loss = 0.37170118\n",
            "Iteration 354, loss = 0.37070170\n",
            "Iteration 355, loss = 0.36970926\n",
            "Iteration 356, loss = 0.36872478\n",
            "Iteration 357, loss = 0.36774646\n",
            "Iteration 358, loss = 0.36677444\n",
            "Iteration 359, loss = 0.36580939\n",
            "Iteration 360, loss = 0.36485256\n",
            "Iteration 361, loss = 0.36390326\n",
            "Iteration 362, loss = 0.36296094\n",
            "Iteration 363, loss = 0.36202587\n",
            "Iteration 364, loss = 0.36109801\n",
            "Iteration 365, loss = 0.36017808\n",
            "Iteration 366, loss = 0.35926553\n",
            "Iteration 367, loss = 0.35836005\n",
            "Iteration 368, loss = 0.35746203\n",
            "Iteration 369, loss = 0.35657123\n",
            "Iteration 370, loss = 0.35568744\n",
            "Iteration 371, loss = 0.35481093\n",
            "Iteration 372, loss = 0.35394210\n",
            "Iteration 373, loss = 0.35308202\n",
            "Iteration 374, loss = 0.35222891\n",
            "Iteration 375, loss = 0.35138266\n",
            "Iteration 376, loss = 0.35054321\n",
            "Iteration 377, loss = 0.34971040\n",
            "Iteration 378, loss = 0.34888383\n",
            "Iteration 379, loss = 0.34806396\n",
            "Iteration 380, loss = 0.34725002\n",
            "Iteration 381, loss = 0.34644212\n",
            "Iteration 382, loss = 0.34564114\n",
            "Iteration 383, loss = 0.34484598\n",
            "Iteration 384, loss = 0.34405786\n",
            "Iteration 385, loss = 0.34327810\n",
            "Iteration 386, loss = 0.34250468\n",
            "Iteration 387, loss = 0.34173803\n",
            "Iteration 388, loss = 0.34097722\n",
            "Iteration 389, loss = 0.34022206\n",
            "Iteration 390, loss = 0.33947199\n",
            "Iteration 391, loss = 0.33872744\n",
            "Iteration 392, loss = 0.33798836\n",
            "Iteration 393, loss = 0.33725494\n",
            "Iteration 394, loss = 0.33652685\n",
            "Iteration 395, loss = 0.33580454\n",
            "Iteration 396, loss = 0.33508778\n",
            "Iteration 397, loss = 0.33437621\n",
            "Iteration 398, loss = 0.33366997\n",
            "Iteration 399, loss = 0.33296927\n",
            "Iteration 400, loss = 0.33227326\n",
            "Iteration 401, loss = 0.33158212\n",
            "Iteration 402, loss = 0.33089589\n",
            "Iteration 403, loss = 0.33021484\n",
            "Iteration 404, loss = 0.32953827\n",
            "Iteration 405, loss = 0.32886679\n",
            "Iteration 406, loss = 0.32819994\n",
            "Iteration 407, loss = 0.32753784\n",
            "Iteration 408, loss = 0.32688190\n",
            "Iteration 409, loss = 0.32623145\n",
            "Iteration 410, loss = 0.32558581\n",
            "Iteration 411, loss = 0.32494555\n",
            "Iteration 412, loss = 0.32431040\n",
            "Iteration 413, loss = 0.32368008\n",
            "Iteration 414, loss = 0.32305450\n",
            "Iteration 415, loss = 0.32243377\n",
            "Iteration 416, loss = 0.32181788\n",
            "Iteration 417, loss = 0.32120676\n",
            "Iteration 418, loss = 0.32060026\n",
            "Iteration 419, loss = 0.31999829\n",
            "Iteration 420, loss = 0.31940077\n",
            "Iteration 421, loss = 0.31880803\n",
            "Iteration 422, loss = 0.31822293\n",
            "Iteration 423, loss = 0.31764176\n",
            "Iteration 424, loss = 0.31706549\n",
            "Iteration 425, loss = 0.31649654\n",
            "Iteration 426, loss = 0.31593192\n",
            "Iteration 427, loss = 0.31537180\n",
            "Iteration 428, loss = 0.31481594\n",
            "Iteration 429, loss = 0.31426412\n",
            "Iteration 430, loss = 0.31371646\n",
            "Iteration 431, loss = 0.31317330\n",
            "Iteration 432, loss = 0.31262943\n",
            "Iteration 433, loss = 0.31205760\n",
            "Iteration 434, loss = 0.31148075\n",
            "Iteration 435, loss = 0.31090013\n",
            "Iteration 436, loss = 0.31031565\n",
            "Iteration 437, loss = 0.30972962\n",
            "Iteration 438, loss = 0.30914529\n",
            "Iteration 439, loss = 0.30856071\n",
            "Iteration 440, loss = 0.30797953\n",
            "Iteration 441, loss = 0.30740127\n",
            "Iteration 442, loss = 0.30682433\n",
            "Iteration 443, loss = 0.30624995\n",
            "Iteration 444, loss = 0.30567820\n",
            "Iteration 445, loss = 0.30511046\n",
            "Iteration 446, loss = 0.30454987\n",
            "Iteration 447, loss = 0.30399417\n",
            "Iteration 448, loss = 0.30344229\n",
            "Iteration 449, loss = 0.30289450\n",
            "Iteration 450, loss = 0.30235430\n",
            "Iteration 451, loss = 0.30181875\n",
            "Iteration 452, loss = 0.30128771\n",
            "Iteration 453, loss = 0.30076060\n",
            "Iteration 454, loss = 0.30023708\n",
            "Iteration 455, loss = 0.29971714\n",
            "Iteration 456, loss = 0.29920088\n",
            "Iteration 457, loss = 0.29868825\n",
            "Iteration 458, loss = 0.29818024\n",
            "Iteration 459, loss = 0.29767573\n",
            "Iteration 460, loss = 0.29717483\n",
            "Iteration 461, loss = 0.29667754\n",
            "Iteration 462, loss = 0.29618411\n",
            "Iteration 463, loss = 0.29569376\n",
            "Iteration 464, loss = 0.29520602\n",
            "Iteration 465, loss = 0.29472185\n",
            "Iteration 466, loss = 0.29424145\n",
            "Iteration 467, loss = 0.29376420\n",
            "Iteration 468, loss = 0.29329001\n",
            "Iteration 469, loss = 0.29281888\n",
            "Iteration 470, loss = 0.29234968\n",
            "Iteration 471, loss = 0.29188340\n",
            "Iteration 472, loss = 0.29142011\n",
            "Iteration 473, loss = 0.29096095\n",
            "Iteration 474, loss = 0.29050523\n",
            "Iteration 475, loss = 0.29005238\n",
            "Iteration 476, loss = 0.28960295\n",
            "Iteration 477, loss = 0.28915653\n",
            "Iteration 478, loss = 0.28871307\n",
            "Iteration 479, loss = 0.28827222\n",
            "Iteration 480, loss = 0.28783412\n",
            "Iteration 481, loss = 0.28739866\n",
            "Iteration 482, loss = 0.28696599\n",
            "Iteration 483, loss = 0.28653647\n",
            "Iteration 484, loss = 0.28610970\n",
            "Iteration 485, loss = 0.28568566\n",
            "Iteration 486, loss = 0.28526416\n",
            "Iteration 487, loss = 0.28484535\n",
            "Iteration 488, loss = 0.28442918\n",
            "Iteration 489, loss = 0.28401531\n",
            "Iteration 490, loss = 0.28360465\n",
            "Iteration 491, loss = 0.28319654\n",
            "Iteration 492, loss = 0.28279102\n",
            "Iteration 493, loss = 0.28238800\n",
            "Iteration 494, loss = 0.28198738\n",
            "Iteration 495, loss = 0.28158916\n",
            "Iteration 496, loss = 0.28119336\n",
            "Iteration 497, loss = 0.28079994\n",
            "Iteration 498, loss = 0.28040873\n",
            "Iteration 499, loss = 0.28001982\n",
            "Iteration 500, loss = 0.27963331\n",
            "Iteration 501, loss = 0.27924901\n",
            "Iteration 502, loss = 0.27886699\n",
            "Iteration 503, loss = 0.27848725\n",
            "Iteration 504, loss = 0.27810967\n",
            "Iteration 505, loss = 0.27773427\n",
            "Iteration 506, loss = 0.27736120\n",
            "Iteration 507, loss = 0.27699028\n",
            "Iteration 508, loss = 0.27662181\n",
            "Iteration 509, loss = 0.27625579\n",
            "Iteration 510, loss = 0.27589190\n",
            "Iteration 511, loss = 0.27552999\n",
            "Iteration 512, loss = 0.27517015\n",
            "Iteration 513, loss = 0.27481324\n",
            "Iteration 514, loss = 0.27445874\n",
            "Iteration 515, loss = 0.27410678\n",
            "Iteration 516, loss = 0.27375690\n",
            "Iteration 517, loss = 0.27341051\n",
            "Iteration 518, loss = 0.27306639\n",
            "Iteration 519, loss = 0.27272512\n",
            "Iteration 520, loss = 0.27238649\n",
            "Iteration 521, loss = 0.27205017\n",
            "Iteration 522, loss = 0.27171701\n",
            "Iteration 523, loss = 0.27138680\n",
            "Iteration 524, loss = 0.27105865\n",
            "Iteration 525, loss = 0.27073227\n",
            "Iteration 526, loss = 0.27040816\n",
            "Iteration 527, loss = 0.27008634\n",
            "Iteration 528, loss = 0.26976659\n",
            "Iteration 529, loss = 0.26944871\n",
            "Iteration 530, loss = 0.26913263\n",
            "Iteration 531, loss = 0.26881831\n",
            "Iteration 532, loss = 0.26850577\n",
            "Iteration 533, loss = 0.26819506\n",
            "Iteration 534, loss = 0.26788611\n",
            "Iteration 535, loss = 0.26757952\n",
            "Iteration 536, loss = 0.26727471\n",
            "Iteration 537, loss = 0.26697111\n",
            "Iteration 538, loss = 0.26666908\n",
            "Iteration 539, loss = 0.26636968\n",
            "Iteration 540, loss = 0.26607166\n",
            "Iteration 541, loss = 0.26577415\n",
            "Iteration 542, loss = 0.26547808\n",
            "Iteration 543, loss = 0.26518433\n",
            "Iteration 544, loss = 0.26489210\n",
            "Iteration 545, loss = 0.26460115\n",
            "Iteration 546, loss = 0.26431163\n",
            "Iteration 547, loss = 0.26402415\n",
            "Iteration 548, loss = 0.26373851\n",
            "Iteration 549, loss = 0.26345459\n",
            "Iteration 550, loss = 0.26317210\n",
            "Iteration 551, loss = 0.26289108\n",
            "Iteration 552, loss = 0.26261141\n",
            "Iteration 553, loss = 0.26233319\n",
            "Iteration 554, loss = 0.26205721\n",
            "Iteration 555, loss = 0.26178192\n",
            "Iteration 556, loss = 0.26150822\n",
            "Iteration 557, loss = 0.26123824\n",
            "Iteration 558, loss = 0.26096989\n",
            "Iteration 559, loss = 0.26070302\n",
            "Iteration 560, loss = 0.26043722\n",
            "Iteration 561, loss = 0.26017276\n",
            "Iteration 562, loss = 0.25990953\n",
            "Iteration 563, loss = 0.25964755\n",
            "Iteration 564, loss = 0.25938714\n",
            "Iteration 565, loss = 0.25912809\n",
            "Iteration 566, loss = 0.25887073\n",
            "Iteration 567, loss = 0.25861457\n",
            "Iteration 568, loss = 0.25835977\n",
            "Iteration 569, loss = 0.25810642\n",
            "Iteration 570, loss = 0.25785438\n",
            "Iteration 571, loss = 0.25760365\n",
            "Iteration 572, loss = 0.25735450\n",
            "Iteration 573, loss = 0.25710681\n",
            "Iteration 574, loss = 0.25686022\n",
            "Iteration 575, loss = 0.25661469\n",
            "Iteration 576, loss = 0.25637033\n",
            "Iteration 577, loss = 0.25612692\n",
            "Iteration 578, loss = 0.25588516\n",
            "Iteration 579, loss = 0.25564504\n",
            "Iteration 580, loss = 0.25540641\n",
            "Iteration 581, loss = 0.25516885\n",
            "Iteration 582, loss = 0.25493241\n",
            "Iteration 583, loss = 0.25469753\n",
            "Iteration 584, loss = 0.25446368\n",
            "Iteration 585, loss = 0.25423086\n",
            "Iteration 586, loss = 0.25399896\n",
            "Iteration 587, loss = 0.25376811\n",
            "Iteration 588, loss = 0.25353822\n",
            "Iteration 589, loss = 0.25330965\n",
            "Iteration 590, loss = 0.25308228\n",
            "Iteration 591, loss = 0.25285581\n",
            "Iteration 592, loss = 0.25263084\n",
            "Iteration 593, loss = 0.25240627\n",
            "Iteration 594, loss = 0.25218305\n",
            "Iteration 595, loss = 0.25196105\n",
            "Iteration 596, loss = 0.25174006\n",
            "Iteration 597, loss = 0.25151993\n",
            "Iteration 598, loss = 0.25130080\n",
            "Iteration 599, loss = 0.25108263\n",
            "Iteration 600, loss = 0.25086543\n",
            "Iteration 601, loss = 0.25064958\n",
            "Iteration 602, loss = 0.25043462\n",
            "Iteration 603, loss = 0.25022056\n",
            "Iteration 604, loss = 0.25000738\n",
            "Iteration 605, loss = 0.24979513\n",
            "Iteration 606, loss = 0.24958371\n",
            "Iteration 607, loss = 0.24937382\n",
            "Iteration 608, loss = 0.24916505\n",
            "Iteration 609, loss = 0.24895689\n",
            "Iteration 610, loss = 0.24874963\n",
            "Iteration 611, loss = 0.24854324\n",
            "Iteration 612, loss = 0.24833851\n",
            "Iteration 613, loss = 0.24813529\n",
            "Iteration 614, loss = 0.24793267\n",
            "Iteration 615, loss = 0.24773053\n",
            "Iteration 616, loss = 0.24752896\n",
            "Iteration 617, loss = 0.24732802\n",
            "Iteration 618, loss = 0.24712772\n",
            "Iteration 619, loss = 0.24692793\n",
            "Iteration 620, loss = 0.24672876\n",
            "Iteration 621, loss = 0.24653059\n",
            "Iteration 622, loss = 0.24633329\n",
            "Iteration 623, loss = 0.24613643\n",
            "Iteration 624, loss = 0.24594026\n",
            "Iteration 625, loss = 0.24574598\n",
            "Iteration 626, loss = 0.24555217\n",
            "Iteration 627, loss = 0.24535885\n",
            "Iteration 628, loss = 0.24516617\n",
            "Iteration 629, loss = 0.24497402\n",
            "Iteration 630, loss = 0.24478250\n",
            "Iteration 631, loss = 0.24459210\n",
            "Iteration 632, loss = 0.24440201\n",
            "Iteration 633, loss = 0.24421289\n",
            "Iteration 634, loss = 0.24402408\n",
            "Iteration 635, loss = 0.24383623\n",
            "Iteration 636, loss = 0.24364917\n",
            "Iteration 637, loss = 0.24346294\n",
            "Iteration 638, loss = 0.24327752\n",
            "Iteration 639, loss = 0.24309240\n",
            "Iteration 640, loss = 0.24290820\n",
            "Iteration 641, loss = 0.24272431\n",
            "Iteration 642, loss = 0.24254113\n",
            "Iteration 643, loss = 0.24235861\n",
            "Iteration 644, loss = 0.24217660\n",
            "Iteration 645, loss = 0.24199511\n",
            "Iteration 646, loss = 0.24181527\n",
            "Iteration 647, loss = 0.24163574\n",
            "Iteration 648, loss = 0.24145685\n",
            "Iteration 649, loss = 0.24127810\n",
            "Iteration 650, loss = 0.24109989\n",
            "Iteration 651, loss = 0.24092293\n",
            "Iteration 652, loss = 0.24074688\n",
            "Iteration 653, loss = 0.24057228\n",
            "Iteration 654, loss = 0.24039883\n",
            "Iteration 655, loss = 0.24022550\n",
            "Iteration 656, loss = 0.24005306\n",
            "Iteration 657, loss = 0.23988090\n",
            "Iteration 658, loss = 0.23970940\n",
            "Iteration 659, loss = 0.23953835\n",
            "Iteration 660, loss = 0.23936817\n",
            "Iteration 661, loss = 0.23919850\n",
            "Iteration 662, loss = 0.23902795\n",
            "Iteration 663, loss = 0.23885803\n",
            "Iteration 664, loss = 0.23868828\n",
            "Iteration 665, loss = 0.23851903\n",
            "Iteration 666, loss = 0.23835044\n",
            "Iteration 667, loss = 0.23818256\n",
            "Iteration 668, loss = 0.23801538\n",
            "Iteration 669, loss = 0.23784874\n",
            "Iteration 670, loss = 0.23768269\n",
            "Iteration 671, loss = 0.23751678\n",
            "Iteration 672, loss = 0.23735133\n",
            "Iteration 673, loss = 0.23718642\n",
            "Iteration 674, loss = 0.23702190\n",
            "Iteration 675, loss = 0.23685854\n",
            "Iteration 676, loss = 0.23669596\n",
            "Iteration 677, loss = 0.23653379\n",
            "Iteration 678, loss = 0.23637219\n",
            "Iteration 679, loss = 0.23621084\n",
            "Iteration 680, loss = 0.23605004\n",
            "Iteration 681, loss = 0.23588986\n",
            "Iteration 682, loss = 0.23573005\n",
            "Iteration 683, loss = 0.23557082\n",
            "Iteration 684, loss = 0.23541219\n",
            "Iteration 685, loss = 0.23525382\n",
            "Iteration 686, loss = 0.23509594\n",
            "Iteration 687, loss = 0.23493855\n",
            "Iteration 688, loss = 0.23478184\n",
            "Iteration 689, loss = 0.23462542\n",
            "Iteration 690, loss = 0.23446932\n",
            "Iteration 691, loss = 0.23431386\n",
            "Iteration 692, loss = 0.23415944\n",
            "Iteration 693, loss = 0.23400534\n",
            "Iteration 694, loss = 0.23385178\n",
            "Iteration 695, loss = 0.23369894\n",
            "Iteration 696, loss = 0.23354663\n",
            "Iteration 697, loss = 0.23339476\n",
            "Iteration 698, loss = 0.23324342\n",
            "Iteration 699, loss = 0.23309253\n",
            "Iteration 700, loss = 0.23294234\n",
            "Iteration 701, loss = 0.23279228\n",
            "Iteration 702, loss = 0.23264270\n",
            "Iteration 703, loss = 0.23249344\n",
            "Iteration 704, loss = 0.23234484\n",
            "Iteration 705, loss = 0.23219651\n",
            "Iteration 706, loss = 0.23204884\n",
            "Iteration 707, loss = 0.23190185\n",
            "Iteration 708, loss = 0.23175577\n",
            "Iteration 709, loss = 0.23160991\n",
            "Iteration 710, loss = 0.23146446\n",
            "Iteration 711, loss = 0.23131944\n",
            "Iteration 712, loss = 0.23117476\n",
            "Iteration 713, loss = 0.23103065\n",
            "Iteration 714, loss = 0.23088693\n",
            "Iteration 715, loss = 0.23074352\n",
            "Iteration 716, loss = 0.23060051\n",
            "Iteration 717, loss = 0.23045789\n",
            "Iteration 718, loss = 0.23031569\n",
            "Iteration 719, loss = 0.23017403\n",
            "Iteration 720, loss = 0.23003326\n",
            "Iteration 721, loss = 0.22989304\n",
            "Iteration 722, loss = 0.22975330\n",
            "Iteration 723, loss = 0.22961425\n",
            "Iteration 724, loss = 0.22947549\n",
            "Iteration 725, loss = 0.22933701\n",
            "Iteration 726, loss = 0.22919913\n",
            "Iteration 727, loss = 0.22906142\n",
            "Iteration 728, loss = 0.22892405\n",
            "Iteration 729, loss = 0.22878721\n",
            "Iteration 730, loss = 0.22865115\n",
            "Iteration 731, loss = 0.22851524\n",
            "Iteration 732, loss = 0.22837984\n",
            "Iteration 733, loss = 0.22824497\n",
            "Iteration 734, loss = 0.22811038\n",
            "Iteration 735, loss = 0.22797604\n",
            "Iteration 736, loss = 0.22784201\n",
            "Iteration 737, loss = 0.22770823\n",
            "Iteration 738, loss = 0.22757470\n",
            "Iteration 739, loss = 0.22744139\n",
            "Iteration 740, loss = 0.22730833\n",
            "Iteration 741, loss = 0.22717600\n",
            "Iteration 742, loss = 0.22704345\n",
            "Iteration 743, loss = 0.22691148\n",
            "Iteration 744, loss = 0.22677977\n",
            "Iteration 745, loss = 0.22664853\n",
            "Iteration 746, loss = 0.22651752\n",
            "Iteration 747, loss = 0.22638662\n",
            "Iteration 748, loss = 0.22625597\n",
            "Iteration 749, loss = 0.22612556\n",
            "Iteration 750, loss = 0.22599554\n",
            "Iteration 751, loss = 0.22586606\n",
            "Iteration 752, loss = 0.22573672\n",
            "Iteration 753, loss = 0.22560790\n",
            "Iteration 754, loss = 0.22547922\n",
            "Iteration 755, loss = 0.22535107\n",
            "Iteration 756, loss = 0.22522350\n",
            "Iteration 757, loss = 0.22509606\n",
            "Iteration 758, loss = 0.22496900\n",
            "Iteration 759, loss = 0.22484216\n",
            "Iteration 760, loss = 0.22471574\n",
            "Iteration 761, loss = 0.22458978\n",
            "Iteration 762, loss = 0.22446452\n",
            "Iteration 763, loss = 0.22433890\n",
            "Iteration 764, loss = 0.22421418\n",
            "Iteration 765, loss = 0.22408988\n",
            "Iteration 766, loss = 0.22396587\n",
            "Iteration 767, loss = 0.22384241\n",
            "Iteration 768, loss = 0.22371879\n",
            "Iteration 769, loss = 0.22359554\n",
            "Iteration 770, loss = 0.22347273\n",
            "Iteration 771, loss = 0.22335005\n",
            "Iteration 772, loss = 0.22322782\n",
            "Iteration 773, loss = 0.22310583\n",
            "Iteration 774, loss = 0.22298385\n",
            "Iteration 775, loss = 0.22286223\n",
            "Iteration 776, loss = 0.22274078\n",
            "Iteration 777, loss = 0.22261950\n",
            "Iteration 778, loss = 0.22249888\n",
            "Iteration 779, loss = 0.22237785\n",
            "Iteration 780, loss = 0.22225724\n",
            "Iteration 781, loss = 0.22213989\n",
            "Iteration 782, loss = 0.22202248\n",
            "Iteration 783, loss = 0.22190496\n",
            "Iteration 784, loss = 0.22178741\n",
            "Iteration 785, loss = 0.22166984\n",
            "Iteration 786, loss = 0.22155246\n",
            "Iteration 787, loss = 0.22143494\n",
            "Iteration 788, loss = 0.22131767\n",
            "Iteration 789, loss = 0.22120052\n",
            "Iteration 790, loss = 0.22108327\n",
            "Iteration 791, loss = 0.22096622\n",
            "Iteration 792, loss = 0.22084915\n",
            "Iteration 793, loss = 0.22073193\n",
            "Iteration 794, loss = 0.22061487\n",
            "Iteration 795, loss = 0.22049807\n",
            "Iteration 796, loss = 0.22038280\n",
            "Iteration 797, loss = 0.22026746\n",
            "Iteration 798, loss = 0.22015184\n",
            "Iteration 799, loss = 0.22003611\n",
            "Iteration 800, loss = 0.21992055\n",
            "Iteration 1, loss = 0.70708844\n",
            "Iteration 2, loss = 0.70647537\n",
            "Iteration 3, loss = 0.70570986\n",
            "Iteration 4, loss = 0.70477867\n",
            "Iteration 5, loss = 0.70369539\n",
            "Iteration 6, loss = 0.70252709\n",
            "Iteration 7, loss = 0.70132272\n",
            "Iteration 8, loss = 0.70012305\n",
            "Iteration 9, loss = 0.69889531\n",
            "Iteration 10, loss = 0.69779317\n",
            "Iteration 11, loss = 0.69668280\n",
            "Iteration 12, loss = 0.69558727\n",
            "Iteration 13, loss = 0.69466078\n",
            "Iteration 14, loss = 0.69376282\n",
            "Iteration 15, loss = 0.69291168\n",
            "Iteration 16, loss = 0.69218790\n",
            "Iteration 17, loss = 0.69166201\n",
            "Iteration 18, loss = 0.69120794\n",
            "Iteration 19, loss = 0.69085375\n",
            "Iteration 20, loss = 0.69058535\n",
            "Iteration 21, loss = 0.69032505\n",
            "Iteration 22, loss = 0.69007519\n",
            "Iteration 23, loss = 0.68987911\n",
            "Iteration 24, loss = 0.68974838\n",
            "Iteration 25, loss = 0.68966014\n",
            "Iteration 26, loss = 0.68958708\n",
            "Iteration 27, loss = 0.68952194\n",
            "Iteration 28, loss = 0.68946734\n",
            "Iteration 29, loss = 0.68941030\n",
            "Iteration 30, loss = 0.68935112\n",
            "Iteration 31, loss = 0.68929000\n",
            "Iteration 32, loss = 0.68922713\n",
            "Iteration 33, loss = 0.68916267\n",
            "Iteration 34, loss = 0.68909289\n",
            "Iteration 35, loss = 0.68902103\n",
            "Iteration 36, loss = 0.68894885\n",
            "Iteration 37, loss = 0.68887639\n",
            "Iteration 38, loss = 0.68880370\n",
            "Iteration 39, loss = 0.68873081\n",
            "Iteration 40, loss = 0.68865769\n",
            "Iteration 41, loss = 0.68858437\n",
            "Iteration 42, loss = 0.68851086\n",
            "Iteration 43, loss = 0.68843717\n",
            "Iteration 44, loss = 0.68836330\n",
            "Iteration 45, loss = 0.68828925\n",
            "Iteration 46, loss = 0.68821503\n",
            "Iteration 47, loss = 0.68814065\n",
            "Iteration 48, loss = 0.68806610\n",
            "Iteration 49, loss = 0.68799137\n",
            "Iteration 50, loss = 0.68791647\n",
            "Iteration 51, loss = 0.68784137\n",
            "Iteration 52, loss = 0.68776610\n",
            "Iteration 53, loss = 0.68769063\n",
            "Iteration 54, loss = 0.68761495\n",
            "Iteration 55, loss = 0.68753909\n",
            "Iteration 56, loss = 0.68746301\n",
            "Iteration 57, loss = 0.68738672\n",
            "Iteration 58, loss = 0.68731019\n",
            "Iteration 59, loss = 0.68723342\n",
            "Iteration 60, loss = 0.68715640\n",
            "Iteration 61, loss = 0.68707913\n",
            "Iteration 62, loss = 0.68700161\n",
            "Iteration 63, loss = 0.68692382\n",
            "Iteration 64, loss = 0.68684573\n",
            "Iteration 65, loss = 0.68676742\n",
            "Iteration 66, loss = 0.68668898\n",
            "Iteration 67, loss = 0.68661023\n",
            "Iteration 68, loss = 0.68653119\n",
            "Iteration 69, loss = 0.68645243\n",
            "Iteration 70, loss = 0.68637393\n",
            "Iteration 71, loss = 0.68629506\n",
            "Iteration 72, loss = 0.68621580\n",
            "Iteration 73, loss = 0.68613615\n",
            "Iteration 74, loss = 0.68605610\n",
            "Iteration 75, loss = 0.68597562\n",
            "Iteration 76, loss = 0.68589471\n",
            "Iteration 77, loss = 0.68581334\n",
            "Iteration 78, loss = 0.68573150\n",
            "Iteration 79, loss = 0.68564918\n",
            "Iteration 80, loss = 0.68556638\n",
            "Iteration 81, loss = 0.68548307\n",
            "Iteration 82, loss = 0.68539925\n",
            "Iteration 83, loss = 0.68531492\n",
            "Iteration 84, loss = 0.68523002\n",
            "Iteration 85, loss = 0.68514451\n",
            "Iteration 86, loss = 0.68505843\n",
            "Iteration 87, loss = 0.68497176\n",
            "Iteration 88, loss = 0.68488450\n",
            "Iteration 89, loss = 0.68479663\n",
            "Iteration 90, loss = 0.68470816\n",
            "Iteration 91, loss = 0.68461905\n",
            "Iteration 92, loss = 0.68452938\n",
            "Iteration 93, loss = 0.68443922\n",
            "Iteration 94, loss = 0.68434842\n",
            "Iteration 95, loss = 0.68425697\n",
            "Iteration 96, loss = 0.68416512\n",
            "Iteration 97, loss = 0.68407249\n",
            "Iteration 98, loss = 0.68397919\n",
            "Iteration 99, loss = 0.68388516\n",
            "Iteration 100, loss = 0.68379058\n",
            "Iteration 101, loss = 0.68369531\n",
            "Iteration 102, loss = 0.68359927\n",
            "Iteration 103, loss = 0.68350246\n",
            "Iteration 104, loss = 0.68340485\n",
            "Iteration 105, loss = 0.68330644\n",
            "Iteration 106, loss = 0.68320721\n",
            "Iteration 107, loss = 0.68310722\n",
            "Iteration 108, loss = 0.68300636\n",
            "Iteration 109, loss = 0.68290481\n",
            "Iteration 110, loss = 0.68280242\n",
            "Iteration 111, loss = 0.68269917\n",
            "Iteration 112, loss = 0.68259508\n",
            "Iteration 113, loss = 0.68249022\n",
            "Iteration 114, loss = 0.68238432\n",
            "Iteration 115, loss = 0.68227785\n",
            "Iteration 116, loss = 0.68217052\n",
            "Iteration 117, loss = 0.68206213\n",
            "Iteration 118, loss = 0.68195265\n",
            "Iteration 119, loss = 0.68184216\n",
            "Iteration 120, loss = 0.68173068\n",
            "Iteration 121, loss = 0.68161796\n",
            "Iteration 122, loss = 0.68150408\n",
            "Iteration 123, loss = 0.68138941\n",
            "Iteration 124, loss = 0.68127362\n",
            "Iteration 125, loss = 0.68115673\n",
            "Iteration 126, loss = 0.68103873\n",
            "Iteration 127, loss = 0.68091972\n",
            "Iteration 128, loss = 0.68079938\n",
            "Iteration 129, loss = 0.68067794\n",
            "Iteration 130, loss = 0.68055536\n",
            "Iteration 131, loss = 0.68043151\n",
            "Iteration 132, loss = 0.68030643\n",
            "Iteration 133, loss = 0.68018008\n",
            "Iteration 134, loss = 0.68005252\n",
            "Iteration 135, loss = 0.67992356\n",
            "Iteration 136, loss = 0.67979327\n",
            "Iteration 137, loss = 0.67966163\n",
            "Iteration 138, loss = 0.67952866\n",
            "Iteration 139, loss = 0.67939428\n",
            "Iteration 140, loss = 0.67925850\n",
            "Iteration 141, loss = 0.67912132\n",
            "Iteration 142, loss = 0.67898281\n",
            "Iteration 143, loss = 0.67884291\n",
            "Iteration 144, loss = 0.67870152\n",
            "Iteration 145, loss = 0.67855870\n",
            "Iteration 146, loss = 0.67841481\n",
            "Iteration 147, loss = 0.67826949\n",
            "Iteration 148, loss = 0.67812269\n",
            "Iteration 149, loss = 0.67797440\n",
            "Iteration 150, loss = 0.67782455\n",
            "Iteration 151, loss = 0.67767312\n",
            "Iteration 152, loss = 0.67752014\n",
            "Iteration 153, loss = 0.67736562\n",
            "Iteration 154, loss = 0.67720940\n",
            "Iteration 155, loss = 0.67705151\n",
            "Iteration 156, loss = 0.67689202\n",
            "Iteration 157, loss = 0.67673071\n",
            "Iteration 158, loss = 0.67656771\n",
            "Iteration 159, loss = 0.67640303\n",
            "Iteration 160, loss = 0.67623647\n",
            "Iteration 161, loss = 0.67606814\n",
            "Iteration 162, loss = 0.67589798\n",
            "Iteration 163, loss = 0.67572637\n",
            "Iteration 164, loss = 0.67555293\n",
            "Iteration 165, loss = 0.67537762\n",
            "Iteration 166, loss = 0.67520042\n",
            "Iteration 167, loss = 0.67502136\n",
            "Iteration 168, loss = 0.67484029\n",
            "Iteration 169, loss = 0.67465718\n",
            "Iteration 170, loss = 0.67447205\n",
            "Iteration 171, loss = 0.67428508\n",
            "Iteration 172, loss = 0.67409579\n",
            "Iteration 173, loss = 0.67390451\n",
            "Iteration 174, loss = 0.67371112\n",
            "Iteration 175, loss = 0.67351557\n",
            "Iteration 176, loss = 0.67331786\n",
            "Iteration 177, loss = 0.67311796\n",
            "Iteration 178, loss = 0.67291585\n",
            "Iteration 179, loss = 0.67271157\n",
            "Iteration 180, loss = 0.67250474\n",
            "Iteration 181, loss = 0.67229582\n",
            "Iteration 182, loss = 0.67208446\n",
            "Iteration 183, loss = 0.67187062\n",
            "Iteration 184, loss = 0.67165437\n",
            "Iteration 185, loss = 0.67143638\n",
            "Iteration 186, loss = 0.67121622\n",
            "Iteration 187, loss = 0.67099365\n",
            "Iteration 188, loss = 0.67076864\n",
            "Iteration 189, loss = 0.67054122\n",
            "Iteration 190, loss = 0.67031129\n",
            "Iteration 191, loss = 0.67007884\n",
            "Iteration 192, loss = 0.66984394\n",
            "Iteration 193, loss = 0.66960651\n",
            "Iteration 194, loss = 0.66936649\n",
            "Iteration 195, loss = 0.66912389\n",
            "Iteration 196, loss = 0.66887868\n",
            "Iteration 197, loss = 0.66863082\n",
            "Iteration 198, loss = 0.66838030\n",
            "Iteration 199, loss = 0.66812704\n",
            "Iteration 200, loss = 0.66787099\n",
            "Iteration 201, loss = 0.66761224\n",
            "Iteration 202, loss = 0.66735061\n",
            "Iteration 203, loss = 0.66708613\n",
            "Iteration 204, loss = 0.66681876\n",
            "Iteration 205, loss = 0.66654839\n",
            "Iteration 206, loss = 0.66627496\n",
            "Iteration 207, loss = 0.66599848\n",
            "Iteration 208, loss = 0.66571909\n",
            "Iteration 209, loss = 0.66543661\n",
            "Iteration 210, loss = 0.66515107\n",
            "Iteration 211, loss = 0.66486240\n",
            "Iteration 212, loss = 0.66457046\n",
            "Iteration 213, loss = 0.66427521\n",
            "Iteration 214, loss = 0.66397672\n",
            "Iteration 215, loss = 0.66367502\n",
            "Iteration 216, loss = 0.66337005\n",
            "Iteration 217, loss = 0.66306171\n",
            "Iteration 218, loss = 0.66275004\n",
            "Iteration 219, loss = 0.66243495\n",
            "Iteration 220, loss = 0.66211665\n",
            "Iteration 221, loss = 0.66179464\n",
            "Iteration 222, loss = 0.66146925\n",
            "Iteration 223, loss = 0.66114046\n",
            "Iteration 224, loss = 0.66080808\n",
            "Iteration 225, loss = 0.66047188\n",
            "Iteration 226, loss = 0.66013195\n",
            "Iteration 227, loss = 0.65978836\n",
            "Iteration 228, loss = 0.65944124\n",
            "Iteration 229, loss = 0.65909019\n",
            "Iteration 230, loss = 0.65873567\n",
            "Iteration 231, loss = 0.65837726\n",
            "Iteration 232, loss = 0.65801498\n",
            "Iteration 233, loss = 0.65764886\n",
            "Iteration 234, loss = 0.65727890\n",
            "Iteration 235, loss = 0.65690522\n",
            "Iteration 236, loss = 0.65652756\n",
            "Iteration 237, loss = 0.65614671\n",
            "Iteration 238, loss = 0.65576253\n",
            "Iteration 239, loss = 0.65537441\n",
            "Iteration 240, loss = 0.65498229\n",
            "Iteration 241, loss = 0.65458621\n",
            "Iteration 242, loss = 0.65418626\n",
            "Iteration 243, loss = 0.65378217\n",
            "Iteration 244, loss = 0.65337423\n",
            "Iteration 245, loss = 0.65296239\n",
            "Iteration 246, loss = 0.65254640\n",
            "Iteration 247, loss = 0.65212626\n",
            "Iteration 248, loss = 0.65170257\n",
            "Iteration 249, loss = 0.65127531\n",
            "Iteration 250, loss = 0.65084383\n",
            "Iteration 251, loss = 0.65040804\n",
            "Iteration 252, loss = 0.64996803\n",
            "Iteration 253, loss = 0.64952397\n",
            "Iteration 254, loss = 0.64907544\n",
            "Iteration 255, loss = 0.64862265\n",
            "Iteration 256, loss = 0.64816572\n",
            "Iteration 257, loss = 0.64770458\n",
            "Iteration 258, loss = 0.64723913\n",
            "Iteration 259, loss = 0.64676930\n",
            "Iteration 260, loss = 0.64629511\n",
            "Iteration 261, loss = 0.64581656\n",
            "Iteration 262, loss = 0.64533340\n",
            "Iteration 263, loss = 0.64484594\n",
            "Iteration 264, loss = 0.64435412\n",
            "Iteration 265, loss = 0.64385763\n",
            "Iteration 266, loss = 0.64335646\n",
            "Iteration 267, loss = 0.64285063\n",
            "Iteration 268, loss = 0.64234012\n",
            "Iteration 269, loss = 0.64182538\n",
            "Iteration 270, loss = 0.64130749\n",
            "Iteration 271, loss = 0.64078639\n",
            "Iteration 272, loss = 0.64026084\n",
            "Iteration 273, loss = 0.63973087\n",
            "Iteration 274, loss = 0.63919603\n",
            "Iteration 275, loss = 0.63865657\n",
            "Iteration 276, loss = 0.63811256\n",
            "Iteration 277, loss = 0.63756382\n",
            "Iteration 278, loss = 0.63701033\n",
            "Iteration 279, loss = 0.63645200\n",
            "Iteration 280, loss = 0.63588908\n",
            "Iteration 281, loss = 0.63532148\n",
            "Iteration 282, loss = 0.63474937\n",
            "Iteration 283, loss = 0.63417271\n",
            "Iteration 284, loss = 0.63359157\n",
            "Iteration 285, loss = 0.63300569\n",
            "Iteration 286, loss = 0.63241496\n",
            "Iteration 287, loss = 0.63181968\n",
            "Iteration 288, loss = 0.63122085\n",
            "Iteration 289, loss = 0.63061885\n",
            "Iteration 290, loss = 0.63001438\n",
            "Iteration 291, loss = 0.62940587\n",
            "Iteration 292, loss = 0.62879338\n",
            "Iteration 293, loss = 0.62817787\n",
            "Iteration 294, loss = 0.62756575\n",
            "Iteration 295, loss = 0.62695062\n",
            "Iteration 296, loss = 0.62633187\n",
            "Iteration 297, loss = 0.62570929\n",
            "Iteration 298, loss = 0.62508309\n",
            "Iteration 299, loss = 0.62445306\n",
            "Iteration 300, loss = 0.62381950\n",
            "Iteration 301, loss = 0.62318202\n",
            "Iteration 302, loss = 0.62254061\n",
            "Iteration 303, loss = 0.62189588\n",
            "Iteration 304, loss = 0.62124772\n",
            "Iteration 305, loss = 0.62059588\n",
            "Iteration 306, loss = 0.61993965\n",
            "Iteration 307, loss = 0.61927941\n",
            "Iteration 308, loss = 0.61861540\n",
            "Iteration 309, loss = 0.61794757\n",
            "Iteration 310, loss = 0.61727607\n",
            "Iteration 311, loss = 0.61660218\n",
            "Iteration 312, loss = 0.61592811\n",
            "Iteration 313, loss = 0.61525189\n",
            "Iteration 314, loss = 0.61457349\n",
            "Iteration 315, loss = 0.61389179\n",
            "Iteration 316, loss = 0.61320762\n",
            "Iteration 317, loss = 0.61252159\n",
            "Iteration 318, loss = 0.61183252\n",
            "Iteration 319, loss = 0.61114090\n",
            "Iteration 320, loss = 0.61045378\n",
            "Iteration 321, loss = 0.60976422\n",
            "Iteration 322, loss = 0.60907223\n",
            "Iteration 323, loss = 0.60837760\n",
            "Iteration 324, loss = 0.60768162\n",
            "Iteration 325, loss = 0.60698486\n",
            "Iteration 326, loss = 0.60628581\n",
            "Iteration 327, loss = 0.60558610\n",
            "Iteration 328, loss = 0.60488438\n",
            "Iteration 329, loss = 0.60418148\n",
            "Iteration 330, loss = 0.60347826\n",
            "Iteration 331, loss = 0.60277389\n",
            "Iteration 332, loss = 0.60206789\n",
            "Iteration 333, loss = 0.60136455\n",
            "Iteration 334, loss = 0.60066196\n",
            "Iteration 335, loss = 0.59995915\n",
            "Iteration 336, loss = 0.59925526\n",
            "Iteration 337, loss = 0.59855028\n",
            "Iteration 338, loss = 0.59784420\n",
            "Iteration 339, loss = 0.59713705\n",
            "Iteration 340, loss = 0.59642878\n",
            "Iteration 341, loss = 0.59571947\n",
            "Iteration 342, loss = 0.59500924\n",
            "Iteration 343, loss = 0.59429777\n",
            "Iteration 344, loss = 0.59358512\n",
            "Iteration 345, loss = 0.59287099\n",
            "Iteration 346, loss = 0.59215584\n",
            "Iteration 347, loss = 0.59143952\n",
            "Iteration 348, loss = 0.59072421\n",
            "Iteration 349, loss = 0.59001022\n",
            "Iteration 350, loss = 0.58929673\n",
            "Iteration 351, loss = 0.58858272\n",
            "Iteration 352, loss = 0.58786840\n",
            "Iteration 353, loss = 0.58715392\n",
            "Iteration 354, loss = 0.58643926\n",
            "Iteration 355, loss = 0.58572433\n",
            "Iteration 356, loss = 0.58500939\n",
            "Iteration 357, loss = 0.58429461\n",
            "Iteration 358, loss = 0.58358160\n",
            "Iteration 359, loss = 0.58286929\n",
            "Iteration 360, loss = 0.58215717\n",
            "Iteration 361, loss = 0.58144568\n",
            "Iteration 362, loss = 0.58073451\n",
            "Iteration 363, loss = 0.58002355\n",
            "Iteration 364, loss = 0.57931251\n",
            "Iteration 365, loss = 0.57860147\n",
            "Iteration 366, loss = 0.57789071\n",
            "Iteration 367, loss = 0.57718041\n",
            "Iteration 368, loss = 0.57647009\n",
            "Iteration 369, loss = 0.57576004\n",
            "Iteration 370, loss = 0.57505017\n",
            "Iteration 371, loss = 0.57434048\n",
            "Iteration 372, loss = 0.57362880\n",
            "Iteration 373, loss = 0.57291725\n",
            "Iteration 374, loss = 0.57220580\n",
            "Iteration 375, loss = 0.57149440\n",
            "Iteration 376, loss = 0.57078326\n",
            "Iteration 377, loss = 0.57007261\n",
            "Iteration 378, loss = 0.56936222\n",
            "Iteration 379, loss = 0.56865509\n",
            "Iteration 380, loss = 0.56794877\n",
            "Iteration 381, loss = 0.56724323\n",
            "Iteration 382, loss = 0.56653804\n",
            "Iteration 383, loss = 0.56583714\n",
            "Iteration 384, loss = 0.56513871\n",
            "Iteration 385, loss = 0.56444084\n",
            "Iteration 386, loss = 0.56374549\n",
            "Iteration 387, loss = 0.56305105\n",
            "Iteration 388, loss = 0.56235952\n",
            "Iteration 389, loss = 0.56167058\n",
            "Iteration 390, loss = 0.56098430\n",
            "Iteration 391, loss = 0.56029917\n",
            "Iteration 392, loss = 0.55961531\n",
            "Iteration 393, loss = 0.55893265\n",
            "Iteration 394, loss = 0.55825113\n",
            "Iteration 395, loss = 0.55757071\n",
            "Iteration 396, loss = 0.55689148\n",
            "Iteration 397, loss = 0.55621338\n",
            "Iteration 398, loss = 0.55553796\n",
            "Iteration 399, loss = 0.55486675\n",
            "Iteration 400, loss = 0.55419707\n",
            "Iteration 401, loss = 0.55352876\n",
            "Iteration 402, loss = 0.55286180\n",
            "Iteration 403, loss = 0.55219614\n",
            "Iteration 404, loss = 0.55153261\n",
            "Iteration 405, loss = 0.55087347\n",
            "Iteration 406, loss = 0.55021570\n",
            "Iteration 407, loss = 0.54955923\n",
            "Iteration 408, loss = 0.54890379\n",
            "Iteration 409, loss = 0.54825075\n",
            "Iteration 410, loss = 0.54760487\n",
            "Iteration 411, loss = 0.54696150\n",
            "Iteration 412, loss = 0.54632018\n",
            "Iteration 413, loss = 0.54568088\n",
            "Iteration 414, loss = 0.54504338\n",
            "Iteration 415, loss = 0.54440754\n",
            "Iteration 416, loss = 0.54377364\n",
            "Iteration 417, loss = 0.54314152\n",
            "Iteration 418, loss = 0.54251110\n",
            "Iteration 419, loss = 0.54188229\n",
            "Iteration 420, loss = 0.54125489\n",
            "Iteration 421, loss = 0.54063122\n",
            "Iteration 422, loss = 0.54001154\n",
            "Iteration 423, loss = 0.53939583\n",
            "Iteration 424, loss = 0.53878465\n",
            "Iteration 425, loss = 0.53817987\n",
            "Iteration 426, loss = 0.53757711\n",
            "Iteration 427, loss = 0.53697664\n",
            "Iteration 428, loss = 0.53637813\n",
            "Iteration 429, loss = 0.53578143\n",
            "Iteration 430, loss = 0.53518672\n",
            "Iteration 431, loss = 0.53459368\n",
            "Iteration 432, loss = 0.53400243\n",
            "Iteration 433, loss = 0.53341304\n",
            "Iteration 434, loss = 0.53282513\n",
            "Iteration 435, loss = 0.53223892\n",
            "Iteration 436, loss = 0.53165439\n",
            "Iteration 437, loss = 0.53107294\n",
            "Iteration 438, loss = 0.53049403\n",
            "Iteration 439, loss = 0.52991644\n",
            "Iteration 440, loss = 0.52934019\n",
            "Iteration 441, loss = 0.52876528\n",
            "Iteration 442, loss = 0.52819157\n",
            "Iteration 443, loss = 0.52761917\n",
            "Iteration 444, loss = 0.52704810\n",
            "Iteration 445, loss = 0.52647924\n",
            "Iteration 446, loss = 0.52591257\n",
            "Iteration 447, loss = 0.52534786\n",
            "Iteration 448, loss = 0.52478173\n",
            "Iteration 449, loss = 0.52421341\n",
            "Iteration 450, loss = 0.52364556\n",
            "Iteration 451, loss = 0.52307837\n",
            "Iteration 452, loss = 0.52251387\n",
            "Iteration 453, loss = 0.52195183\n",
            "Iteration 454, loss = 0.52139079\n",
            "Iteration 455, loss = 0.52083072\n",
            "Iteration 456, loss = 0.52027182\n",
            "Iteration 457, loss = 0.51971408\n",
            "Iteration 458, loss = 0.51915940\n",
            "Iteration 459, loss = 0.51860936\n",
            "Iteration 460, loss = 0.51806469\n",
            "Iteration 461, loss = 0.51752245\n",
            "Iteration 462, loss = 0.51698186\n",
            "Iteration 463, loss = 0.51644281\n",
            "Iteration 464, loss = 0.51590524\n",
            "Iteration 465, loss = 0.51536909\n",
            "Iteration 466, loss = 0.51483419\n",
            "Iteration 467, loss = 0.51430055\n",
            "Iteration 468, loss = 0.51376785\n",
            "Iteration 469, loss = 0.51323617\n",
            "Iteration 470, loss = 0.51270969\n",
            "Iteration 471, loss = 0.51218752\n",
            "Iteration 472, loss = 0.51166759\n",
            "Iteration 473, loss = 0.51114906\n",
            "Iteration 474, loss = 0.51063171\n",
            "Iteration 475, loss = 0.51011565\n",
            "Iteration 476, loss = 0.50960079\n",
            "Iteration 477, loss = 0.50908705\n",
            "Iteration 478, loss = 0.50857430\n",
            "Iteration 479, loss = 0.50806257\n",
            "Iteration 480, loss = 0.50755194\n",
            "Iteration 481, loss = 0.50704242\n",
            "Iteration 482, loss = 0.50653401\n",
            "Iteration 483, loss = 0.50602675\n",
            "Iteration 484, loss = 0.50552087\n",
            "Iteration 485, loss = 0.50501614\n",
            "Iteration 486, loss = 0.50451271\n",
            "Iteration 487, loss = 0.50401016\n",
            "Iteration 488, loss = 0.50350689\n",
            "Iteration 489, loss = 0.50300368\n",
            "Iteration 490, loss = 0.50250109\n",
            "Iteration 491, loss = 0.50200515\n",
            "Iteration 492, loss = 0.50151270\n",
            "Iteration 493, loss = 0.50102146\n",
            "Iteration 494, loss = 0.50053137\n",
            "Iteration 495, loss = 0.50004225\n",
            "Iteration 496, loss = 0.49955449\n",
            "Iteration 497, loss = 0.49906842\n",
            "Iteration 498, loss = 0.49858360\n",
            "Iteration 499, loss = 0.49809986\n",
            "Iteration 500, loss = 0.49761714\n",
            "Iteration 501, loss = 0.49713503\n",
            "Iteration 502, loss = 0.49665425\n",
            "Iteration 503, loss = 0.49617771\n",
            "Iteration 504, loss = 0.49570213\n",
            "Iteration 505, loss = 0.49522763\n",
            "Iteration 506, loss = 0.49475413\n",
            "Iteration 507, loss = 0.49428318\n",
            "Iteration 508, loss = 0.49381697\n",
            "Iteration 509, loss = 0.49335199\n",
            "Iteration 510, loss = 0.49288812\n",
            "Iteration 511, loss = 0.49242531\n",
            "Iteration 512, loss = 0.49196345\n",
            "Iteration 513, loss = 0.49150341\n",
            "Iteration 514, loss = 0.49104431\n",
            "Iteration 515, loss = 0.49058624\n",
            "Iteration 516, loss = 0.49012939\n",
            "Iteration 517, loss = 0.48967350\n",
            "Iteration 518, loss = 0.48921829\n",
            "Iteration 519, loss = 0.48876395\n",
            "Iteration 520, loss = 0.48831434\n",
            "Iteration 521, loss = 0.48786679\n",
            "Iteration 522, loss = 0.48742067\n",
            "Iteration 523, loss = 0.48697565\n",
            "Iteration 524, loss = 0.48653171\n",
            "Iteration 525, loss = 0.48608874\n",
            "Iteration 526, loss = 0.48564624\n",
            "Iteration 527, loss = 0.48520458\n",
            "Iteration 528, loss = 0.48476385\n",
            "Iteration 529, loss = 0.48432424\n",
            "Iteration 530, loss = 0.48388626\n",
            "Iteration 531, loss = 0.48344925\n",
            "Iteration 532, loss = 0.48301316\n",
            "Iteration 533, loss = 0.48257796\n",
            "Iteration 534, loss = 0.48214366\n",
            "Iteration 535, loss = 0.48171028\n",
            "Iteration 536, loss = 0.48127770\n",
            "Iteration 537, loss = 0.48084593\n",
            "Iteration 538, loss = 0.48041519\n",
            "Iteration 539, loss = 0.47998546\n",
            "Iteration 540, loss = 0.47955610\n",
            "Iteration 541, loss = 0.47912645\n",
            "Iteration 542, loss = 0.47869691\n",
            "Iteration 543, loss = 0.47826812\n",
            "Iteration 544, loss = 0.47784038\n",
            "Iteration 545, loss = 0.47741409\n",
            "Iteration 546, loss = 0.47699641\n",
            "Iteration 547, loss = 0.47657985\n",
            "Iteration 548, loss = 0.47616177\n",
            "Iteration 549, loss = 0.47574242\n",
            "Iteration 550, loss = 0.47532504\n",
            "Iteration 551, loss = 0.47490945\n",
            "Iteration 552, loss = 0.47449447\n",
            "Iteration 553, loss = 0.47407775\n",
            "Iteration 554, loss = 0.47366070\n",
            "Iteration 555, loss = 0.47324392\n",
            "Iteration 556, loss = 0.47282736\n",
            "Iteration 557, loss = 0.47241107\n",
            "Iteration 558, loss = 0.47199522\n",
            "Iteration 559, loss = 0.47157947\n",
            "Iteration 560, loss = 0.47116406\n",
            "Iteration 561, loss = 0.47074913\n",
            "Iteration 562, loss = 0.47033476\n",
            "Iteration 563, loss = 0.46992084\n",
            "Iteration 564, loss = 0.46950717\n",
            "Iteration 565, loss = 0.46909184\n",
            "Iteration 566, loss = 0.46867660\n",
            "Iteration 567, loss = 0.46826172\n",
            "Iteration 568, loss = 0.46784847\n",
            "Iteration 569, loss = 0.46743839\n",
            "Iteration 570, loss = 0.46703004\n",
            "Iteration 571, loss = 0.46662136\n",
            "Iteration 572, loss = 0.46621337\n",
            "Iteration 573, loss = 0.46580590\n",
            "Iteration 574, loss = 0.46540151\n",
            "Iteration 575, loss = 0.46500004\n",
            "Iteration 576, loss = 0.46459964\n",
            "Iteration 577, loss = 0.46420013\n",
            "Iteration 578, loss = 0.46380109\n",
            "Iteration 579, loss = 0.46340282\n",
            "Iteration 580, loss = 0.46300542\n",
            "Iteration 581, loss = 0.46260886\n",
            "Iteration 582, loss = 0.46221137\n",
            "Iteration 583, loss = 0.46180677\n",
            "Iteration 584, loss = 0.46140074\n",
            "Iteration 585, loss = 0.46099255\n",
            "Iteration 586, loss = 0.46058185\n",
            "Iteration 587, loss = 0.46017064\n",
            "Iteration 588, loss = 0.45975273\n",
            "Iteration 589, loss = 0.45932928\n",
            "Iteration 590, loss = 0.45890227\n",
            "Iteration 591, loss = 0.45847459\n",
            "Iteration 592, loss = 0.45804565\n",
            "Iteration 593, loss = 0.45761561\n",
            "Iteration 594, loss = 0.45718487\n",
            "Iteration 595, loss = 0.45675338\n",
            "Iteration 596, loss = 0.45632122\n",
            "Iteration 597, loss = 0.45588819\n",
            "Iteration 598, loss = 0.45544652\n",
            "Iteration 599, loss = 0.45499192\n",
            "Iteration 600, loss = 0.45452870\n",
            "Iteration 601, loss = 0.45405864\n",
            "Iteration 602, loss = 0.45358732\n",
            "Iteration 603, loss = 0.45311428\n",
            "Iteration 604, loss = 0.45264231\n",
            "Iteration 605, loss = 0.45218138\n",
            "Iteration 606, loss = 0.45171930\n",
            "Iteration 607, loss = 0.45126295\n",
            "Iteration 608, loss = 0.45080321\n",
            "Iteration 609, loss = 0.45033878\n",
            "Iteration 610, loss = 0.44987650\n",
            "Iteration 611, loss = 0.44941481\n",
            "Iteration 612, loss = 0.44895476\n",
            "Iteration 613, loss = 0.44849838\n",
            "Iteration 614, loss = 0.44804289\n",
            "Iteration 615, loss = 0.44758855\n",
            "Iteration 616, loss = 0.44714199\n",
            "Iteration 617, loss = 0.44669197\n",
            "Iteration 618, loss = 0.44623977\n",
            "Iteration 619, loss = 0.44579566\n",
            "Iteration 620, loss = 0.44535533\n",
            "Iteration 621, loss = 0.44491191\n",
            "Iteration 622, loss = 0.44447872\n",
            "Iteration 623, loss = 0.44404123\n",
            "Iteration 624, loss = 0.44361015\n",
            "Iteration 625, loss = 0.44318083\n",
            "Iteration 626, loss = 0.44275955\n",
            "Iteration 627, loss = 0.44234041\n",
            "Iteration 628, loss = 0.44191620\n",
            "Iteration 629, loss = 0.44149145\n",
            "Iteration 630, loss = 0.44106716\n",
            "Iteration 631, loss = 0.44065249\n",
            "Iteration 632, loss = 0.44023409\n",
            "Iteration 633, loss = 0.43981167\n",
            "Iteration 634, loss = 0.43939013\n",
            "Iteration 635, loss = 0.43896983\n",
            "Iteration 636, loss = 0.43855088\n",
            "Iteration 637, loss = 0.43813275\n",
            "Iteration 638, loss = 0.43771564\n",
            "Iteration 639, loss = 0.43729871\n",
            "Iteration 640, loss = 0.43687864\n",
            "Iteration 641, loss = 0.43645696\n",
            "Iteration 642, loss = 0.43603430\n",
            "Iteration 643, loss = 0.43561233\n",
            "Iteration 644, loss = 0.43519170\n",
            "Iteration 645, loss = 0.43477204\n",
            "Iteration 646, loss = 0.43435328\n",
            "Iteration 647, loss = 0.43393563\n",
            "Iteration 648, loss = 0.43351800\n",
            "Iteration 649, loss = 0.43309926\n",
            "Iteration 650, loss = 0.43265837\n",
            "Iteration 651, loss = 0.43221239\n",
            "Iteration 652, loss = 0.43175623\n",
            "Iteration 653, loss = 0.43129608\n",
            "Iteration 654, loss = 0.43083347\n",
            "Iteration 655, loss = 0.43036462\n",
            "Iteration 656, loss = 0.42988202\n",
            "Iteration 657, loss = 0.42939417\n",
            "Iteration 658, loss = 0.42890549\n",
            "Iteration 659, loss = 0.42841630\n",
            "Iteration 660, loss = 0.42792314\n",
            "Iteration 661, loss = 0.42741995\n",
            "Iteration 662, loss = 0.42691177\n",
            "Iteration 663, loss = 0.42639587\n",
            "Iteration 664, loss = 0.42587282\n",
            "Iteration 665, loss = 0.42534855\n",
            "Iteration 666, loss = 0.42482408\n",
            "Iteration 667, loss = 0.42428683\n",
            "Iteration 668, loss = 0.42373504\n",
            "Iteration 669, loss = 0.42317579\n",
            "Iteration 670, loss = 0.42260085\n",
            "Iteration 671, loss = 0.42200848\n",
            "Iteration 672, loss = 0.42139591\n",
            "Iteration 673, loss = 0.42074132\n",
            "Iteration 674, loss = 0.42005751\n",
            "Iteration 675, loss = 0.41935386\n",
            "Iteration 676, loss = 0.41863890\n",
            "Iteration 677, loss = 0.41792335\n",
            "Iteration 678, loss = 0.41719728\n",
            "Iteration 679, loss = 0.41646225\n",
            "Iteration 680, loss = 0.41571873\n",
            "Iteration 681, loss = 0.41496762\n",
            "Iteration 682, loss = 0.41421123\n",
            "Iteration 683, loss = 0.41344028\n",
            "Iteration 684, loss = 0.41267384\n",
            "Iteration 685, loss = 0.41189656\n",
            "Iteration 686, loss = 0.41110007\n",
            "Iteration 687, loss = 0.41029233\n",
            "Iteration 688, loss = 0.40946594\n",
            "Iteration 689, loss = 0.40861198\n",
            "Iteration 690, loss = 0.40775144\n",
            "Iteration 691, loss = 0.40686851\n",
            "Iteration 692, loss = 0.40596216\n",
            "Iteration 693, loss = 0.40504575\n",
            "Iteration 694, loss = 0.40412286\n",
            "Iteration 695, loss = 0.40319703\n",
            "Iteration 696, loss = 0.40227073\n",
            "Iteration 697, loss = 0.40134581\n",
            "Iteration 698, loss = 0.40042515\n",
            "Iteration 699, loss = 0.39947153\n",
            "Iteration 700, loss = 0.39847892\n",
            "Iteration 701, loss = 0.39743862\n",
            "Iteration 702, loss = 0.39634653\n",
            "Iteration 703, loss = 0.39521900\n",
            "Iteration 704, loss = 0.39408330\n",
            "Iteration 705, loss = 0.39294320\n",
            "Iteration 706, loss = 0.39179314\n",
            "Iteration 707, loss = 0.39059477\n",
            "Iteration 708, loss = 0.38934513\n",
            "Iteration 709, loss = 0.38807826\n",
            "Iteration 710, loss = 0.38674024\n",
            "Iteration 711, loss = 0.38535165\n",
            "Iteration 712, loss = 0.38393092\n",
            "Iteration 713, loss = 0.38247534\n",
            "Iteration 714, loss = 0.38102265\n",
            "Iteration 715, loss = 0.37958655\n",
            "Iteration 716, loss = 0.37816677\n",
            "Iteration 717, loss = 0.37676218\n",
            "Iteration 718, loss = 0.37537586\n",
            "Iteration 719, loss = 0.37400668\n",
            "Iteration 720, loss = 0.37265620\n",
            "Iteration 721, loss = 0.37132797\n",
            "Iteration 722, loss = 0.36999399\n",
            "Iteration 723, loss = 0.36867843\n",
            "Iteration 724, loss = 0.36739669\n",
            "Iteration 725, loss = 0.36613615\n",
            "Iteration 726, loss = 0.36489700\n",
            "Iteration 727, loss = 0.36367961\n",
            "Iteration 728, loss = 0.36248909\n",
            "Iteration 729, loss = 0.36132243\n",
            "Iteration 730, loss = 0.36018049\n",
            "Iteration 731, loss = 0.35906231\n",
            "Iteration 732, loss = 0.35796395\n",
            "Iteration 733, loss = 0.35688386\n",
            "Iteration 734, loss = 0.35582321\n",
            "Iteration 735, loss = 0.35478097\n",
            "Iteration 736, loss = 0.35375879\n",
            "Iteration 737, loss = 0.35276316\n",
            "Iteration 738, loss = 0.35174098\n",
            "Iteration 739, loss = 0.35072009\n",
            "Iteration 740, loss = 0.34971410\n",
            "Iteration 741, loss = 0.34872206\n",
            "Iteration 742, loss = 0.34774456\n",
            "Iteration 743, loss = 0.34682111\n",
            "Iteration 744, loss = 0.34591307\n",
            "Iteration 745, loss = 0.34502058\n",
            "Iteration 746, loss = 0.34414423\n",
            "Iteration 747, loss = 0.34328428\n",
            "Iteration 748, loss = 0.34243927\n",
            "Iteration 749, loss = 0.34160689\n",
            "Iteration 750, loss = 0.34078643\n",
            "Iteration 751, loss = 0.33997845\n",
            "Iteration 752, loss = 0.33918273\n",
            "Iteration 753, loss = 0.33839950\n",
            "Iteration 754, loss = 0.33762806\n",
            "Iteration 755, loss = 0.33686715\n",
            "Iteration 756, loss = 0.33611696\n",
            "Iteration 757, loss = 0.33537659\n",
            "Iteration 758, loss = 0.33464618\n",
            "Iteration 759, loss = 0.33392573\n",
            "Iteration 760, loss = 0.33321481\n",
            "Iteration 761, loss = 0.33251308\n",
            "Iteration 762, loss = 0.33182068\n",
            "Iteration 763, loss = 0.33113775\n",
            "Iteration 764, loss = 0.33046563\n",
            "Iteration 765, loss = 0.32980215\n",
            "Iteration 766, loss = 0.32914742\n",
            "Iteration 767, loss = 0.32850114\n",
            "Iteration 768, loss = 0.32786259\n",
            "Iteration 769, loss = 0.32723191\n",
            "Iteration 770, loss = 0.32660869\n",
            "Iteration 771, loss = 0.32599273\n",
            "Iteration 772, loss = 0.32538411\n",
            "Iteration 773, loss = 0.32478246\n",
            "Iteration 774, loss = 0.32418729\n",
            "Iteration 775, loss = 0.32359866\n",
            "Iteration 776, loss = 0.32301679\n",
            "Iteration 777, loss = 0.32244139\n",
            "Iteration 778, loss = 0.32187213\n",
            "Iteration 779, loss = 0.32130838\n",
            "Iteration 780, loss = 0.32074947\n",
            "Iteration 781, loss = 0.32019658\n",
            "Iteration 782, loss = 0.31964974\n",
            "Iteration 783, loss = 0.31910864\n",
            "Iteration 784, loss = 0.31857287\n",
            "Iteration 785, loss = 0.31804242\n",
            "Iteration 786, loss = 0.31751712\n",
            "Iteration 787, loss = 0.31699687\n",
            "Iteration 788, loss = 0.31648179\n",
            "Iteration 789, loss = 0.31597191\n",
            "Iteration 790, loss = 0.31546763\n",
            "Iteration 791, loss = 0.31496800\n",
            "Iteration 792, loss = 0.31447284\n",
            "Iteration 793, loss = 0.31398222\n",
            "Iteration 794, loss = 0.31349622\n",
            "Iteration 795, loss = 0.31301422\n",
            "Iteration 796, loss = 0.31253647\n",
            "Iteration 797, loss = 0.31206297\n",
            "Iteration 798, loss = 0.31159371\n",
            "Iteration 799, loss = 0.31112797\n",
            "Iteration 800, loss = 0.31066602\n",
            "Iteration 1, loss = 0.70631520\n",
            "Iteration 2, loss = 0.70570309\n",
            "Iteration 3, loss = 0.70493495\n",
            "Iteration 4, loss = 0.70399530\n",
            "Iteration 5, loss = 0.70291033\n",
            "Iteration 6, loss = 0.70180572\n",
            "Iteration 7, loss = 0.70069336\n",
            "Iteration 8, loss = 0.69959513\n",
            "Iteration 9, loss = 0.69843125\n",
            "Iteration 10, loss = 0.69735991\n",
            "Iteration 11, loss = 0.69636410\n",
            "Iteration 12, loss = 0.69533926\n",
            "Iteration 13, loss = 0.69444960\n",
            "Iteration 14, loss = 0.69360798\n",
            "Iteration 15, loss = 0.69282058\n",
            "Iteration 16, loss = 0.69216439\n",
            "Iteration 17, loss = 0.69162461\n",
            "Iteration 18, loss = 0.69118426\n",
            "Iteration 19, loss = 0.69081639\n",
            "Iteration 20, loss = 0.69051966\n",
            "Iteration 21, loss = 0.69024908\n",
            "Iteration 22, loss = 0.68996679\n",
            "Iteration 23, loss = 0.68970033\n",
            "Iteration 24, loss = 0.68949042\n",
            "Iteration 25, loss = 0.68935453\n",
            "Iteration 26, loss = 0.68924874\n",
            "Iteration 27, loss = 0.68917113\n",
            "Iteration 28, loss = 0.68910209\n",
            "Iteration 29, loss = 0.68903901\n",
            "Iteration 30, loss = 0.68897306\n",
            "Iteration 31, loss = 0.68890449\n",
            "Iteration 32, loss = 0.68883353\n",
            "Iteration 33, loss = 0.68876040\n",
            "Iteration 34, loss = 0.68868528\n",
            "Iteration 35, loss = 0.68860836\n",
            "Iteration 36, loss = 0.68852979\n",
            "Iteration 37, loss = 0.68844972\n",
            "Iteration 38, loss = 0.68836827\n",
            "Iteration 39, loss = 0.68828595\n",
            "Iteration 40, loss = 0.68820195\n",
            "Iteration 41, loss = 0.68811650\n",
            "Iteration 42, loss = 0.68802993\n",
            "Iteration 43, loss = 0.68794285\n",
            "Iteration 44, loss = 0.68785494\n",
            "Iteration 45, loss = 0.68776627\n",
            "Iteration 46, loss = 0.68767687\n",
            "Iteration 47, loss = 0.68758681\n",
            "Iteration 48, loss = 0.68749610\n",
            "Iteration 49, loss = 0.68740479\n",
            "Iteration 50, loss = 0.68731288\n",
            "Iteration 51, loss = 0.68722042\n",
            "Iteration 52, loss = 0.68712742\n",
            "Iteration 53, loss = 0.68703388\n",
            "Iteration 54, loss = 0.68693991\n",
            "Iteration 55, loss = 0.68684553\n",
            "Iteration 56, loss = 0.68675068\n",
            "Iteration 57, loss = 0.68665530\n",
            "Iteration 58, loss = 0.68655942\n",
            "Iteration 59, loss = 0.68646299\n",
            "Iteration 60, loss = 0.68636607\n",
            "Iteration 61, loss = 0.68626866\n",
            "Iteration 62, loss = 0.68617075\n",
            "Iteration 63, loss = 0.68607253\n",
            "Iteration 64, loss = 0.68597391\n",
            "Iteration 65, loss = 0.68587478\n",
            "Iteration 66, loss = 0.68577513\n",
            "Iteration 67, loss = 0.68567490\n",
            "Iteration 68, loss = 0.68557414\n",
            "Iteration 69, loss = 0.68547284\n",
            "Iteration 70, loss = 0.68537098\n",
            "Iteration 71, loss = 0.68526834\n",
            "Iteration 72, loss = 0.68516504\n",
            "Iteration 73, loss = 0.68506111\n",
            "Iteration 74, loss = 0.68495654\n",
            "Iteration 75, loss = 0.68485135\n",
            "Iteration 76, loss = 0.68474553\n",
            "Iteration 77, loss = 0.68463814\n",
            "Iteration 78, loss = 0.68452744\n",
            "Iteration 79, loss = 0.68441570\n",
            "Iteration 80, loss = 0.68430296\n",
            "Iteration 81, loss = 0.68418923\n",
            "Iteration 82, loss = 0.68407453\n",
            "Iteration 83, loss = 0.68395886\n",
            "Iteration 84, loss = 0.68384223\n",
            "Iteration 85, loss = 0.68372498\n",
            "Iteration 86, loss = 0.68361029\n",
            "Iteration 87, loss = 0.68349472\n",
            "Iteration 88, loss = 0.68337266\n",
            "Iteration 89, loss = 0.68325341\n",
            "Iteration 90, loss = 0.68313387\n",
            "Iteration 91, loss = 0.68301336\n",
            "Iteration 92, loss = 0.68289183\n",
            "Iteration 93, loss = 0.68276928\n",
            "Iteration 94, loss = 0.68264576\n",
            "Iteration 95, loss = 0.68252216\n",
            "Iteration 96, loss = 0.68239927\n",
            "Iteration 97, loss = 0.68227510\n",
            "Iteration 98, loss = 0.68214971\n",
            "Iteration 99, loss = 0.68202312\n",
            "Iteration 100, loss = 0.68189533\n",
            "Iteration 101, loss = 0.68176632\n",
            "Iteration 102, loss = 0.68163608\n",
            "Iteration 103, loss = 0.68150441\n",
            "Iteration 104, loss = 0.68136755\n",
            "Iteration 105, loss = 0.68122916\n",
            "Iteration 106, loss = 0.68108924\n",
            "Iteration 107, loss = 0.68094780\n",
            "Iteration 108, loss = 0.68080488\n",
            "Iteration 109, loss = 0.68066048\n",
            "Iteration 110, loss = 0.68051459\n",
            "Iteration 111, loss = 0.68036753\n",
            "Iteration 112, loss = 0.68021928\n",
            "Iteration 113, loss = 0.68006962\n",
            "Iteration 114, loss = 0.67991863\n",
            "Iteration 115, loss = 0.67976613\n",
            "Iteration 116, loss = 0.67961224\n",
            "Iteration 117, loss = 0.67945691\n",
            "Iteration 118, loss = 0.67930015\n",
            "Iteration 119, loss = 0.67914184\n",
            "Iteration 120, loss = 0.67898146\n",
            "Iteration 121, loss = 0.67881844\n",
            "Iteration 122, loss = 0.67865365\n",
            "Iteration 123, loss = 0.67848710\n",
            "Iteration 124, loss = 0.67831886\n",
            "Iteration 125, loss = 0.67814895\n",
            "Iteration 126, loss = 0.67797671\n",
            "Iteration 127, loss = 0.67780179\n",
            "Iteration 128, loss = 0.67762470\n",
            "Iteration 129, loss = 0.67744555\n",
            "Iteration 130, loss = 0.67726435\n",
            "Iteration 131, loss = 0.67708114\n",
            "Iteration 132, loss = 0.67689616\n",
            "Iteration 133, loss = 0.67670876\n",
            "Iteration 134, loss = 0.67651922\n",
            "Iteration 135, loss = 0.67632776\n",
            "Iteration 136, loss = 0.67613426\n",
            "Iteration 137, loss = 0.67593912\n",
            "Iteration 138, loss = 0.67574134\n",
            "Iteration 139, loss = 0.67554177\n",
            "Iteration 140, loss = 0.67534021\n",
            "Iteration 141, loss = 0.67513659\n",
            "Iteration 142, loss = 0.67493089\n",
            "Iteration 143, loss = 0.67472357\n",
            "Iteration 144, loss = 0.67451424\n",
            "Iteration 145, loss = 0.67430313\n",
            "Iteration 146, loss = 0.67408997\n",
            "Iteration 147, loss = 0.67387472\n",
            "Iteration 148, loss = 0.67365737\n",
            "Iteration 149, loss = 0.67343824\n",
            "Iteration 150, loss = 0.67321651\n",
            "Iteration 151, loss = 0.67299279\n",
            "Iteration 152, loss = 0.67276680\n",
            "Iteration 153, loss = 0.67253852\n",
            "Iteration 154, loss = 0.67230797\n",
            "Iteration 155, loss = 0.67207519\n",
            "Iteration 156, loss = 0.67183999\n",
            "Iteration 157, loss = 0.67160698\n",
            "Iteration 158, loss = 0.67137228\n",
            "Iteration 159, loss = 0.67113529\n",
            "Iteration 160, loss = 0.67089592\n",
            "Iteration 161, loss = 0.67065406\n",
            "Iteration 162, loss = 0.67040965\n",
            "Iteration 163, loss = 0.67016273\n",
            "Iteration 164, loss = 0.66991324\n",
            "Iteration 165, loss = 0.66966044\n",
            "Iteration 166, loss = 0.66940127\n",
            "Iteration 167, loss = 0.66913901\n",
            "Iteration 168, loss = 0.66887373\n",
            "Iteration 169, loss = 0.66860580\n",
            "Iteration 170, loss = 0.66833365\n",
            "Iteration 171, loss = 0.66805711\n",
            "Iteration 172, loss = 0.66777763\n",
            "Iteration 173, loss = 0.66749498\n",
            "Iteration 174, loss = 0.66720904\n",
            "Iteration 175, loss = 0.66691990\n",
            "Iteration 176, loss = 0.66662755\n",
            "Iteration 177, loss = 0.66633547\n",
            "Iteration 178, loss = 0.66604065\n",
            "Iteration 179, loss = 0.66574279\n",
            "Iteration 180, loss = 0.66544154\n",
            "Iteration 181, loss = 0.66513727\n",
            "Iteration 182, loss = 0.66483139\n",
            "Iteration 183, loss = 0.66452310\n",
            "Iteration 184, loss = 0.66421177\n",
            "Iteration 185, loss = 0.66389736\n",
            "Iteration 186, loss = 0.66357971\n",
            "Iteration 187, loss = 0.66325882\n",
            "Iteration 188, loss = 0.66293467\n",
            "Iteration 189, loss = 0.66260719\n",
            "Iteration 190, loss = 0.66227640\n",
            "Iteration 191, loss = 0.66194226\n",
            "Iteration 192, loss = 0.66160464\n",
            "Iteration 193, loss = 0.66126359\n",
            "Iteration 194, loss = 0.66091904\n",
            "Iteration 195, loss = 0.66057098\n",
            "Iteration 196, loss = 0.66021939\n",
            "Iteration 197, loss = 0.65986415\n",
            "Iteration 198, loss = 0.65950509\n",
            "Iteration 199, loss = 0.65913802\n",
            "Iteration 200, loss = 0.65876426\n",
            "Iteration 201, loss = 0.65838606\n",
            "Iteration 202, loss = 0.65800056\n",
            "Iteration 203, loss = 0.65760544\n",
            "Iteration 204, loss = 0.65720493\n",
            "Iteration 205, loss = 0.65679907\n",
            "Iteration 206, loss = 0.65638803\n",
            "Iteration 207, loss = 0.65597209\n",
            "Iteration 208, loss = 0.65555111\n",
            "Iteration 209, loss = 0.65512508\n",
            "Iteration 210, loss = 0.65469484\n",
            "Iteration 211, loss = 0.65425747\n",
            "Iteration 212, loss = 0.65381111\n",
            "Iteration 213, loss = 0.65335941\n",
            "Iteration 214, loss = 0.65290241\n",
            "Iteration 215, loss = 0.65244025\n",
            "Iteration 216, loss = 0.65197297\n",
            "Iteration 217, loss = 0.65150037\n",
            "Iteration 218, loss = 0.65102266\n",
            "Iteration 219, loss = 0.65053994\n",
            "Iteration 220, loss = 0.65005008\n",
            "Iteration 221, loss = 0.64954814\n",
            "Iteration 222, loss = 0.64903691\n",
            "Iteration 223, loss = 0.64851475\n",
            "Iteration 224, loss = 0.64798337\n",
            "Iteration 225, loss = 0.64744546\n",
            "Iteration 226, loss = 0.64689820\n",
            "Iteration 227, loss = 0.64633075\n",
            "Iteration 228, loss = 0.64574418\n",
            "Iteration 229, loss = 0.64513725\n",
            "Iteration 230, loss = 0.64450846\n",
            "Iteration 231, loss = 0.64386033\n",
            "Iteration 232, loss = 0.64319546\n",
            "Iteration 233, loss = 0.64251769\n",
            "Iteration 234, loss = 0.64182904\n",
            "Iteration 235, loss = 0.64112311\n",
            "Iteration 236, loss = 0.64039736\n",
            "Iteration 237, loss = 0.63966317\n",
            "Iteration 238, loss = 0.63891887\n",
            "Iteration 239, loss = 0.63817880\n",
            "Iteration 240, loss = 0.63743667\n",
            "Iteration 241, loss = 0.63668820\n",
            "Iteration 242, loss = 0.63592232\n",
            "Iteration 243, loss = 0.63513878\n",
            "Iteration 244, loss = 0.63433198\n",
            "Iteration 245, loss = 0.63351098\n",
            "Iteration 246, loss = 0.63265516\n",
            "Iteration 247, loss = 0.63177062\n",
            "Iteration 248, loss = 0.63087436\n",
            "Iteration 249, loss = 0.62996711\n",
            "Iteration 250, loss = 0.62905113\n",
            "Iteration 251, loss = 0.62812026\n",
            "Iteration 252, loss = 0.62716685\n",
            "Iteration 253, loss = 0.62619939\n",
            "Iteration 254, loss = 0.62523398\n",
            "Iteration 255, loss = 0.62426259\n",
            "Iteration 256, loss = 0.62327190\n",
            "Iteration 257, loss = 0.62225976\n",
            "Iteration 258, loss = 0.62123074\n",
            "Iteration 259, loss = 0.62017734\n",
            "Iteration 260, loss = 0.61910042\n",
            "Iteration 261, loss = 0.61799578\n",
            "Iteration 262, loss = 0.61685571\n",
            "Iteration 263, loss = 0.61569863\n",
            "Iteration 264, loss = 0.61453403\n",
            "Iteration 265, loss = 0.61331894\n",
            "Iteration 266, loss = 0.61207706\n",
            "Iteration 267, loss = 0.61082178\n",
            "Iteration 268, loss = 0.60953569\n",
            "Iteration 269, loss = 0.60820445\n",
            "Iteration 270, loss = 0.60683092\n",
            "Iteration 271, loss = 0.60541524\n",
            "Iteration 272, loss = 0.60398061\n",
            "Iteration 273, loss = 0.60253022\n",
            "Iteration 274, loss = 0.60105127\n",
            "Iteration 275, loss = 0.59952496\n",
            "Iteration 276, loss = 0.59790216\n",
            "Iteration 277, loss = 0.59622804\n",
            "Iteration 278, loss = 0.59451084\n",
            "Iteration 279, loss = 0.59276517\n",
            "Iteration 280, loss = 0.59099309\n",
            "Iteration 281, loss = 0.58920596\n",
            "Iteration 282, loss = 0.58741070\n",
            "Iteration 283, loss = 0.58551066\n",
            "Iteration 284, loss = 0.58343593\n",
            "Iteration 285, loss = 0.58125907\n",
            "Iteration 286, loss = 0.57894989\n",
            "Iteration 287, loss = 0.57644170\n",
            "Iteration 288, loss = 0.57387714\n",
            "Iteration 289, loss = 0.57124441\n",
            "Iteration 290, loss = 0.56854930\n",
            "Iteration 291, loss = 0.56584188\n",
            "Iteration 292, loss = 0.56312184\n",
            "Iteration 293, loss = 0.56040565\n",
            "Iteration 294, loss = 0.55770904\n",
            "Iteration 295, loss = 0.55507092\n",
            "Iteration 296, loss = 0.55244722\n",
            "Iteration 297, loss = 0.54984139\n",
            "Iteration 298, loss = 0.54725551\n",
            "Iteration 299, loss = 0.54467688\n",
            "Iteration 300, loss = 0.54208934\n",
            "Iteration 301, loss = 0.53948453\n",
            "Iteration 302, loss = 0.53689718\n",
            "Iteration 303, loss = 0.53433106\n",
            "Iteration 304, loss = 0.53178879\n",
            "Iteration 305, loss = 0.52927078\n",
            "Iteration 306, loss = 0.52677845\n",
            "Iteration 307, loss = 0.52431238\n",
            "Iteration 308, loss = 0.52187688\n",
            "Iteration 309, loss = 0.51947440\n",
            "Iteration 310, loss = 0.51707582\n",
            "Iteration 311, loss = 0.51465721\n",
            "Iteration 312, loss = 0.51225461\n",
            "Iteration 313, loss = 0.50981131\n",
            "Iteration 314, loss = 0.50738162\n",
            "Iteration 315, loss = 0.50497484\n",
            "Iteration 316, loss = 0.50258636\n",
            "Iteration 317, loss = 0.50021334\n",
            "Iteration 318, loss = 0.49786132\n",
            "Iteration 319, loss = 0.49552959\n",
            "Iteration 320, loss = 0.49321731\n",
            "Iteration 321, loss = 0.49092438\n",
            "Iteration 322, loss = 0.48865246\n",
            "Iteration 323, loss = 0.48640125\n",
            "Iteration 324, loss = 0.48417459\n",
            "Iteration 325, loss = 0.48197106\n",
            "Iteration 326, loss = 0.47978576\n",
            "Iteration 327, loss = 0.47761786\n",
            "Iteration 328, loss = 0.47546862\n",
            "Iteration 329, loss = 0.47333862\n",
            "Iteration 330, loss = 0.47123500\n",
            "Iteration 331, loss = 0.46916333\n",
            "Iteration 332, loss = 0.46711374\n",
            "Iteration 333, loss = 0.46508643\n",
            "Iteration 334, loss = 0.46308291\n",
            "Iteration 335, loss = 0.46110147\n",
            "Iteration 336, loss = 0.45913572\n",
            "Iteration 337, loss = 0.45718744\n",
            "Iteration 338, loss = 0.45525727\n",
            "Iteration 339, loss = 0.45334391\n",
            "Iteration 340, loss = 0.45144943\n",
            "Iteration 341, loss = 0.44957667\n",
            "Iteration 342, loss = 0.44772158\n",
            "Iteration 343, loss = 0.44588401\n",
            "Iteration 344, loss = 0.44406454\n",
            "Iteration 345, loss = 0.44226464\n",
            "Iteration 346, loss = 0.44048175\n",
            "Iteration 347, loss = 0.43871437\n",
            "Iteration 348, loss = 0.43696669\n",
            "Iteration 349, loss = 0.43523620\n",
            "Iteration 350, loss = 0.43352222\n",
            "Iteration 351, loss = 0.43182450\n",
            "Iteration 352, loss = 0.43014687\n",
            "Iteration 353, loss = 0.42848790\n",
            "Iteration 354, loss = 0.42684485\n",
            "Iteration 355, loss = 0.42521853\n",
            "Iteration 356, loss = 0.42360912\n",
            "Iteration 357, loss = 0.42201554\n",
            "Iteration 358, loss = 0.42044111\n",
            "Iteration 359, loss = 0.41888559\n",
            "Iteration 360, loss = 0.41734587\n",
            "Iteration 361, loss = 0.41582261\n",
            "Iteration 362, loss = 0.41431554\n",
            "Iteration 363, loss = 0.41282302\n",
            "Iteration 364, loss = 0.41134564\n",
            "Iteration 365, loss = 0.40988415\n",
            "Iteration 366, loss = 0.40843854\n",
            "Iteration 367, loss = 0.40700761\n",
            "Iteration 368, loss = 0.40559121\n",
            "Iteration 369, loss = 0.40418944\n",
            "Iteration 370, loss = 0.40280174\n",
            "Iteration 371, loss = 0.40142774\n",
            "Iteration 372, loss = 0.40007032\n",
            "Iteration 373, loss = 0.39872778\n",
            "Iteration 374, loss = 0.39739793\n",
            "Iteration 375, loss = 0.39608215\n",
            "Iteration 376, loss = 0.39477994\n",
            "Iteration 377, loss = 0.39349124\n",
            "Iteration 378, loss = 0.39221640\n",
            "Iteration 379, loss = 0.39095494\n",
            "Iteration 380, loss = 0.38970857\n",
            "Iteration 381, loss = 0.38847576\n",
            "Iteration 382, loss = 0.38725568\n",
            "Iteration 383, loss = 0.38604808\n",
            "Iteration 384, loss = 0.38485300\n",
            "Iteration 385, loss = 0.38367105\n",
            "Iteration 386, loss = 0.38250180\n",
            "Iteration 387, loss = 0.38134541\n",
            "Iteration 388, loss = 0.38020250\n",
            "Iteration 389, loss = 0.37907162\n",
            "Iteration 390, loss = 0.37795343\n",
            "Iteration 391, loss = 0.37684703\n",
            "Iteration 392, loss = 0.37575170\n",
            "Iteration 393, loss = 0.37466668\n",
            "Iteration 394, loss = 0.37359266\n",
            "Iteration 395, loss = 0.37252973\n",
            "Iteration 396, loss = 0.37147788\n",
            "Iteration 397, loss = 0.37043667\n",
            "Iteration 398, loss = 0.36940498\n",
            "Iteration 399, loss = 0.36838362\n",
            "Iteration 400, loss = 0.36737354\n",
            "Iteration 401, loss = 0.36637432\n",
            "Iteration 402, loss = 0.36538594\n",
            "Iteration 403, loss = 0.36440852\n",
            "Iteration 404, loss = 0.36344183\n",
            "Iteration 405, loss = 0.36248563\n",
            "Iteration 406, loss = 0.36153858\n",
            "Iteration 407, loss = 0.36060119\n",
            "Iteration 408, loss = 0.35967357\n",
            "Iteration 409, loss = 0.35875519\n",
            "Iteration 410, loss = 0.35784621\n",
            "Iteration 411, loss = 0.35694627\n",
            "Iteration 412, loss = 0.35605459\n",
            "Iteration 413, loss = 0.35517196\n",
            "Iteration 414, loss = 0.35429865\n",
            "Iteration 415, loss = 0.35343412\n",
            "Iteration 416, loss = 0.35257806\n",
            "Iteration 417, loss = 0.35173016\n",
            "Iteration 418, loss = 0.35089103\n",
            "Iteration 419, loss = 0.35006070\n",
            "Iteration 420, loss = 0.34923925\n",
            "Iteration 421, loss = 0.34842613\n",
            "Iteration 422, loss = 0.34761992\n",
            "Iteration 423, loss = 0.34682170\n",
            "Iteration 424, loss = 0.34603232\n",
            "Iteration 425, loss = 0.34525132\n",
            "Iteration 426, loss = 0.34447965\n",
            "Iteration 427, loss = 0.34371568\n",
            "Iteration 428, loss = 0.34295960\n",
            "Iteration 429, loss = 0.34221096\n",
            "Iteration 430, loss = 0.34146977\n",
            "Iteration 431, loss = 0.34073605\n",
            "Iteration 432, loss = 0.34000981\n",
            "Iteration 433, loss = 0.33928221\n",
            "Iteration 434, loss = 0.33853380\n",
            "Iteration 435, loss = 0.33778582\n",
            "Iteration 436, loss = 0.33703889\n",
            "Iteration 437, loss = 0.33629346\n",
            "Iteration 438, loss = 0.33555039\n",
            "Iteration 439, loss = 0.33481031\n",
            "Iteration 440, loss = 0.33407268\n",
            "Iteration 441, loss = 0.33334358\n",
            "Iteration 442, loss = 0.33261857\n",
            "Iteration 443, loss = 0.33189740\n",
            "Iteration 444, loss = 0.33118060\n",
            "Iteration 445, loss = 0.33046905\n",
            "Iteration 446, loss = 0.32976235\n",
            "Iteration 447, loss = 0.32906082\n",
            "Iteration 448, loss = 0.32836491\n",
            "Iteration 449, loss = 0.32767452\n",
            "Iteration 450, loss = 0.32698987\n",
            "Iteration 451, loss = 0.32631080\n",
            "Iteration 452, loss = 0.32563692\n",
            "Iteration 453, loss = 0.32496847\n",
            "Iteration 454, loss = 0.32430572\n",
            "Iteration 455, loss = 0.32364849\n",
            "Iteration 456, loss = 0.32299671\n",
            "Iteration 457, loss = 0.32235441\n",
            "Iteration 458, loss = 0.32172077\n",
            "Iteration 459, loss = 0.32109249\n",
            "Iteration 460, loss = 0.32046968\n",
            "Iteration 461, loss = 0.31985504\n",
            "Iteration 462, loss = 0.31924649\n",
            "Iteration 463, loss = 0.31864303\n",
            "Iteration 464, loss = 0.31804493\n",
            "Iteration 465, loss = 0.31745069\n",
            "Iteration 466, loss = 0.31686047\n",
            "Iteration 467, loss = 0.31627491\n",
            "Iteration 468, loss = 0.31569427\n",
            "Iteration 469, loss = 0.31511869\n",
            "Iteration 470, loss = 0.31454833\n",
            "Iteration 471, loss = 0.31398268\n",
            "Iteration 472, loss = 0.31342061\n",
            "Iteration 473, loss = 0.31286316\n",
            "Iteration 474, loss = 0.31231016\n",
            "Iteration 475, loss = 0.31176166\n",
            "Iteration 476, loss = 0.31121748\n",
            "Iteration 477, loss = 0.31067770\n",
            "Iteration 478, loss = 0.31014233\n",
            "Iteration 479, loss = 0.30961110\n",
            "Iteration 480, loss = 0.30908519\n",
            "Iteration 481, loss = 0.30856499\n",
            "Iteration 482, loss = 0.30804986\n",
            "Iteration 483, loss = 0.30753990\n",
            "Iteration 484, loss = 0.30703438\n",
            "Iteration 485, loss = 0.30653337\n",
            "Iteration 486, loss = 0.30603728\n",
            "Iteration 487, loss = 0.30554503\n",
            "Iteration 488, loss = 0.30505749\n",
            "Iteration 489, loss = 0.30457487\n",
            "Iteration 490, loss = 0.30409616\n",
            "Iteration 491, loss = 0.30362131\n",
            "Iteration 492, loss = 0.30315041\n",
            "Iteration 493, loss = 0.30268285\n",
            "Iteration 494, loss = 0.30221875\n",
            "Iteration 495, loss = 0.30175858\n",
            "Iteration 496, loss = 0.30130213\n",
            "Iteration 497, loss = 0.30084972\n",
            "Iteration 498, loss = 0.30040140\n",
            "Iteration 499, loss = 0.29995675\n",
            "Iteration 500, loss = 0.29951612\n",
            "Iteration 501, loss = 0.29907829\n",
            "Iteration 502, loss = 0.29864439\n",
            "Iteration 503, loss = 0.29821483\n",
            "Iteration 504, loss = 0.29778863\n",
            "Iteration 505, loss = 0.29736570\n",
            "Iteration 506, loss = 0.29694615\n",
            "Iteration 507, loss = 0.29653034\n",
            "Iteration 508, loss = 0.29611868\n",
            "Iteration 509, loss = 0.29570973\n",
            "Iteration 510, loss = 0.29530437\n",
            "Iteration 511, loss = 0.29490222\n",
            "Iteration 512, loss = 0.29450381\n",
            "Iteration 513, loss = 0.29410869\n",
            "Iteration 514, loss = 0.29371666\n",
            "Iteration 515, loss = 0.29332714\n",
            "Iteration 516, loss = 0.29294103\n",
            "Iteration 517, loss = 0.29255779\n",
            "Iteration 518, loss = 0.29217768\n",
            "Iteration 519, loss = 0.29180066\n",
            "Iteration 520, loss = 0.29142647\n",
            "Iteration 521, loss = 0.29105510\n",
            "Iteration 522, loss = 0.29068665\n",
            "Iteration 523, loss = 0.29032139\n",
            "Iteration 524, loss = 0.28995882\n",
            "Iteration 525, loss = 0.28959883\n",
            "Iteration 526, loss = 0.28924134\n",
            "Iteration 527, loss = 0.28888593\n",
            "Iteration 528, loss = 0.28853304\n",
            "Iteration 529, loss = 0.28818250\n",
            "Iteration 530, loss = 0.28783419\n",
            "Iteration 531, loss = 0.28748851\n",
            "Iteration 532, loss = 0.28714520\n",
            "Iteration 533, loss = 0.28680437\n",
            "Iteration 534, loss = 0.28646928\n",
            "Iteration 535, loss = 0.28613866\n",
            "Iteration 536, loss = 0.28581070\n",
            "Iteration 537, loss = 0.28548519\n",
            "Iteration 538, loss = 0.28516205\n",
            "Iteration 539, loss = 0.28484120\n",
            "Iteration 540, loss = 0.28452246\n",
            "Iteration 541, loss = 0.28420613\n",
            "Iteration 542, loss = 0.28389194\n",
            "Iteration 543, loss = 0.28358073\n",
            "Iteration 544, loss = 0.28327187\n",
            "Iteration 545, loss = 0.28296526\n",
            "Iteration 546, loss = 0.28266080\n",
            "Iteration 547, loss = 0.28235829\n",
            "Iteration 548, loss = 0.28205777\n",
            "Iteration 549, loss = 0.28175914\n",
            "Iteration 550, loss = 0.28146276\n",
            "Iteration 551, loss = 0.28116824\n",
            "Iteration 552, loss = 0.28087674\n",
            "Iteration 553, loss = 0.28058709\n",
            "Iteration 554, loss = 0.28029917\n",
            "Iteration 555, loss = 0.28001294\n",
            "Iteration 556, loss = 0.27972871\n",
            "Iteration 557, loss = 0.27944625\n",
            "Iteration 558, loss = 0.27916558\n",
            "Iteration 559, loss = 0.27888837\n",
            "Iteration 560, loss = 0.27861586\n",
            "Iteration 561, loss = 0.27834541\n",
            "Iteration 562, loss = 0.27807700\n",
            "Iteration 563, loss = 0.27781032\n",
            "Iteration 564, loss = 0.27754537\n",
            "Iteration 565, loss = 0.27728211\n",
            "Iteration 566, loss = 0.27702033\n",
            "Iteration 567, loss = 0.27676027\n",
            "Iteration 568, loss = 0.27650226\n",
            "Iteration 569, loss = 0.27624619\n",
            "Iteration 570, loss = 0.27599171\n",
            "Iteration 571, loss = 0.27574003\n",
            "Iteration 572, loss = 0.27549066\n",
            "Iteration 573, loss = 0.27524300\n",
            "Iteration 574, loss = 0.27499698\n",
            "Iteration 575, loss = 0.27475277\n",
            "Iteration 576, loss = 0.27450986\n",
            "Iteration 577, loss = 0.27426852\n",
            "Iteration 578, loss = 0.27402821\n",
            "Iteration 579, loss = 0.27378757\n",
            "Iteration 580, loss = 0.27354805\n",
            "Iteration 581, loss = 0.27330952\n",
            "Iteration 582, loss = 0.27307187\n",
            "Iteration 583, loss = 0.27283531\n",
            "Iteration 584, loss = 0.27260011\n",
            "Iteration 585, loss = 0.27236610\n",
            "Iteration 586, loss = 0.27213320\n",
            "Iteration 587, loss = 0.27190170\n",
            "Iteration 588, loss = 0.27167128\n",
            "Iteration 589, loss = 0.27144296\n",
            "Iteration 590, loss = 0.27121574\n",
            "Iteration 591, loss = 0.27098981\n",
            "Iteration 592, loss = 0.27076531\n",
            "Iteration 593, loss = 0.27054184\n",
            "Iteration 594, loss = 0.27031990\n",
            "Iteration 595, loss = 0.27009919\n",
            "Iteration 596, loss = 0.26987942\n",
            "Iteration 597, loss = 0.26966095\n",
            "Iteration 598, loss = 0.26944323\n",
            "Iteration 599, loss = 0.26922671\n",
            "Iteration 600, loss = 0.26901125\n",
            "Iteration 601, loss = 0.26879647\n",
            "Iteration 602, loss = 0.26858259\n",
            "Iteration 603, loss = 0.26837007\n",
            "Iteration 604, loss = 0.26815822\n",
            "Iteration 605, loss = 0.26794756\n",
            "Iteration 606, loss = 0.26773783\n",
            "Iteration 607, loss = 0.26752937\n",
            "Iteration 608, loss = 0.26732199\n",
            "Iteration 609, loss = 0.26711576\n",
            "Iteration 610, loss = 0.26691058\n",
            "Iteration 611, loss = 0.26670644\n",
            "Iteration 612, loss = 0.26650369\n",
            "Iteration 613, loss = 0.26630192\n",
            "Iteration 614, loss = 0.26610124\n",
            "Iteration 615, loss = 0.26590145\n",
            "Iteration 616, loss = 0.26570298\n",
            "Iteration 617, loss = 0.26550521\n",
            "Iteration 618, loss = 0.26530871\n",
            "Iteration 619, loss = 0.26511285\n",
            "Iteration 620, loss = 0.26491778\n",
            "Iteration 621, loss = 0.26472387\n",
            "Iteration 622, loss = 0.26453093\n",
            "Iteration 623, loss = 0.26433874\n",
            "Iteration 624, loss = 0.26414771\n",
            "Iteration 625, loss = 0.26395745\n",
            "Iteration 626, loss = 0.26376804\n",
            "Iteration 627, loss = 0.26357893\n",
            "Iteration 628, loss = 0.26339062\n",
            "Iteration 629, loss = 0.26320315\n",
            "Iteration 630, loss = 0.26301616\n",
            "Iteration 631, loss = 0.26283043\n",
            "Iteration 632, loss = 0.26264525\n",
            "Iteration 633, loss = 0.26246113\n",
            "Iteration 634, loss = 0.26227768\n",
            "Iteration 635, loss = 0.26209511\n",
            "Iteration 636, loss = 0.26191354\n",
            "Iteration 637, loss = 0.26173293\n",
            "Iteration 638, loss = 0.26155319\n",
            "Iteration 639, loss = 0.26137434\n",
            "Iteration 640, loss = 0.26119657\n",
            "Iteration 641, loss = 0.26101977\n",
            "Iteration 642, loss = 0.26084377\n",
            "Iteration 643, loss = 0.26066853\n",
            "Iteration 644, loss = 0.26049407\n",
            "Iteration 645, loss = 0.26031969\n",
            "Iteration 646, loss = 0.26014628\n",
            "Iteration 647, loss = 0.25997333\n",
            "Iteration 648, loss = 0.25980124\n",
            "Iteration 649, loss = 0.25962970\n",
            "Iteration 650, loss = 0.25945925\n",
            "Iteration 651, loss = 0.25928970\n",
            "Iteration 652, loss = 0.25912102\n",
            "Iteration 653, loss = 0.25895298\n",
            "Iteration 654, loss = 0.25878607\n",
            "Iteration 655, loss = 0.25862018\n",
            "Iteration 656, loss = 0.25845544\n",
            "Iteration 657, loss = 0.25829088\n",
            "Iteration 658, loss = 0.25812687\n",
            "Iteration 659, loss = 0.25796400\n",
            "Iteration 660, loss = 0.25780202\n",
            "Iteration 661, loss = 0.25764064\n",
            "Iteration 662, loss = 0.25748015\n",
            "Iteration 663, loss = 0.25732015\n",
            "Iteration 664, loss = 0.25716179\n",
            "Iteration 665, loss = 0.25700405\n",
            "Iteration 666, loss = 0.25684747\n",
            "Iteration 667, loss = 0.25669159\n",
            "Iteration 668, loss = 0.25653651\n",
            "Iteration 669, loss = 0.25638184\n",
            "Iteration 670, loss = 0.25622769\n",
            "Iteration 671, loss = 0.25607446\n",
            "Iteration 672, loss = 0.25592188\n",
            "Iteration 673, loss = 0.25577033\n",
            "Iteration 674, loss = 0.25561906\n",
            "Iteration 675, loss = 0.25546897\n",
            "Iteration 676, loss = 0.25531934\n",
            "Iteration 677, loss = 0.25517190\n",
            "Iteration 678, loss = 0.25502663\n",
            "Iteration 679, loss = 0.25488209\n",
            "Iteration 680, loss = 0.25473804\n",
            "Iteration 681, loss = 0.25459467\n",
            "Iteration 682, loss = 0.25445176\n",
            "Iteration 683, loss = 0.25430963\n",
            "Iteration 684, loss = 0.25416813\n",
            "Iteration 685, loss = 0.25402723\n",
            "Iteration 686, loss = 0.25388756\n",
            "Iteration 687, loss = 0.25374844\n",
            "Iteration 688, loss = 0.25360974\n",
            "Iteration 689, loss = 0.25347196\n",
            "Iteration 690, loss = 0.25333455\n",
            "Iteration 691, loss = 0.25319796\n",
            "Iteration 692, loss = 0.25306193\n",
            "Iteration 693, loss = 0.25292615\n",
            "Iteration 694, loss = 0.25279105\n",
            "Iteration 695, loss = 0.25265588\n",
            "Iteration 696, loss = 0.25252103\n",
            "Iteration 697, loss = 0.25238693\n",
            "Iteration 698, loss = 0.25225298\n",
            "Iteration 699, loss = 0.25212029\n",
            "Iteration 700, loss = 0.25198819\n",
            "Iteration 701, loss = 0.25185655\n",
            "Iteration 702, loss = 0.25172549\n",
            "Iteration 703, loss = 0.25159507\n",
            "Iteration 704, loss = 0.25146512\n",
            "Iteration 705, loss = 0.25133534\n",
            "Iteration 706, loss = 0.25120548\n",
            "Iteration 707, loss = 0.25107595\n",
            "Iteration 708, loss = 0.25094689\n",
            "Iteration 709, loss = 0.25081801\n",
            "Iteration 710, loss = 0.25068916\n",
            "Iteration 711, loss = 0.25056090\n",
            "Iteration 712, loss = 0.25043244\n",
            "Iteration 713, loss = 0.25030479\n",
            "Iteration 714, loss = 0.25017744\n",
            "Iteration 715, loss = 0.25005040\n",
            "Iteration 716, loss = 0.24992358\n",
            "Iteration 717, loss = 0.24979734\n",
            "Iteration 718, loss = 0.24967124\n",
            "Iteration 719, loss = 0.24954558\n",
            "Iteration 720, loss = 0.24942020\n",
            "Iteration 721, loss = 0.24929518\n",
            "Iteration 722, loss = 0.24917062\n",
            "Iteration 723, loss = 0.24904617\n",
            "Iteration 724, loss = 0.24892224\n",
            "Iteration 725, loss = 0.24879845\n",
            "Iteration 726, loss = 0.24867477\n",
            "Iteration 727, loss = 0.24855142\n",
            "Iteration 728, loss = 0.24842869\n",
            "Iteration 729, loss = 0.24830629\n",
            "Iteration 730, loss = 0.24818421\n",
            "Iteration 731, loss = 0.24806257\n",
            "Iteration 732, loss = 0.24794159\n",
            "Iteration 733, loss = 0.24782081\n",
            "Iteration 734, loss = 0.24770056\n",
            "Iteration 735, loss = 0.24758034\n",
            "Iteration 736, loss = 0.24746044\n",
            "Iteration 737, loss = 0.24734075\n",
            "Iteration 738, loss = 0.24722171\n",
            "Iteration 739, loss = 0.24710282\n",
            "Iteration 740, loss = 0.24698439\n",
            "Iteration 741, loss = 0.24686622\n",
            "Iteration 742, loss = 0.24674838\n",
            "Iteration 743, loss = 0.24663078\n",
            "Iteration 744, loss = 0.24651371\n",
            "Iteration 745, loss = 0.24639670\n",
            "Iteration 746, loss = 0.24628019\n",
            "Iteration 747, loss = 0.24616374\n",
            "Iteration 748, loss = 0.24604742\n",
            "Iteration 749, loss = 0.24593119\n",
            "Iteration 750, loss = 0.24581588\n",
            "Iteration 751, loss = 0.24570013\n",
            "Iteration 752, loss = 0.24558416\n",
            "Iteration 753, loss = 0.24546813\n",
            "Iteration 754, loss = 0.24535258\n",
            "Iteration 755, loss = 0.24523702\n",
            "Iteration 756, loss = 0.24512170\n",
            "Iteration 757, loss = 0.24500655\n",
            "Iteration 758, loss = 0.24489162\n",
            "Iteration 759, loss = 0.24477683\n",
            "Iteration 760, loss = 0.24466231\n",
            "Iteration 761, loss = 0.24454802\n",
            "Iteration 762, loss = 0.24443376\n",
            "Iteration 763, loss = 0.24431992\n",
            "Iteration 764, loss = 0.24420645\n",
            "Iteration 765, loss = 0.24409310\n",
            "Iteration 766, loss = 0.24398012\n",
            "Iteration 767, loss = 0.24386722\n",
            "Iteration 768, loss = 0.24375481\n",
            "Iteration 769, loss = 0.24364273\n",
            "Iteration 770, loss = 0.24353131\n",
            "Iteration 771, loss = 0.24342003\n",
            "Iteration 772, loss = 0.24330916\n",
            "Iteration 773, loss = 0.24319843\n",
            "Iteration 774, loss = 0.24308818\n",
            "Iteration 775, loss = 0.24297802\n",
            "Iteration 776, loss = 0.24286827\n",
            "Iteration 777, loss = 0.24275881\n",
            "Iteration 778, loss = 0.24264952\n",
            "Iteration 779, loss = 0.24254060\n",
            "Iteration 780, loss = 0.24243178\n",
            "Iteration 781, loss = 0.24232343\n",
            "Iteration 782, loss = 0.24221542\n",
            "Iteration 783, loss = 0.24210751\n",
            "Iteration 784, loss = 0.24200003\n",
            "Iteration 785, loss = 0.24189273\n",
            "Iteration 786, loss = 0.24178541\n",
            "Iteration 787, loss = 0.24167770\n",
            "Iteration 788, loss = 0.24157014\n",
            "Iteration 789, loss = 0.24146287\n",
            "Iteration 790, loss = 0.24135587\n",
            "Iteration 791, loss = 0.24124938\n",
            "Iteration 792, loss = 0.24114260\n",
            "Iteration 793, loss = 0.24103587\n",
            "Iteration 794, loss = 0.24092945\n",
            "Iteration 795, loss = 0.24082327\n",
            "Iteration 796, loss = 0.24071810\n",
            "Iteration 797, loss = 0.24061690\n",
            "Iteration 798, loss = 0.24051654\n",
            "Iteration 799, loss = 0.24041655\n",
            "Iteration 800, loss = 0.24031680\n",
            "Iteration 1, loss = 0.70689432\n",
            "Iteration 2, loss = 0.70635622\n",
            "Iteration 3, loss = 0.70562629\n",
            "Iteration 4, loss = 0.70474615\n",
            "Iteration 5, loss = 0.70372005\n",
            "Iteration 6, loss = 0.70265530\n",
            "Iteration 7, loss = 0.70152372\n",
            "Iteration 8, loss = 0.70030808\n",
            "Iteration 9, loss = 0.69903520\n",
            "Iteration 10, loss = 0.69783349\n",
            "Iteration 11, loss = 0.69676976\n",
            "Iteration 12, loss = 0.69569509\n",
            "Iteration 13, loss = 0.69468910\n",
            "Iteration 14, loss = 0.69370758\n",
            "Iteration 15, loss = 0.69283181\n",
            "Iteration 16, loss = 0.69208474\n",
            "Iteration 17, loss = 0.69152834\n",
            "Iteration 18, loss = 0.69107516\n",
            "Iteration 19, loss = 0.69070750\n",
            "Iteration 20, loss = 0.69041532\n",
            "Iteration 21, loss = 0.69015665\n",
            "Iteration 22, loss = 0.68989257\n",
            "Iteration 23, loss = 0.68965135\n",
            "Iteration 24, loss = 0.68948535\n",
            "Iteration 25, loss = 0.68937413\n",
            "Iteration 26, loss = 0.68930042\n",
            "Iteration 27, loss = 0.68924384\n",
            "Iteration 28, loss = 0.68919029\n",
            "Iteration 29, loss = 0.68913253\n",
            "Iteration 30, loss = 0.68907096\n",
            "Iteration 31, loss = 0.68900597\n",
            "Iteration 32, loss = 0.68893785\n",
            "Iteration 33, loss = 0.68886686\n",
            "Iteration 34, loss = 0.68878978\n",
            "Iteration 35, loss = 0.68871157\n",
            "Iteration 36, loss = 0.68863234\n",
            "Iteration 37, loss = 0.68855216\n",
            "Iteration 38, loss = 0.68847111\n",
            "Iteration 39, loss = 0.68838927\n",
            "Iteration 40, loss = 0.68830669\n",
            "Iteration 41, loss = 0.68822343\n",
            "Iteration 42, loss = 0.68813953\n",
            "Iteration 43, loss = 0.68805505\n",
            "Iteration 44, loss = 0.68797000\n",
            "Iteration 45, loss = 0.68788441\n",
            "Iteration 46, loss = 0.68779830\n",
            "Iteration 47, loss = 0.68771172\n",
            "Iteration 48, loss = 0.68762468\n",
            "Iteration 49, loss = 0.68753719\n",
            "Iteration 50, loss = 0.68744926\n",
            "Iteration 51, loss = 0.68736085\n",
            "Iteration 52, loss = 0.68727201\n",
            "Iteration 53, loss = 0.68718276\n",
            "Iteration 54, loss = 0.68709303\n",
            "Iteration 55, loss = 0.68700287\n",
            "Iteration 56, loss = 0.68691230\n",
            "Iteration 57, loss = 0.68682132\n",
            "Iteration 58, loss = 0.68672989\n",
            "Iteration 59, loss = 0.68663805\n",
            "Iteration 60, loss = 0.68654577\n",
            "Iteration 61, loss = 0.68645307\n",
            "Iteration 62, loss = 0.68635856\n",
            "Iteration 63, loss = 0.68626202\n",
            "Iteration 64, loss = 0.68616446\n",
            "Iteration 65, loss = 0.68606601\n",
            "Iteration 66, loss = 0.68596669\n",
            "Iteration 67, loss = 0.68586653\n",
            "Iteration 68, loss = 0.68576585\n",
            "Iteration 69, loss = 0.68566442\n",
            "Iteration 70, loss = 0.68556224\n",
            "Iteration 71, loss = 0.68545939\n",
            "Iteration 72, loss = 0.68535588\n",
            "Iteration 73, loss = 0.68525168\n",
            "Iteration 74, loss = 0.68514677\n",
            "Iteration 75, loss = 0.68504118\n",
            "Iteration 76, loss = 0.68493491\n",
            "Iteration 77, loss = 0.68482795\n",
            "Iteration 78, loss = 0.68472028\n",
            "Iteration 79, loss = 0.68461192\n",
            "Iteration 80, loss = 0.68450289\n",
            "Iteration 81, loss = 0.68439317\n",
            "Iteration 82, loss = 0.68428274\n",
            "Iteration 83, loss = 0.68417159\n",
            "Iteration 84, loss = 0.68405971\n",
            "Iteration 85, loss = 0.68394686\n",
            "Iteration 86, loss = 0.68382709\n",
            "Iteration 87, loss = 0.68370596\n",
            "Iteration 88, loss = 0.68358329\n",
            "Iteration 89, loss = 0.68346008\n",
            "Iteration 90, loss = 0.68333734\n",
            "Iteration 91, loss = 0.68321776\n",
            "Iteration 92, loss = 0.68309773\n",
            "Iteration 93, loss = 0.68298084\n",
            "Iteration 94, loss = 0.68286301\n",
            "Iteration 95, loss = 0.68274429\n",
            "Iteration 96, loss = 0.68262450\n",
            "Iteration 97, loss = 0.68250365\n",
            "Iteration 98, loss = 0.68238172\n",
            "Iteration 99, loss = 0.68225874\n",
            "Iteration 100, loss = 0.68213469\n",
            "Iteration 101, loss = 0.68200956\n",
            "Iteration 102, loss = 0.68188330\n",
            "Iteration 103, loss = 0.68175574\n",
            "Iteration 104, loss = 0.68162698\n",
            "Iteration 105, loss = 0.68149705\n",
            "Iteration 106, loss = 0.68136630\n",
            "Iteration 107, loss = 0.68123525\n",
            "Iteration 108, loss = 0.68110054\n",
            "Iteration 109, loss = 0.68096275\n",
            "Iteration 110, loss = 0.68082327\n",
            "Iteration 111, loss = 0.68068243\n",
            "Iteration 112, loss = 0.68053986\n",
            "Iteration 113, loss = 0.68039577\n",
            "Iteration 114, loss = 0.68025019\n",
            "Iteration 115, loss = 0.68010324\n",
            "Iteration 116, loss = 0.67995535\n",
            "Iteration 117, loss = 0.67980628\n",
            "Iteration 118, loss = 0.67965569\n",
            "Iteration 119, loss = 0.67950366\n",
            "Iteration 120, loss = 0.67935012\n",
            "Iteration 121, loss = 0.67919502\n",
            "Iteration 122, loss = 0.67903839\n",
            "Iteration 123, loss = 0.67888024\n",
            "Iteration 124, loss = 0.67872058\n",
            "Iteration 125, loss = 0.67855990\n",
            "Iteration 126, loss = 0.67839776\n",
            "Iteration 127, loss = 0.67823406\n",
            "Iteration 128, loss = 0.67806879\n",
            "Iteration 129, loss = 0.67790196\n",
            "Iteration 130, loss = 0.67773354\n",
            "Iteration 131, loss = 0.67756343\n",
            "Iteration 132, loss = 0.67739158\n",
            "Iteration 133, loss = 0.67721821\n",
            "Iteration 134, loss = 0.67704314\n",
            "Iteration 135, loss = 0.67686633\n",
            "Iteration 136, loss = 0.67668777\n",
            "Iteration 137, loss = 0.67650742\n",
            "Iteration 138, loss = 0.67632577\n",
            "Iteration 139, loss = 0.67614201\n",
            "Iteration 140, loss = 0.67595627\n",
            "Iteration 141, loss = 0.67576893\n",
            "Iteration 142, loss = 0.67557987\n",
            "Iteration 143, loss = 0.67538865\n",
            "Iteration 144, loss = 0.67519547\n",
            "Iteration 145, loss = 0.67500049\n",
            "Iteration 146, loss = 0.67480325\n",
            "Iteration 147, loss = 0.67460411\n",
            "Iteration 148, loss = 0.67440289\n",
            "Iteration 149, loss = 0.67419991\n",
            "Iteration 150, loss = 0.67399515\n",
            "Iteration 151, loss = 0.67378843\n",
            "Iteration 152, loss = 0.67357950\n",
            "Iteration 153, loss = 0.67336839\n",
            "Iteration 154, loss = 0.67315520\n",
            "Iteration 155, loss = 0.67293976\n",
            "Iteration 156, loss = 0.67272333\n",
            "Iteration 157, loss = 0.67250626\n",
            "Iteration 158, loss = 0.67228722\n",
            "Iteration 159, loss = 0.67206445\n",
            "Iteration 160, loss = 0.67183580\n",
            "Iteration 161, loss = 0.67160440\n",
            "Iteration 162, loss = 0.67137034\n",
            "Iteration 163, loss = 0.67113408\n",
            "Iteration 164, loss = 0.67089474\n",
            "Iteration 165, loss = 0.67065295\n",
            "Iteration 166, loss = 0.67040838\n",
            "Iteration 167, loss = 0.67016111\n",
            "Iteration 168, loss = 0.66991092\n",
            "Iteration 169, loss = 0.66965790\n",
            "Iteration 170, loss = 0.66940205\n",
            "Iteration 171, loss = 0.66914319\n",
            "Iteration 172, loss = 0.66888164\n",
            "Iteration 173, loss = 0.66861713\n",
            "Iteration 174, loss = 0.66834866\n",
            "Iteration 175, loss = 0.66807202\n",
            "Iteration 176, loss = 0.66779186\n",
            "Iteration 177, loss = 0.66750824\n",
            "Iteration 178, loss = 0.66722133\n",
            "Iteration 179, loss = 0.66693075\n",
            "Iteration 180, loss = 0.66663688\n",
            "Iteration 181, loss = 0.66633936\n",
            "Iteration 182, loss = 0.66603854\n",
            "Iteration 183, loss = 0.66573416\n",
            "Iteration 184, loss = 0.66542644\n",
            "Iteration 185, loss = 0.66511566\n",
            "Iteration 186, loss = 0.66479566\n",
            "Iteration 187, loss = 0.66446981\n",
            "Iteration 188, loss = 0.66413723\n",
            "Iteration 189, loss = 0.66379405\n",
            "Iteration 190, loss = 0.66344928\n",
            "Iteration 191, loss = 0.66309995\n",
            "Iteration 192, loss = 0.66274599\n",
            "Iteration 193, loss = 0.66238761\n",
            "Iteration 194, loss = 0.66202484\n",
            "Iteration 195, loss = 0.66165773\n",
            "Iteration 196, loss = 0.66128661\n",
            "Iteration 197, loss = 0.66091127\n",
            "Iteration 198, loss = 0.66053163\n",
            "Iteration 199, loss = 0.66014781\n",
            "Iteration 200, loss = 0.65975984\n",
            "Iteration 201, loss = 0.65936773\n",
            "Iteration 202, loss = 0.65897147\n",
            "Iteration 203, loss = 0.65857155\n",
            "Iteration 204, loss = 0.65816812\n",
            "Iteration 205, loss = 0.65776089\n",
            "Iteration 206, loss = 0.65734975\n",
            "Iteration 207, loss = 0.65693466\n",
            "Iteration 208, loss = 0.65651564\n",
            "Iteration 209, loss = 0.65609269\n",
            "Iteration 210, loss = 0.65566571\n",
            "Iteration 211, loss = 0.65523490\n",
            "Iteration 212, loss = 0.65480079\n",
            "Iteration 213, loss = 0.65436363\n",
            "Iteration 214, loss = 0.65392201\n",
            "Iteration 215, loss = 0.65347574\n",
            "Iteration 216, loss = 0.65302496\n",
            "Iteration 217, loss = 0.65256964\n",
            "Iteration 218, loss = 0.65210985\n",
            "Iteration 219, loss = 0.65164598\n",
            "Iteration 220, loss = 0.65117830\n",
            "Iteration 221, loss = 0.65070416\n",
            "Iteration 222, loss = 0.65021196\n",
            "Iteration 223, loss = 0.64971473\n",
            "Iteration 224, loss = 0.64921203\n",
            "Iteration 225, loss = 0.64870383\n",
            "Iteration 226, loss = 0.64818954\n",
            "Iteration 227, loss = 0.64765632\n",
            "Iteration 228, loss = 0.64711595\n",
            "Iteration 229, loss = 0.64656929\n",
            "Iteration 230, loss = 0.64601606\n",
            "Iteration 231, loss = 0.64545648\n",
            "Iteration 232, loss = 0.64489091\n",
            "Iteration 233, loss = 0.64431938\n",
            "Iteration 234, loss = 0.64374206\n",
            "Iteration 235, loss = 0.64315948\n",
            "Iteration 236, loss = 0.64255963\n",
            "Iteration 237, loss = 0.64195050\n",
            "Iteration 238, loss = 0.64133448\n",
            "Iteration 239, loss = 0.64071177\n",
            "Iteration 240, loss = 0.64008147\n",
            "Iteration 241, loss = 0.63943043\n",
            "Iteration 242, loss = 0.63877205\n",
            "Iteration 243, loss = 0.63810705\n",
            "Iteration 244, loss = 0.63743758\n",
            "Iteration 245, loss = 0.63676153\n",
            "Iteration 246, loss = 0.63607902\n",
            "Iteration 247, loss = 0.63538618\n",
            "Iteration 248, loss = 0.63467449\n",
            "Iteration 249, loss = 0.63394920\n",
            "Iteration 250, loss = 0.63321680\n",
            "Iteration 251, loss = 0.63247737\n",
            "Iteration 252, loss = 0.63170922\n",
            "Iteration 253, loss = 0.63092732\n",
            "Iteration 254, loss = 0.63013602\n",
            "Iteration 255, loss = 0.62933583\n",
            "Iteration 256, loss = 0.62852655\n",
            "Iteration 257, loss = 0.62770930\n",
            "Iteration 258, loss = 0.62688469\n",
            "Iteration 259, loss = 0.62605280\n",
            "Iteration 260, loss = 0.62521409\n",
            "Iteration 261, loss = 0.62436896\n",
            "Iteration 262, loss = 0.62351819\n",
            "Iteration 263, loss = 0.62266187\n",
            "Iteration 264, loss = 0.62179992\n",
            "Iteration 265, loss = 0.62093310\n",
            "Iteration 266, loss = 0.62006296\n",
            "Iteration 267, loss = 0.61918774\n",
            "Iteration 268, loss = 0.61830763\n",
            "Iteration 269, loss = 0.61742667\n",
            "Iteration 270, loss = 0.61655124\n",
            "Iteration 271, loss = 0.61567284\n",
            "Iteration 272, loss = 0.61479057\n",
            "Iteration 273, loss = 0.61390494\n",
            "Iteration 274, loss = 0.61301506\n",
            "Iteration 275, loss = 0.61212118\n",
            "Iteration 276, loss = 0.61122359\n",
            "Iteration 277, loss = 0.61028337\n",
            "Iteration 278, loss = 0.60932700\n",
            "Iteration 279, loss = 0.60836331\n",
            "Iteration 280, loss = 0.60737274\n",
            "Iteration 281, loss = 0.60635223\n",
            "Iteration 282, loss = 0.60531856\n",
            "Iteration 283, loss = 0.60427606\n",
            "Iteration 284, loss = 0.60322528\n",
            "Iteration 285, loss = 0.60213328\n",
            "Iteration 286, loss = 0.60093282\n",
            "Iteration 287, loss = 0.59968229\n",
            "Iteration 288, loss = 0.59834488\n",
            "Iteration 289, loss = 0.59696359\n",
            "Iteration 290, loss = 0.59555792\n",
            "Iteration 291, loss = 0.59413410\n",
            "Iteration 292, loss = 0.59269605\n",
            "Iteration 293, loss = 0.59124725\n",
            "Iteration 294, loss = 0.58979221\n",
            "Iteration 295, loss = 0.58832341\n",
            "Iteration 296, loss = 0.58683476\n",
            "Iteration 297, loss = 0.58534375\n",
            "Iteration 298, loss = 0.58385070\n",
            "Iteration 299, loss = 0.58236783\n",
            "Iteration 300, loss = 0.58087795\n",
            "Iteration 301, loss = 0.57938141\n",
            "Iteration 302, loss = 0.57788948\n",
            "Iteration 303, loss = 0.57639966\n",
            "Iteration 304, loss = 0.57491530\n",
            "Iteration 305, loss = 0.57343305\n",
            "Iteration 306, loss = 0.57195707\n",
            "Iteration 307, loss = 0.57047876\n",
            "Iteration 308, loss = 0.56900713\n",
            "Iteration 309, loss = 0.56760635\n",
            "Iteration 310, loss = 0.56621560\n",
            "Iteration 311, loss = 0.56483358\n",
            "Iteration 312, loss = 0.56346153\n",
            "Iteration 313, loss = 0.56210031\n",
            "Iteration 314, loss = 0.56074074\n",
            "Iteration 315, loss = 0.55936894\n",
            "Iteration 316, loss = 0.55799291\n",
            "Iteration 317, loss = 0.55662241\n",
            "Iteration 318, loss = 0.55524410\n",
            "Iteration 319, loss = 0.55385064\n",
            "Iteration 320, loss = 0.55243481\n",
            "Iteration 321, loss = 0.55100746\n",
            "Iteration 322, loss = 0.54957158\n",
            "Iteration 323, loss = 0.54812852\n",
            "Iteration 324, loss = 0.54666970\n",
            "Iteration 325, loss = 0.54520700\n",
            "Iteration 326, loss = 0.54374628\n",
            "Iteration 327, loss = 0.54227682\n",
            "Iteration 328, loss = 0.54078947\n",
            "Iteration 329, loss = 0.53926192\n",
            "Iteration 330, loss = 0.53772374\n",
            "Iteration 331, loss = 0.53617509\n",
            "Iteration 332, loss = 0.53462526\n",
            "Iteration 333, loss = 0.53305954\n",
            "Iteration 334, loss = 0.53150998\n",
            "Iteration 335, loss = 0.52993599\n",
            "Iteration 336, loss = 0.52834835\n",
            "Iteration 337, loss = 0.52674997\n",
            "Iteration 338, loss = 0.52512248\n",
            "Iteration 339, loss = 0.52346470\n",
            "Iteration 340, loss = 0.52173910\n",
            "Iteration 341, loss = 0.51999886\n",
            "Iteration 342, loss = 0.51820482\n",
            "Iteration 343, loss = 0.51635624\n",
            "Iteration 344, loss = 0.51448278\n",
            "Iteration 345, loss = 0.51253074\n",
            "Iteration 346, loss = 0.51052713\n",
            "Iteration 347, loss = 0.50846652\n",
            "Iteration 348, loss = 0.50638193\n",
            "Iteration 349, loss = 0.50428902\n",
            "Iteration 350, loss = 0.50217531\n",
            "Iteration 351, loss = 0.50003497\n",
            "Iteration 352, loss = 0.49782941\n",
            "Iteration 353, loss = 0.49561975\n",
            "Iteration 354, loss = 0.49342383\n",
            "Iteration 355, loss = 0.49121690\n",
            "Iteration 356, loss = 0.48901553\n",
            "Iteration 357, loss = 0.48680411\n",
            "Iteration 358, loss = 0.48459640\n",
            "Iteration 359, loss = 0.48240925\n",
            "Iteration 360, loss = 0.48023944\n",
            "Iteration 361, loss = 0.47809805\n",
            "Iteration 362, loss = 0.47597218\n",
            "Iteration 363, loss = 0.47385967\n",
            "Iteration 364, loss = 0.47173519\n",
            "Iteration 365, loss = 0.46965061\n",
            "Iteration 366, loss = 0.46759945\n",
            "Iteration 367, loss = 0.46556766\n",
            "Iteration 368, loss = 0.46355745\n",
            "Iteration 369, loss = 0.46156443\n",
            "Iteration 370, loss = 0.45955000\n",
            "Iteration 371, loss = 0.45753422\n",
            "Iteration 372, loss = 0.45553637\n",
            "Iteration 373, loss = 0.45355675\n",
            "Iteration 374, loss = 0.45155691\n",
            "Iteration 375, loss = 0.44953121\n",
            "Iteration 376, loss = 0.44751309\n",
            "Iteration 377, loss = 0.44551043\n",
            "Iteration 378, loss = 0.44348772\n",
            "Iteration 379, loss = 0.44146248\n",
            "Iteration 380, loss = 0.43945217\n",
            "Iteration 381, loss = 0.43745836\n",
            "Iteration 382, loss = 0.43548170\n",
            "Iteration 383, loss = 0.43354316\n",
            "Iteration 384, loss = 0.43163350\n",
            "Iteration 385, loss = 0.42975377\n",
            "Iteration 386, loss = 0.42790289\n",
            "Iteration 387, loss = 0.42607181\n",
            "Iteration 388, loss = 0.42426203\n",
            "Iteration 389, loss = 0.42247608\n",
            "Iteration 390, loss = 0.42071021\n",
            "Iteration 391, loss = 0.41897382\n",
            "Iteration 392, loss = 0.41726153\n",
            "Iteration 393, loss = 0.41557004\n",
            "Iteration 394, loss = 0.41389666\n",
            "Iteration 395, loss = 0.41224179\n",
            "Iteration 396, loss = 0.41060598\n",
            "Iteration 397, loss = 0.40898852\n",
            "Iteration 398, loss = 0.40738999\n",
            "Iteration 399, loss = 0.40581019\n",
            "Iteration 400, loss = 0.40425207\n",
            "Iteration 401, loss = 0.40271367\n",
            "Iteration 402, loss = 0.40119288\n",
            "Iteration 403, loss = 0.39969027\n",
            "Iteration 404, loss = 0.39820655\n",
            "Iteration 405, loss = 0.39673904\n",
            "Iteration 406, loss = 0.39528840\n",
            "Iteration 407, loss = 0.39385427\n",
            "Iteration 408, loss = 0.39243631\n",
            "Iteration 409, loss = 0.39103603\n",
            "Iteration 410, loss = 0.38965151\n",
            "Iteration 411, loss = 0.38828204\n",
            "Iteration 412, loss = 0.38692930\n",
            "Iteration 413, loss = 0.38559484\n",
            "Iteration 414, loss = 0.38427571\n",
            "Iteration 415, loss = 0.38297128\n",
            "Iteration 416, loss = 0.38168299\n",
            "Iteration 417, loss = 0.38041062\n",
            "Iteration 418, loss = 0.37915249\n",
            "Iteration 419, loss = 0.37790872\n",
            "Iteration 420, loss = 0.37667938\n",
            "Iteration 421, loss = 0.37546346\n",
            "Iteration 422, loss = 0.37426135\n",
            "Iteration 423, loss = 0.37307234\n",
            "Iteration 424, loss = 0.37189625\n",
            "Iteration 425, loss = 0.37073334\n",
            "Iteration 426, loss = 0.36958257\n",
            "Iteration 427, loss = 0.36844375\n",
            "Iteration 428, loss = 0.36731646\n",
            "Iteration 429, loss = 0.36620157\n",
            "Iteration 430, loss = 0.36509860\n",
            "Iteration 431, loss = 0.36400853\n",
            "Iteration 432, loss = 0.36292952\n",
            "Iteration 433, loss = 0.36186158\n",
            "Iteration 434, loss = 0.36080416\n",
            "Iteration 435, loss = 0.35975701\n",
            "Iteration 436, loss = 0.35872030\n",
            "Iteration 437, loss = 0.35769448\n",
            "Iteration 438, loss = 0.35668140\n",
            "Iteration 439, loss = 0.35567923\n",
            "Iteration 440, loss = 0.35468765\n",
            "Iteration 441, loss = 0.35370552\n",
            "Iteration 442, loss = 0.35273164\n",
            "Iteration 443, loss = 0.35176695\n",
            "Iteration 444, loss = 0.35081005\n",
            "Iteration 445, loss = 0.34986246\n",
            "Iteration 446, loss = 0.34892410\n",
            "Iteration 447, loss = 0.34799501\n",
            "Iteration 448, loss = 0.34707541\n",
            "Iteration 449, loss = 0.34616472\n",
            "Iteration 450, loss = 0.34526247\n",
            "Iteration 451, loss = 0.34436840\n",
            "Iteration 452, loss = 0.34348365\n",
            "Iteration 453, loss = 0.34260784\n",
            "Iteration 454, loss = 0.34173983\n",
            "Iteration 455, loss = 0.34087955\n",
            "Iteration 456, loss = 0.34002986\n",
            "Iteration 457, loss = 0.33918921\n",
            "Iteration 458, loss = 0.33835681\n",
            "Iteration 459, loss = 0.33753254\n",
            "Iteration 460, loss = 0.33671608\n",
            "Iteration 461, loss = 0.33590771\n",
            "Iteration 462, loss = 0.33510717\n",
            "Iteration 463, loss = 0.33431472\n",
            "Iteration 464, loss = 0.33353120\n",
            "Iteration 465, loss = 0.33275541\n",
            "Iteration 466, loss = 0.33198714\n",
            "Iteration 467, loss = 0.33122605\n",
            "Iteration 468, loss = 0.33047222\n",
            "Iteration 469, loss = 0.32972556\n",
            "Iteration 470, loss = 0.32898607\n",
            "Iteration 471, loss = 0.32825436\n",
            "Iteration 472, loss = 0.32752988\n",
            "Iteration 473, loss = 0.32681236\n",
            "Iteration 474, loss = 0.32610135\n",
            "Iteration 475, loss = 0.32539692\n",
            "Iteration 476, loss = 0.32469910\n",
            "Iteration 477, loss = 0.32400837\n",
            "Iteration 478, loss = 0.32332378\n",
            "Iteration 479, loss = 0.32264437\n",
            "Iteration 480, loss = 0.32197098\n",
            "Iteration 481, loss = 0.32130368\n",
            "Iteration 482, loss = 0.32064337\n",
            "Iteration 483, loss = 0.31998885\n",
            "Iteration 484, loss = 0.31934023\n",
            "Iteration 485, loss = 0.31869759\n",
            "Iteration 486, loss = 0.31806133\n",
            "Iteration 487, loss = 0.31743079\n",
            "Iteration 488, loss = 0.31680558\n",
            "Iteration 489, loss = 0.31618610\n",
            "Iteration 490, loss = 0.31557228\n",
            "Iteration 491, loss = 0.31496429\n",
            "Iteration 492, loss = 0.31436212\n",
            "Iteration 493, loss = 0.31376602\n",
            "Iteration 494, loss = 0.31317529\n",
            "Iteration 495, loss = 0.31258966\n",
            "Iteration 496, loss = 0.31200963\n",
            "Iteration 497, loss = 0.31143476\n",
            "Iteration 498, loss = 0.31086475\n",
            "Iteration 499, loss = 0.31029980\n",
            "Iteration 500, loss = 0.30973992\n",
            "Iteration 501, loss = 0.30918642\n",
            "Iteration 502, loss = 0.30864003\n",
            "Iteration 503, loss = 0.30809896\n",
            "Iteration 504, loss = 0.30756255\n",
            "Iteration 505, loss = 0.30703057\n",
            "Iteration 506, loss = 0.30650347\n",
            "Iteration 507, loss = 0.30598159\n",
            "Iteration 508, loss = 0.30546419\n",
            "Iteration 509, loss = 0.30495113\n",
            "Iteration 510, loss = 0.30444291\n",
            "Iteration 511, loss = 0.30393930\n",
            "Iteration 512, loss = 0.30344086\n",
            "Iteration 513, loss = 0.30294612\n",
            "Iteration 514, loss = 0.30245562\n",
            "Iteration 515, loss = 0.30196981\n",
            "Iteration 516, loss = 0.30148843\n",
            "Iteration 517, loss = 0.30099761\n",
            "Iteration 518, loss = 0.30049165\n",
            "Iteration 519, loss = 0.29998318\n",
            "Iteration 520, loss = 0.29947309\n",
            "Iteration 521, loss = 0.29896190\n",
            "Iteration 522, loss = 0.29845002\n",
            "Iteration 523, loss = 0.29793728\n",
            "Iteration 524, loss = 0.29742473\n",
            "Iteration 525, loss = 0.29691316\n",
            "Iteration 526, loss = 0.29640323\n",
            "Iteration 527, loss = 0.29589647\n",
            "Iteration 528, loss = 0.29539339\n",
            "Iteration 529, loss = 0.29489439\n",
            "Iteration 530, loss = 0.29440146\n",
            "Iteration 531, loss = 0.29391218\n",
            "Iteration 532, loss = 0.29342906\n",
            "Iteration 533, loss = 0.29295064\n",
            "Iteration 534, loss = 0.29247511\n",
            "Iteration 535, loss = 0.29200360\n",
            "Iteration 536, loss = 0.29153579\n",
            "Iteration 537, loss = 0.29107161\n",
            "Iteration 538, loss = 0.29061096\n",
            "Iteration 539, loss = 0.29015344\n",
            "Iteration 540, loss = 0.28969934\n",
            "Iteration 541, loss = 0.28925043\n",
            "Iteration 542, loss = 0.28880659\n",
            "Iteration 543, loss = 0.28836799\n",
            "Iteration 544, loss = 0.28793310\n",
            "Iteration 545, loss = 0.28750355\n",
            "Iteration 546, loss = 0.28707965\n",
            "Iteration 547, loss = 0.28665699\n",
            "Iteration 548, loss = 0.28623280\n",
            "Iteration 549, loss = 0.28581097\n",
            "Iteration 550, loss = 0.28539136\n",
            "Iteration 551, loss = 0.28497412\n",
            "Iteration 552, loss = 0.28455891\n",
            "Iteration 553, loss = 0.28414614\n",
            "Iteration 554, loss = 0.28373568\n",
            "Iteration 555, loss = 0.28332754\n",
            "Iteration 556, loss = 0.28292194\n",
            "Iteration 557, loss = 0.28251928\n",
            "Iteration 558, loss = 0.28212010\n",
            "Iteration 559, loss = 0.28172356\n",
            "Iteration 560, loss = 0.28132964\n",
            "Iteration 561, loss = 0.28093887\n",
            "Iteration 562, loss = 0.28055060\n",
            "Iteration 563, loss = 0.28016521\n",
            "Iteration 564, loss = 0.27978236\n",
            "Iteration 565, loss = 0.27940231\n",
            "Iteration 566, loss = 0.27902475\n",
            "Iteration 567, loss = 0.27864973\n",
            "Iteration 568, loss = 0.27827731\n",
            "Iteration 569, loss = 0.27790746\n",
            "Iteration 570, loss = 0.27753984\n",
            "Iteration 571, loss = 0.27717459\n",
            "Iteration 572, loss = 0.27681196\n",
            "Iteration 573, loss = 0.27645161\n",
            "Iteration 574, loss = 0.27609363\n",
            "Iteration 575, loss = 0.27573812\n",
            "Iteration 576, loss = 0.27538522\n",
            "Iteration 577, loss = 0.27503483\n",
            "Iteration 578, loss = 0.27468645\n",
            "Iteration 579, loss = 0.27433997\n",
            "Iteration 580, loss = 0.27399574\n",
            "Iteration 581, loss = 0.27365345\n",
            "Iteration 582, loss = 0.27331330\n",
            "Iteration 583, loss = 0.27297510\n",
            "Iteration 584, loss = 0.27263962\n",
            "Iteration 585, loss = 0.27230664\n",
            "Iteration 586, loss = 0.27197604\n",
            "Iteration 587, loss = 0.27164728\n",
            "Iteration 588, loss = 0.27132061\n",
            "Iteration 589, loss = 0.27099598\n",
            "Iteration 590, loss = 0.27067360\n",
            "Iteration 591, loss = 0.27035316\n",
            "Iteration 592, loss = 0.27003554\n",
            "Iteration 593, loss = 0.26972208\n",
            "Iteration 594, loss = 0.26941032\n",
            "Iteration 595, loss = 0.26909987\n",
            "Iteration 596, loss = 0.26879118\n",
            "Iteration 597, loss = 0.26848642\n",
            "Iteration 598, loss = 0.26818439\n",
            "Iteration 599, loss = 0.26788478\n",
            "Iteration 600, loss = 0.26758713\n",
            "Iteration 601, loss = 0.26729135\n",
            "Iteration 602, loss = 0.26699790\n",
            "Iteration 603, loss = 0.26670628\n",
            "Iteration 604, loss = 0.26641688\n",
            "Iteration 605, loss = 0.26612935\n",
            "Iteration 606, loss = 0.26584355\n",
            "Iteration 607, loss = 0.26555958\n",
            "Iteration 608, loss = 0.26527749\n",
            "Iteration 609, loss = 0.26499775\n",
            "Iteration 610, loss = 0.26471970\n",
            "Iteration 611, loss = 0.26444345\n",
            "Iteration 612, loss = 0.26416886\n",
            "Iteration 613, loss = 0.26389574\n",
            "Iteration 614, loss = 0.26362251\n",
            "Iteration 615, loss = 0.26335017\n",
            "Iteration 616, loss = 0.26307905\n",
            "Iteration 617, loss = 0.26280903\n",
            "Iteration 618, loss = 0.26254038\n",
            "Iteration 619, loss = 0.26227313\n",
            "Iteration 620, loss = 0.26200716\n",
            "Iteration 621, loss = 0.26174230\n",
            "Iteration 622, loss = 0.26147884\n",
            "Iteration 623, loss = 0.26121670\n",
            "Iteration 624, loss = 0.26095612\n",
            "Iteration 625, loss = 0.26069672\n",
            "Iteration 626, loss = 0.26043870\n",
            "Iteration 627, loss = 0.26018184\n",
            "Iteration 628, loss = 0.25992620\n",
            "Iteration 629, loss = 0.25967177\n",
            "Iteration 630, loss = 0.25941864\n",
            "Iteration 631, loss = 0.25916693\n",
            "Iteration 632, loss = 0.25891647\n",
            "Iteration 633, loss = 0.25866748\n",
            "Iteration 634, loss = 0.25842076\n",
            "Iteration 635, loss = 0.25817561\n",
            "Iteration 636, loss = 0.25793157\n",
            "Iteration 637, loss = 0.25768897\n",
            "Iteration 638, loss = 0.25744747\n",
            "Iteration 639, loss = 0.25720749\n",
            "Iteration 640, loss = 0.25696858\n",
            "Iteration 641, loss = 0.25673093\n",
            "Iteration 642, loss = 0.25649449\n",
            "Iteration 643, loss = 0.25625938\n",
            "Iteration 644, loss = 0.25602638\n",
            "Iteration 645, loss = 0.25579439\n",
            "Iteration 646, loss = 0.25556457\n",
            "Iteration 647, loss = 0.25533599\n",
            "Iteration 648, loss = 0.25510872\n",
            "Iteration 649, loss = 0.25488260\n",
            "Iteration 650, loss = 0.25465765\n",
            "Iteration 651, loss = 0.25443370\n",
            "Iteration 652, loss = 0.25421098\n",
            "Iteration 653, loss = 0.25398995\n",
            "Iteration 654, loss = 0.25377081\n",
            "Iteration 655, loss = 0.25355300\n",
            "Iteration 656, loss = 0.25333589\n",
            "Iteration 657, loss = 0.25311987\n",
            "Iteration 658, loss = 0.25290478\n",
            "Iteration 659, loss = 0.25269071\n",
            "Iteration 660, loss = 0.25247750\n",
            "Iteration 661, loss = 0.25226492\n",
            "Iteration 662, loss = 0.25205346\n",
            "Iteration 663, loss = 0.25184281\n",
            "Iteration 664, loss = 0.25163294\n",
            "Iteration 665, loss = 0.25142388\n",
            "Iteration 666, loss = 0.25121587\n",
            "Iteration 667, loss = 0.25100889\n",
            "Iteration 668, loss = 0.25080383\n",
            "Iteration 669, loss = 0.25059905\n",
            "Iteration 670, loss = 0.25039526\n",
            "Iteration 671, loss = 0.25019180\n",
            "Iteration 672, loss = 0.24998892\n",
            "Iteration 673, loss = 0.24978790\n",
            "Iteration 674, loss = 0.24958872\n",
            "Iteration 675, loss = 0.24939070\n",
            "Iteration 676, loss = 0.24919344\n",
            "Iteration 677, loss = 0.24899707\n",
            "Iteration 678, loss = 0.24880157\n",
            "Iteration 679, loss = 0.24860712\n",
            "Iteration 680, loss = 0.24841356\n",
            "Iteration 681, loss = 0.24822071\n",
            "Iteration 682, loss = 0.24802902\n",
            "Iteration 683, loss = 0.24783777\n",
            "Iteration 684, loss = 0.24764740\n",
            "Iteration 685, loss = 0.24745759\n",
            "Iteration 686, loss = 0.24726868\n",
            "Iteration 687, loss = 0.24708098\n",
            "Iteration 688, loss = 0.24689410\n",
            "Iteration 689, loss = 0.24670799\n",
            "Iteration 690, loss = 0.24652280\n",
            "Iteration 691, loss = 0.24633855\n",
            "Iteration 692, loss = 0.24615489\n",
            "Iteration 693, loss = 0.24597224\n",
            "Iteration 694, loss = 0.24579020\n",
            "Iteration 695, loss = 0.24560920\n",
            "Iteration 696, loss = 0.24542892\n",
            "Iteration 697, loss = 0.24525007\n",
            "Iteration 698, loss = 0.24507173\n",
            "Iteration 699, loss = 0.24489432\n",
            "Iteration 700, loss = 0.24471747\n",
            "Iteration 701, loss = 0.24454187\n",
            "Iteration 702, loss = 0.24436697\n",
            "Iteration 703, loss = 0.24419316\n",
            "Iteration 704, loss = 0.24401973\n",
            "Iteration 705, loss = 0.24384716\n",
            "Iteration 706, loss = 0.24367507\n",
            "Iteration 707, loss = 0.24350405\n",
            "Iteration 708, loss = 0.24333345\n",
            "Iteration 709, loss = 0.24316389\n",
            "Iteration 710, loss = 0.24299472\n",
            "Iteration 711, loss = 0.24282542\n",
            "Iteration 712, loss = 0.24265644\n",
            "Iteration 713, loss = 0.24248821\n",
            "Iteration 714, loss = 0.24232032\n",
            "Iteration 715, loss = 0.24215322\n",
            "Iteration 716, loss = 0.24198606\n",
            "Iteration 717, loss = 0.24181961\n",
            "Iteration 718, loss = 0.24165349\n",
            "Iteration 719, loss = 0.24148802\n",
            "Iteration 720, loss = 0.24132306\n",
            "Iteration 721, loss = 0.24115839\n",
            "Iteration 722, loss = 0.24099427\n",
            "Iteration 723, loss = 0.24083058\n",
            "Iteration 724, loss = 0.24066758\n",
            "Iteration 725, loss = 0.24050507\n",
            "Iteration 726, loss = 0.24034312\n",
            "Iteration 727, loss = 0.24018141\n",
            "Iteration 728, loss = 0.24002027\n",
            "Iteration 729, loss = 0.23985964\n",
            "Iteration 730, loss = 0.23969955\n",
            "Iteration 731, loss = 0.23953991\n",
            "Iteration 732, loss = 0.23938098\n",
            "Iteration 733, loss = 0.23922217\n",
            "Iteration 734, loss = 0.23906399\n",
            "Iteration 735, loss = 0.23890614\n",
            "Iteration 736, loss = 0.23874916\n",
            "Iteration 737, loss = 0.23859286\n",
            "Iteration 738, loss = 0.23843692\n",
            "Iteration 739, loss = 0.23828183\n",
            "Iteration 740, loss = 0.23812743\n",
            "Iteration 741, loss = 0.23797428\n",
            "Iteration 742, loss = 0.23782132\n",
            "Iteration 743, loss = 0.23766895\n",
            "Iteration 744, loss = 0.23751668\n",
            "Iteration 745, loss = 0.23736434\n",
            "Iteration 746, loss = 0.23721257\n",
            "Iteration 747, loss = 0.23706117\n",
            "Iteration 748, loss = 0.23691006\n",
            "Iteration 749, loss = 0.23675940\n",
            "Iteration 750, loss = 0.23660920\n",
            "Iteration 751, loss = 0.23645927\n",
            "Iteration 752, loss = 0.23630966\n",
            "Iteration 753, loss = 0.23616056\n",
            "Iteration 754, loss = 0.23601175\n",
            "Iteration 755, loss = 0.23586396\n",
            "Iteration 756, loss = 0.23571602\n",
            "Iteration 757, loss = 0.23556887\n",
            "Iteration 758, loss = 0.23542190\n",
            "Iteration 759, loss = 0.23527535\n",
            "Iteration 760, loss = 0.23512887\n",
            "Iteration 761, loss = 0.23498326\n",
            "Iteration 762, loss = 0.23483789\n",
            "Iteration 763, loss = 0.23469281\n",
            "Iteration 764, loss = 0.23454781\n",
            "Iteration 765, loss = 0.23440398\n",
            "Iteration 766, loss = 0.23426028\n",
            "Iteration 767, loss = 0.23411677\n",
            "Iteration 768, loss = 0.23397393\n",
            "Iteration 769, loss = 0.23383135\n",
            "Iteration 770, loss = 0.23368928\n",
            "Iteration 771, loss = 0.23354739\n",
            "Iteration 772, loss = 0.23340610\n",
            "Iteration 773, loss = 0.23326518\n",
            "Iteration 774, loss = 0.23312463\n",
            "Iteration 775, loss = 0.23298434\n",
            "Iteration 776, loss = 0.23284454\n",
            "Iteration 777, loss = 0.23270520\n",
            "Iteration 778, loss = 0.23256611\n",
            "Iteration 779, loss = 0.23242725\n",
            "Iteration 780, loss = 0.23228910\n",
            "Iteration 781, loss = 0.23215104\n",
            "Iteration 782, loss = 0.23201358\n",
            "Iteration 783, loss = 0.23187629\n",
            "Iteration 784, loss = 0.23173888\n",
            "Iteration 785, loss = 0.23160224\n",
            "Iteration 786, loss = 0.23146759\n",
            "Iteration 787, loss = 0.23133340\n",
            "Iteration 788, loss = 0.23119923\n",
            "Iteration 789, loss = 0.23106564\n",
            "Iteration 790, loss = 0.23093203\n",
            "Iteration 791, loss = 0.23079850\n",
            "Iteration 792, loss = 0.23066520\n",
            "Iteration 793, loss = 0.23053212\n",
            "Iteration 794, loss = 0.23039940\n",
            "Iteration 795, loss = 0.23026675\n",
            "Iteration 796, loss = 0.23013422\n",
            "Iteration 797, loss = 0.23000209\n",
            "Iteration 798, loss = 0.22987023\n",
            "Iteration 799, loss = 0.22973868\n",
            "Iteration 800, loss = 0.22960763\n",
            "Iteration 1, loss = 0.70387741\n",
            "Iteration 2, loss = 0.70326798\n",
            "Iteration 3, loss = 0.70240309\n",
            "Iteration 4, loss = 0.70131519\n",
            "Iteration 5, loss = 0.70010513\n",
            "Iteration 6, loss = 0.69878163\n",
            "Iteration 7, loss = 0.69742465\n",
            "Iteration 8, loss = 0.69619873\n",
            "Iteration 9, loss = 0.69518920\n",
            "Iteration 10, loss = 0.69422702\n",
            "Iteration 11, loss = 0.69331318\n",
            "Iteration 12, loss = 0.69249526\n",
            "Iteration 13, loss = 0.69174034\n",
            "Iteration 14, loss = 0.69110471\n",
            "Iteration 15, loss = 0.69061571\n",
            "Iteration 16, loss = 0.69025043\n",
            "Iteration 17, loss = 0.68993770\n",
            "Iteration 18, loss = 0.68970056\n",
            "Iteration 19, loss = 0.68953536\n",
            "Iteration 20, loss = 0.68941155\n",
            "Iteration 21, loss = 0.68930694\n",
            "Iteration 22, loss = 0.68920440\n",
            "Iteration 23, loss = 0.68911527\n",
            "Iteration 24, loss = 0.68904224\n",
            "Iteration 25, loss = 0.68896769\n",
            "Iteration 26, loss = 0.68889164\n",
            "Iteration 27, loss = 0.68881413\n",
            "Iteration 28, loss = 0.68873528\n",
            "Iteration 29, loss = 0.68865503\n",
            "Iteration 30, loss = 0.68857348\n",
            "Iteration 31, loss = 0.68849068\n",
            "Iteration 32, loss = 0.68840659\n",
            "Iteration 33, loss = 0.68832132\n",
            "Iteration 34, loss = 0.68823493\n",
            "Iteration 35, loss = 0.68814746\n",
            "Iteration 36, loss = 0.68805895\n",
            "Iteration 37, loss = 0.68796947\n",
            "Iteration 38, loss = 0.68787899\n",
            "Iteration 39, loss = 0.68778756\n",
            "Iteration 40, loss = 0.68769523\n",
            "Iteration 41, loss = 0.68760200\n",
            "Iteration 42, loss = 0.68750790\n",
            "Iteration 43, loss = 0.68741294\n",
            "Iteration 44, loss = 0.68731714\n",
            "Iteration 45, loss = 0.68722050\n",
            "Iteration 46, loss = 0.68712301\n",
            "Iteration 47, loss = 0.68702470\n",
            "Iteration 48, loss = 0.68692563\n",
            "Iteration 49, loss = 0.68682578\n",
            "Iteration 50, loss = 0.68672518\n",
            "Iteration 51, loss = 0.68662379\n",
            "Iteration 52, loss = 0.68652164\n",
            "Iteration 53, loss = 0.68641869\n",
            "Iteration 54, loss = 0.68631505\n",
            "Iteration 55, loss = 0.68621055\n",
            "Iteration 56, loss = 0.68610522\n",
            "Iteration 57, loss = 0.68599908\n",
            "Iteration 58, loss = 0.68589215\n",
            "Iteration 59, loss = 0.68578440\n",
            "Iteration 60, loss = 0.68567581\n",
            "Iteration 61, loss = 0.68556637\n",
            "Iteration 62, loss = 0.68545638\n",
            "Iteration 63, loss = 0.68534566\n",
            "Iteration 64, loss = 0.68523561\n",
            "Iteration 65, loss = 0.68512423\n",
            "Iteration 66, loss = 0.68501142\n",
            "Iteration 67, loss = 0.68489725\n",
            "Iteration 68, loss = 0.68478172\n",
            "Iteration 69, loss = 0.68466590\n",
            "Iteration 70, loss = 0.68455011\n",
            "Iteration 71, loss = 0.68443873\n",
            "Iteration 72, loss = 0.68432622\n",
            "Iteration 73, loss = 0.68421325\n",
            "Iteration 74, loss = 0.68409948\n",
            "Iteration 75, loss = 0.68398470\n",
            "Iteration 76, loss = 0.68386890\n",
            "Iteration 77, loss = 0.68375286\n",
            "Iteration 78, loss = 0.68363507\n",
            "Iteration 79, loss = 0.68351662\n",
            "Iteration 80, loss = 0.68339680\n",
            "Iteration 81, loss = 0.68327795\n",
            "Iteration 82, loss = 0.68316008\n",
            "Iteration 83, loss = 0.68304073\n",
            "Iteration 84, loss = 0.68292004\n",
            "Iteration 85, loss = 0.68279819\n",
            "Iteration 86, loss = 0.68267492\n",
            "Iteration 87, loss = 0.68255022\n",
            "Iteration 88, loss = 0.68242461\n",
            "Iteration 89, loss = 0.68229690\n",
            "Iteration 90, loss = 0.68216814\n",
            "Iteration 91, loss = 0.68203801\n",
            "Iteration 92, loss = 0.68190687\n",
            "Iteration 93, loss = 0.68177393\n",
            "Iteration 94, loss = 0.68163975\n",
            "Iteration 95, loss = 0.68150304\n",
            "Iteration 96, loss = 0.68136152\n",
            "Iteration 97, loss = 0.68121847\n",
            "Iteration 98, loss = 0.68107356\n",
            "Iteration 99, loss = 0.68092768\n",
            "Iteration 100, loss = 0.68077872\n",
            "Iteration 101, loss = 0.68062168\n",
            "Iteration 102, loss = 0.68046208\n",
            "Iteration 103, loss = 0.68030026\n",
            "Iteration 104, loss = 0.68013628\n",
            "Iteration 105, loss = 0.67997268\n",
            "Iteration 106, loss = 0.67980539\n",
            "Iteration 107, loss = 0.67963483\n",
            "Iteration 108, loss = 0.67946169\n",
            "Iteration 109, loss = 0.67928587\n",
            "Iteration 110, loss = 0.67910779\n",
            "Iteration 111, loss = 0.67892719\n",
            "Iteration 112, loss = 0.67874426\n",
            "Iteration 113, loss = 0.67855933\n",
            "Iteration 114, loss = 0.67837212\n",
            "Iteration 115, loss = 0.67818274\n",
            "Iteration 116, loss = 0.67799123\n",
            "Iteration 117, loss = 0.67779781\n",
            "Iteration 118, loss = 0.67759598\n",
            "Iteration 119, loss = 0.67739184\n",
            "Iteration 120, loss = 0.67718519\n",
            "Iteration 121, loss = 0.67697576\n",
            "Iteration 122, loss = 0.67676182\n",
            "Iteration 123, loss = 0.67654346\n",
            "Iteration 124, loss = 0.67632206\n",
            "Iteration 125, loss = 0.67609740\n",
            "Iteration 126, loss = 0.67587020\n",
            "Iteration 127, loss = 0.67564036\n",
            "Iteration 128, loss = 0.67540553\n",
            "Iteration 129, loss = 0.67515887\n",
            "Iteration 130, loss = 0.67490218\n",
            "Iteration 131, loss = 0.67463479\n",
            "Iteration 132, loss = 0.67436197\n",
            "Iteration 133, loss = 0.67408383\n",
            "Iteration 134, loss = 0.67379765\n",
            "Iteration 135, loss = 0.67350184\n",
            "Iteration 136, loss = 0.67320098\n",
            "Iteration 137, loss = 0.67289165\n",
            "Iteration 138, loss = 0.67257468\n",
            "Iteration 139, loss = 0.67225411\n",
            "Iteration 140, loss = 0.67192712\n",
            "Iteration 141, loss = 0.67158480\n",
            "Iteration 142, loss = 0.67123689\n",
            "Iteration 143, loss = 0.67088234\n",
            "Iteration 144, loss = 0.67049893\n",
            "Iteration 145, loss = 0.67009096\n",
            "Iteration 146, loss = 0.66966327\n",
            "Iteration 147, loss = 0.66920621\n",
            "Iteration 148, loss = 0.66873291\n",
            "Iteration 149, loss = 0.66824714\n",
            "Iteration 150, loss = 0.66775859\n",
            "Iteration 151, loss = 0.66726821\n",
            "Iteration 152, loss = 0.66678239\n",
            "Iteration 153, loss = 0.66629339\n",
            "Iteration 154, loss = 0.66579696\n",
            "Iteration 155, loss = 0.66527128\n",
            "Iteration 156, loss = 0.66474065\n",
            "Iteration 157, loss = 0.66421867\n",
            "Iteration 158, loss = 0.66369059\n",
            "Iteration 159, loss = 0.66315308\n",
            "Iteration 160, loss = 0.66261055\n",
            "Iteration 161, loss = 0.66206279\n",
            "Iteration 162, loss = 0.66150973\n",
            "Iteration 163, loss = 0.66094385\n",
            "Iteration 164, loss = 0.66036431\n",
            "Iteration 165, loss = 0.65977938\n",
            "Iteration 166, loss = 0.65917921\n",
            "Iteration 167, loss = 0.65855993\n",
            "Iteration 168, loss = 0.65793437\n",
            "Iteration 169, loss = 0.65730389\n",
            "Iteration 170, loss = 0.65663000\n",
            "Iteration 171, loss = 0.65589839\n",
            "Iteration 172, loss = 0.65513077\n",
            "Iteration 173, loss = 0.65432631\n",
            "Iteration 174, loss = 0.65349987\n",
            "Iteration 175, loss = 0.65266406\n",
            "Iteration 176, loss = 0.65181539\n",
            "Iteration 177, loss = 0.65095565\n",
            "Iteration 178, loss = 0.65009933\n",
            "Iteration 179, loss = 0.64923834\n",
            "Iteration 180, loss = 0.64833909\n",
            "Iteration 181, loss = 0.64741986\n",
            "Iteration 182, loss = 0.64649299\n",
            "Iteration 183, loss = 0.64554373\n",
            "Iteration 184, loss = 0.64456604\n",
            "Iteration 185, loss = 0.64357347\n",
            "Iteration 186, loss = 0.64257881\n",
            "Iteration 187, loss = 0.64158807\n",
            "Iteration 188, loss = 0.64057136\n",
            "Iteration 189, loss = 0.63953690\n",
            "Iteration 190, loss = 0.63846132\n",
            "Iteration 191, loss = 0.63730320\n",
            "Iteration 192, loss = 0.63607467\n",
            "Iteration 193, loss = 0.63477597\n",
            "Iteration 194, loss = 0.63341281\n",
            "Iteration 195, loss = 0.63202569\n",
            "Iteration 196, loss = 0.63060573\n",
            "Iteration 197, loss = 0.62914410\n",
            "Iteration 198, loss = 0.62764931\n",
            "Iteration 199, loss = 0.62607988\n",
            "Iteration 200, loss = 0.62447995\n",
            "Iteration 201, loss = 0.62287094\n",
            "Iteration 202, loss = 0.62125895\n",
            "Iteration 203, loss = 0.61963315\n",
            "Iteration 204, loss = 0.61799223\n",
            "Iteration 205, loss = 0.61635161\n",
            "Iteration 206, loss = 0.61472065\n",
            "Iteration 207, loss = 0.61308882\n",
            "Iteration 208, loss = 0.61144580\n",
            "Iteration 209, loss = 0.60976286\n",
            "Iteration 210, loss = 0.60807891\n",
            "Iteration 211, loss = 0.60639388\n",
            "Iteration 212, loss = 0.60470991\n",
            "Iteration 213, loss = 0.60302808\n",
            "Iteration 214, loss = 0.60134712\n",
            "Iteration 215, loss = 0.59966906\n",
            "Iteration 216, loss = 0.59801649\n",
            "Iteration 217, loss = 0.59638964\n",
            "Iteration 218, loss = 0.59476923\n",
            "Iteration 219, loss = 0.59315234\n",
            "Iteration 220, loss = 0.59153819\n",
            "Iteration 221, loss = 0.58992690\n",
            "Iteration 222, loss = 0.58831879\n",
            "Iteration 223, loss = 0.58671422\n",
            "Iteration 224, loss = 0.58511844\n",
            "Iteration 225, loss = 0.58352853\n",
            "Iteration 226, loss = 0.58191057\n",
            "Iteration 227, loss = 0.58027667\n",
            "Iteration 228, loss = 0.57864571\n",
            "Iteration 229, loss = 0.57701917\n",
            "Iteration 230, loss = 0.57539509\n",
            "Iteration 231, loss = 0.57377385\n",
            "Iteration 232, loss = 0.57215557\n",
            "Iteration 233, loss = 0.57054875\n",
            "Iteration 234, loss = 0.56895768\n",
            "Iteration 235, loss = 0.56737178\n",
            "Iteration 236, loss = 0.56579101\n",
            "Iteration 237, loss = 0.56420838\n",
            "Iteration 238, loss = 0.56256434\n",
            "Iteration 239, loss = 0.56091793\n",
            "Iteration 240, loss = 0.55924093\n",
            "Iteration 241, loss = 0.55744567\n",
            "Iteration 242, loss = 0.55563213\n",
            "Iteration 243, loss = 0.55380328\n",
            "Iteration 244, loss = 0.55194223\n",
            "Iteration 245, loss = 0.55003240\n",
            "Iteration 246, loss = 0.54811157\n",
            "Iteration 247, loss = 0.54618374\n",
            "Iteration 248, loss = 0.54425159\n",
            "Iteration 249, loss = 0.54231701\n",
            "Iteration 250, loss = 0.54038084\n",
            "Iteration 251, loss = 0.53844544\n",
            "Iteration 252, loss = 0.53651223\n",
            "Iteration 253, loss = 0.53458214\n",
            "Iteration 254, loss = 0.53265561\n",
            "Iteration 255, loss = 0.53073470\n",
            "Iteration 256, loss = 0.52882002\n",
            "Iteration 257, loss = 0.52691261\n",
            "Iteration 258, loss = 0.52501182\n",
            "Iteration 259, loss = 0.52311820\n",
            "Iteration 260, loss = 0.52123325\n",
            "Iteration 261, loss = 0.51935814\n",
            "Iteration 262, loss = 0.51749281\n",
            "Iteration 263, loss = 0.51563604\n",
            "Iteration 264, loss = 0.51378800\n",
            "Iteration 265, loss = 0.51195221\n",
            "Iteration 266, loss = 0.51012768\n",
            "Iteration 267, loss = 0.50831263\n",
            "Iteration 268, loss = 0.50650645\n",
            "Iteration 269, loss = 0.50471072\n",
            "Iteration 270, loss = 0.50292297\n",
            "Iteration 271, loss = 0.50114453\n",
            "Iteration 272, loss = 0.49937395\n",
            "Iteration 273, loss = 0.49761105\n",
            "Iteration 274, loss = 0.49585818\n",
            "Iteration 275, loss = 0.49413211\n",
            "Iteration 276, loss = 0.49241784\n",
            "Iteration 277, loss = 0.49071443\n",
            "Iteration 278, loss = 0.48901963\n",
            "Iteration 279, loss = 0.48729455\n",
            "Iteration 280, loss = 0.48551928\n",
            "Iteration 281, loss = 0.48374000\n",
            "Iteration 282, loss = 0.48192286\n",
            "Iteration 283, loss = 0.48003191\n",
            "Iteration 284, loss = 0.47818474\n",
            "Iteration 285, loss = 0.47634281\n",
            "Iteration 286, loss = 0.47450273\n",
            "Iteration 287, loss = 0.47266469\n",
            "Iteration 288, loss = 0.47082929\n",
            "Iteration 289, loss = 0.46899916\n",
            "Iteration 290, loss = 0.46717598\n",
            "Iteration 291, loss = 0.46535870\n",
            "Iteration 292, loss = 0.46354772\n",
            "Iteration 293, loss = 0.46174147\n",
            "Iteration 294, loss = 0.45994239\n",
            "Iteration 295, loss = 0.45815114\n",
            "Iteration 296, loss = 0.45636653\n",
            "Iteration 297, loss = 0.45459140\n",
            "Iteration 298, loss = 0.45282740\n",
            "Iteration 299, loss = 0.45107675\n",
            "Iteration 300, loss = 0.44933408\n",
            "Iteration 301, loss = 0.44759948\n",
            "Iteration 302, loss = 0.44587371\n",
            "Iteration 303, loss = 0.44415754\n",
            "Iteration 304, loss = 0.44245101\n",
            "Iteration 305, loss = 0.44075354\n",
            "Iteration 306, loss = 0.43906592\n",
            "Iteration 307, loss = 0.43738982\n",
            "Iteration 308, loss = 0.43572602\n",
            "Iteration 309, loss = 0.43407115\n",
            "Iteration 310, loss = 0.43242623\n",
            "Iteration 311, loss = 0.43079071\n",
            "Iteration 312, loss = 0.42916415\n",
            "Iteration 313, loss = 0.42754646\n",
            "Iteration 314, loss = 0.42593831\n",
            "Iteration 315, loss = 0.42433972\n",
            "Iteration 316, loss = 0.42275119\n",
            "Iteration 317, loss = 0.42117267\n",
            "Iteration 318, loss = 0.41960315\n",
            "Iteration 319, loss = 0.41804301\n",
            "Iteration 320, loss = 0.41649228\n",
            "Iteration 321, loss = 0.41495348\n",
            "Iteration 322, loss = 0.41342432\n",
            "Iteration 323, loss = 0.41190583\n",
            "Iteration 324, loss = 0.41039804\n",
            "Iteration 325, loss = 0.40889787\n",
            "Iteration 326, loss = 0.40740713\n",
            "Iteration 327, loss = 0.40592594\n",
            "Iteration 328, loss = 0.40445499\n",
            "Iteration 329, loss = 0.40299396\n",
            "Iteration 330, loss = 0.40154112\n",
            "Iteration 331, loss = 0.40009188\n",
            "Iteration 332, loss = 0.39865251\n",
            "Iteration 333, loss = 0.39722310\n",
            "Iteration 334, loss = 0.39580278\n",
            "Iteration 335, loss = 0.39439247\n",
            "Iteration 336, loss = 0.39299199\n",
            "Iteration 337, loss = 0.39160103\n",
            "Iteration 338, loss = 0.39021904\n",
            "Iteration 339, loss = 0.38884710\n",
            "Iteration 340, loss = 0.38749823\n",
            "Iteration 341, loss = 0.38616196\n",
            "Iteration 342, loss = 0.38483520\n",
            "Iteration 343, loss = 0.38351761\n",
            "Iteration 344, loss = 0.38220938\n",
            "Iteration 345, loss = 0.38091124\n",
            "Iteration 346, loss = 0.37962254\n",
            "Iteration 347, loss = 0.37834303\n",
            "Iteration 348, loss = 0.37707386\n",
            "Iteration 349, loss = 0.37581483\n",
            "Iteration 350, loss = 0.37456584\n",
            "Iteration 351, loss = 0.37332643\n",
            "Iteration 352, loss = 0.37209826\n",
            "Iteration 353, loss = 0.37087775\n",
            "Iteration 354, loss = 0.36966526\n",
            "Iteration 355, loss = 0.36846165\n",
            "Iteration 356, loss = 0.36726901\n",
            "Iteration 357, loss = 0.36608569\n",
            "Iteration 358, loss = 0.36491681\n",
            "Iteration 359, loss = 0.36375681\n",
            "Iteration 360, loss = 0.36260533\n",
            "Iteration 361, loss = 0.36146186\n",
            "Iteration 362, loss = 0.36032575\n",
            "Iteration 363, loss = 0.35919716\n",
            "Iteration 364, loss = 0.35807661\n",
            "Iteration 365, loss = 0.35697034\n",
            "Iteration 366, loss = 0.35587089\n",
            "Iteration 367, loss = 0.35477857\n",
            "Iteration 368, loss = 0.35369316\n",
            "Iteration 369, loss = 0.35261510\n",
            "Iteration 370, loss = 0.35154405\n",
            "Iteration 371, loss = 0.35048019\n",
            "Iteration 372, loss = 0.34942445\n",
            "Iteration 373, loss = 0.34837529\n",
            "Iteration 374, loss = 0.34733273\n",
            "Iteration 375, loss = 0.34629545\n",
            "Iteration 376, loss = 0.34526288\n",
            "Iteration 377, loss = 0.34423353\n",
            "Iteration 378, loss = 0.34321085\n",
            "Iteration 379, loss = 0.34219284\n",
            "Iteration 380, loss = 0.34118073\n",
            "Iteration 381, loss = 0.34017357\n",
            "Iteration 382, loss = 0.33917203\n",
            "Iteration 383, loss = 0.33817579\n",
            "Iteration 384, loss = 0.33718574\n",
            "Iteration 385, loss = 0.33620286\n",
            "Iteration 386, loss = 0.33522451\n",
            "Iteration 387, loss = 0.33425147\n",
            "Iteration 388, loss = 0.33328533\n",
            "Iteration 389, loss = 0.33232361\n",
            "Iteration 390, loss = 0.33136704\n",
            "Iteration 391, loss = 0.33041558\n",
            "Iteration 392, loss = 0.32947283\n",
            "Iteration 393, loss = 0.32853912\n",
            "Iteration 394, loss = 0.32760974\n",
            "Iteration 395, loss = 0.32668493\n",
            "Iteration 396, loss = 0.32576459\n",
            "Iteration 397, loss = 0.32484897\n",
            "Iteration 398, loss = 0.32393741\n",
            "Iteration 399, loss = 0.32303034\n",
            "Iteration 400, loss = 0.32212783\n",
            "Iteration 401, loss = 0.32122936\n",
            "Iteration 402, loss = 0.32033361\n",
            "Iteration 403, loss = 0.31944097\n",
            "Iteration 404, loss = 0.31855114\n",
            "Iteration 405, loss = 0.31766441\n",
            "Iteration 406, loss = 0.31678126\n",
            "Iteration 407, loss = 0.31590134\n",
            "Iteration 408, loss = 0.31502575\n",
            "Iteration 409, loss = 0.31415366\n",
            "Iteration 410, loss = 0.31328540\n",
            "Iteration 411, loss = 0.31241918\n",
            "Iteration 412, loss = 0.31155629\n",
            "Iteration 413, loss = 0.31069647\n",
            "Iteration 414, loss = 0.30983955\n",
            "Iteration 415, loss = 0.30898544\n",
            "Iteration 416, loss = 0.30813352\n",
            "Iteration 417, loss = 0.30728389\n",
            "Iteration 418, loss = 0.30643710\n",
            "Iteration 419, loss = 0.30559307\n",
            "Iteration 420, loss = 0.30475155\n",
            "Iteration 421, loss = 0.30391404\n",
            "Iteration 422, loss = 0.30307922\n",
            "Iteration 423, loss = 0.30224687\n",
            "Iteration 424, loss = 0.30141780\n",
            "Iteration 425, loss = 0.30058859\n",
            "Iteration 426, loss = 0.29976128\n",
            "Iteration 427, loss = 0.29893797\n",
            "Iteration 428, loss = 0.29811666\n",
            "Iteration 429, loss = 0.29729751\n",
            "Iteration 430, loss = 0.29648102\n",
            "Iteration 431, loss = 0.29566723\n",
            "Iteration 432, loss = 0.29485819\n",
            "Iteration 433, loss = 0.29405349\n",
            "Iteration 434, loss = 0.29325129\n",
            "Iteration 435, loss = 0.29245131\n",
            "Iteration 436, loss = 0.29165390\n",
            "Iteration 437, loss = 0.29085985\n",
            "Iteration 438, loss = 0.29006724\n",
            "Iteration 439, loss = 0.28927770\n",
            "Iteration 440, loss = 0.28849032\n",
            "Iteration 441, loss = 0.28770568\n",
            "Iteration 442, loss = 0.28692331\n",
            "Iteration 443, loss = 0.28614302\n",
            "Iteration 444, loss = 0.28536568\n",
            "Iteration 445, loss = 0.28458924\n",
            "Iteration 446, loss = 0.28381465\n",
            "Iteration 447, loss = 0.28304221\n",
            "Iteration 448, loss = 0.28227165\n",
            "Iteration 449, loss = 0.28150414\n",
            "Iteration 450, loss = 0.28075129\n",
            "Iteration 451, loss = 0.28000024\n",
            "Iteration 452, loss = 0.27925250\n",
            "Iteration 453, loss = 0.27851119\n",
            "Iteration 454, loss = 0.27777203\n",
            "Iteration 455, loss = 0.27703687\n",
            "Iteration 456, loss = 0.27630431\n",
            "Iteration 457, loss = 0.27557457\n",
            "Iteration 458, loss = 0.27484690\n",
            "Iteration 459, loss = 0.27411964\n",
            "Iteration 460, loss = 0.27339408\n",
            "Iteration 461, loss = 0.27267124\n",
            "Iteration 462, loss = 0.27195060\n",
            "Iteration 463, loss = 0.27123217\n",
            "Iteration 464, loss = 0.27051573\n",
            "Iteration 465, loss = 0.26980759\n",
            "Iteration 466, loss = 0.26910270\n",
            "Iteration 467, loss = 0.26839978\n",
            "Iteration 468, loss = 0.26769880\n",
            "Iteration 469, loss = 0.26700686\n",
            "Iteration 470, loss = 0.26632347\n",
            "Iteration 471, loss = 0.26564152\n",
            "Iteration 472, loss = 0.26496164\n",
            "Iteration 473, loss = 0.26428403\n",
            "Iteration 474, loss = 0.26360854\n",
            "Iteration 475, loss = 0.26293488\n",
            "Iteration 476, loss = 0.26226271\n",
            "Iteration 477, loss = 0.26159195\n",
            "Iteration 478, loss = 0.26092298\n",
            "Iteration 479, loss = 0.26026556\n",
            "Iteration 480, loss = 0.25962164\n",
            "Iteration 481, loss = 0.25898123\n",
            "Iteration 482, loss = 0.25834383\n",
            "Iteration 483, loss = 0.25770834\n",
            "Iteration 484, loss = 0.25707540\n",
            "Iteration 485, loss = 0.25644452\n",
            "Iteration 486, loss = 0.25581480\n",
            "Iteration 487, loss = 0.25516400\n",
            "Iteration 488, loss = 0.25415312\n",
            "Iteration 489, loss = 0.25305843\n",
            "Iteration 490, loss = 0.25189820\n",
            "Iteration 491, loss = 0.25068810\n",
            "Iteration 492, loss = 0.24944521\n",
            "Iteration 493, loss = 0.24818628\n",
            "Iteration 494, loss = 0.24692588\n",
            "Iteration 495, loss = 0.24567812\n",
            "Iteration 496, loss = 0.24445417\n",
            "Iteration 497, loss = 0.24326489\n",
            "Iteration 498, loss = 0.24211974\n",
            "Iteration 499, loss = 0.24102526\n",
            "Iteration 500, loss = 0.23997963\n",
            "Iteration 501, loss = 0.23898583\n",
            "Iteration 502, loss = 0.23804583\n",
            "Iteration 503, loss = 0.23715391\n",
            "Iteration 504, loss = 0.23630402\n",
            "Iteration 505, loss = 0.23549524\n",
            "Iteration 506, loss = 0.23472400\n",
            "Iteration 507, loss = 0.23398458\n",
            "Iteration 508, loss = 0.23327371\n",
            "Iteration 509, loss = 0.23258878\n",
            "Iteration 510, loss = 0.23192605\n",
            "Iteration 511, loss = 0.23128305\n",
            "Iteration 512, loss = 0.23065831\n",
            "Iteration 513, loss = 0.23005190\n",
            "Iteration 514, loss = 0.22946035\n",
            "Iteration 515, loss = 0.22888211\n",
            "Iteration 516, loss = 0.22831488\n",
            "Iteration 517, loss = 0.22775788\n",
            "Iteration 518, loss = 0.22721087\n",
            "Iteration 519, loss = 0.22667339\n",
            "Iteration 520, loss = 0.22614479\n",
            "Iteration 521, loss = 0.22562432\n",
            "Iteration 522, loss = 0.22511183\n",
            "Iteration 523, loss = 0.22460652\n",
            "Iteration 524, loss = 0.22410855\n",
            "Iteration 525, loss = 0.22361735\n",
            "Iteration 526, loss = 0.22313218\n",
            "Iteration 527, loss = 0.22265380\n",
            "Iteration 528, loss = 0.22218183\n",
            "Iteration 529, loss = 0.22171469\n",
            "Iteration 530, loss = 0.22125292\n",
            "Iteration 531, loss = 0.22079621\n",
            "Iteration 532, loss = 0.22034460\n",
            "Iteration 533, loss = 0.21989726\n",
            "Iteration 534, loss = 0.21945461\n",
            "Iteration 535, loss = 0.21901588\n",
            "Iteration 536, loss = 0.21858117\n",
            "Iteration 537, loss = 0.21815057\n",
            "Iteration 538, loss = 0.21772385\n",
            "Iteration 539, loss = 0.21730048\n",
            "Iteration 540, loss = 0.21688054\n",
            "Iteration 541, loss = 0.21646411\n",
            "Iteration 542, loss = 0.21605124\n",
            "Iteration 543, loss = 0.21564107\n",
            "Iteration 544, loss = 0.21523442\n",
            "Iteration 545, loss = 0.21483073\n",
            "Iteration 546, loss = 0.21443016\n",
            "Iteration 547, loss = 0.21403236\n",
            "Iteration 548, loss = 0.21363735\n",
            "Iteration 549, loss = 0.21324511\n",
            "Iteration 550, loss = 0.21285568\n",
            "Iteration 551, loss = 0.21247067\n",
            "Iteration 552, loss = 0.21208745\n",
            "Iteration 553, loss = 0.21170728\n",
            "Iteration 554, loss = 0.21133001\n",
            "Iteration 555, loss = 0.21095599\n",
            "Iteration 556, loss = 0.21058477\n",
            "Iteration 557, loss = 0.21021609\n",
            "Iteration 558, loss = 0.20985041\n",
            "Iteration 559, loss = 0.20948744\n",
            "Iteration 560, loss = 0.20912669\n",
            "Iteration 561, loss = 0.20876829\n",
            "Iteration 562, loss = 0.20841255\n",
            "Iteration 563, loss = 0.20805949\n",
            "Iteration 564, loss = 0.20770818\n",
            "Iteration 565, loss = 0.20735914\n",
            "Iteration 566, loss = 0.20701249\n",
            "Iteration 567, loss = 0.20666756\n",
            "Iteration 568, loss = 0.20632491\n",
            "Iteration 569, loss = 0.20598443\n",
            "Iteration 570, loss = 0.20564584\n",
            "Iteration 571, loss = 0.20530943\n",
            "Iteration 572, loss = 0.20497478\n",
            "Iteration 573, loss = 0.20464336\n",
            "Iteration 574, loss = 0.20431313\n",
            "Iteration 575, loss = 0.20398517\n",
            "Iteration 576, loss = 0.20365922\n",
            "Iteration 577, loss = 0.20333480\n",
            "Iteration 578, loss = 0.20301215\n",
            "Iteration 579, loss = 0.20269166\n",
            "Iteration 580, loss = 0.20237245\n",
            "Iteration 581, loss = 0.20205479\n",
            "Iteration 582, loss = 0.20173890\n",
            "Iteration 583, loss = 0.20142460\n",
            "Iteration 584, loss = 0.20111227\n",
            "Iteration 585, loss = 0.20080139\n",
            "Iteration 586, loss = 0.20049246\n",
            "Iteration 587, loss = 0.20018481\n",
            "Iteration 588, loss = 0.19987937\n",
            "Iteration 589, loss = 0.19957549\n",
            "Iteration 590, loss = 0.19927293\n",
            "Iteration 591, loss = 0.19897208\n",
            "Iteration 592, loss = 0.19867328\n",
            "Iteration 593, loss = 0.19837625\n",
            "Iteration 594, loss = 0.19808000\n",
            "Iteration 595, loss = 0.19778521\n",
            "Iteration 596, loss = 0.19749262\n",
            "Iteration 597, loss = 0.19720165\n",
            "Iteration 598, loss = 0.19691157\n",
            "Iteration 599, loss = 0.19662308\n",
            "Iteration 600, loss = 0.19633608\n",
            "Iteration 601, loss = 0.19604996\n",
            "Iteration 602, loss = 0.19576512\n",
            "Iteration 603, loss = 0.19548190\n",
            "Iteration 604, loss = 0.19519936\n",
            "Iteration 605, loss = 0.19491821\n",
            "Iteration 606, loss = 0.19463866\n",
            "Iteration 607, loss = 0.19436001\n",
            "Iteration 608, loss = 0.19408309\n",
            "Iteration 609, loss = 0.19380694\n",
            "Iteration 610, loss = 0.19353228\n",
            "Iteration 611, loss = 0.19325875\n",
            "Iteration 612, loss = 0.19298703\n",
            "Iteration 613, loss = 0.19271707\n",
            "Iteration 614, loss = 0.19244927\n",
            "Iteration 615, loss = 0.19218240\n",
            "Iteration 616, loss = 0.19191640\n",
            "Iteration 617, loss = 0.19165199\n",
            "Iteration 618, loss = 0.19138851\n",
            "Iteration 619, loss = 0.19112590\n",
            "Iteration 620, loss = 0.19086475\n",
            "Iteration 621, loss = 0.19060429\n",
            "Iteration 622, loss = 0.19034515\n",
            "Iteration 623, loss = 0.19008668\n",
            "Iteration 624, loss = 0.18982930\n",
            "Iteration 625, loss = 0.18957326\n",
            "Iteration 626, loss = 0.18931806\n",
            "Iteration 627, loss = 0.18906393\n",
            "Iteration 628, loss = 0.18881175\n",
            "Iteration 629, loss = 0.18856055\n",
            "Iteration 630, loss = 0.18831052\n",
            "Iteration 631, loss = 0.18806161\n",
            "Iteration 632, loss = 0.18781369\n",
            "Iteration 633, loss = 0.18756665\n",
            "Iteration 634, loss = 0.18732059\n",
            "Iteration 635, loss = 0.18707579\n",
            "Iteration 636, loss = 0.18683173\n",
            "Iteration 637, loss = 0.18658859\n",
            "Iteration 638, loss = 0.18634712\n",
            "Iteration 639, loss = 0.18610576\n",
            "Iteration 640, loss = 0.18586560\n",
            "Iteration 641, loss = 0.18562654\n",
            "Iteration 642, loss = 0.18538827\n",
            "Iteration 643, loss = 0.18515068\n",
            "Iteration 644, loss = 0.18491392\n",
            "Iteration 645, loss = 0.18467855\n",
            "Iteration 646, loss = 0.18444356\n",
            "Iteration 647, loss = 0.18420935\n",
            "Iteration 648, loss = 0.18397571\n",
            "Iteration 649, loss = 0.18374307\n",
            "Iteration 650, loss = 0.18351109\n",
            "Iteration 651, loss = 0.18327988\n",
            "Iteration 652, loss = 0.18304977\n",
            "Iteration 653, loss = 0.18282057\n",
            "Iteration 654, loss = 0.18259163\n",
            "Iteration 655, loss = 0.18236426\n",
            "Iteration 656, loss = 0.18213725\n",
            "Iteration 657, loss = 0.18191120\n",
            "Iteration 658, loss = 0.18168587\n",
            "Iteration 659, loss = 0.18146203\n",
            "Iteration 660, loss = 0.18123870\n",
            "Iteration 661, loss = 0.18101751\n",
            "Iteration 662, loss = 0.18079712\n",
            "Iteration 663, loss = 0.18057739\n",
            "Iteration 664, loss = 0.18035827\n",
            "Iteration 665, loss = 0.18014017\n",
            "Iteration 666, loss = 0.17992421\n",
            "Iteration 667, loss = 0.17970880\n",
            "Iteration 668, loss = 0.17949401\n",
            "Iteration 669, loss = 0.17927928\n",
            "Iteration 670, loss = 0.17906490\n",
            "Iteration 671, loss = 0.17885147\n",
            "Iteration 672, loss = 0.17863891\n",
            "Iteration 673, loss = 0.17842688\n",
            "Iteration 674, loss = 0.17821597\n",
            "Iteration 675, loss = 0.17800558\n",
            "Iteration 676, loss = 0.17779557\n",
            "Iteration 677, loss = 0.17758665\n",
            "Iteration 678, loss = 0.17737826\n",
            "Iteration 679, loss = 0.17717008\n",
            "Iteration 680, loss = 0.17696302\n",
            "Iteration 681, loss = 0.17675616\n",
            "Iteration 682, loss = 0.17655041\n",
            "Iteration 683, loss = 0.17634552\n",
            "Iteration 684, loss = 0.17614107\n",
            "Iteration 685, loss = 0.17593751\n",
            "Iteration 686, loss = 0.17573441\n",
            "Iteration 687, loss = 0.17553206\n",
            "Iteration 688, loss = 0.17533034\n",
            "Iteration 689, loss = 0.17512866\n",
            "Iteration 690, loss = 0.17492780\n",
            "Iteration 691, loss = 0.17472794\n",
            "Iteration 692, loss = 0.17452774\n",
            "Iteration 693, loss = 0.17432885\n",
            "Iteration 694, loss = 0.17412991\n",
            "Iteration 695, loss = 0.17393184\n",
            "Iteration 696, loss = 0.17373448\n",
            "Iteration 697, loss = 0.17353830\n",
            "Iteration 698, loss = 0.17334289\n",
            "Iteration 699, loss = 0.17314808\n",
            "Iteration 700, loss = 0.17295343\n",
            "Iteration 701, loss = 0.17275966\n",
            "Iteration 702, loss = 0.17256675\n",
            "Iteration 703, loss = 0.17237367\n",
            "Iteration 704, loss = 0.17218134\n",
            "Iteration 705, loss = 0.17198905\n",
            "Iteration 706, loss = 0.17179677\n",
            "Iteration 707, loss = 0.17160393\n",
            "Iteration 708, loss = 0.17141128\n",
            "Iteration 709, loss = 0.17121920\n",
            "Iteration 710, loss = 0.17102687\n",
            "Iteration 711, loss = 0.17083571\n",
            "Iteration 712, loss = 0.17064588\n",
            "Iteration 713, loss = 0.17045623\n",
            "Iteration 714, loss = 0.17026621\n",
            "Iteration 715, loss = 0.17007584\n",
            "Iteration 716, loss = 0.16988630\n",
            "Iteration 717, loss = 0.16969788\n",
            "Iteration 718, loss = 0.16950974\n",
            "Iteration 719, loss = 0.16932283\n",
            "Iteration 720, loss = 0.16913631\n",
            "Iteration 721, loss = 0.16895136\n",
            "Iteration 722, loss = 0.16876547\n",
            "Iteration 723, loss = 0.16858124\n",
            "Iteration 724, loss = 0.16839683\n",
            "Iteration 725, loss = 0.16821289\n",
            "Iteration 726, loss = 0.16802988\n",
            "Iteration 727, loss = 0.16784688\n",
            "Iteration 728, loss = 0.16766418\n",
            "Iteration 729, loss = 0.16748317\n",
            "Iteration 730, loss = 0.16730171\n",
            "Iteration 731, loss = 0.16712084\n",
            "Iteration 732, loss = 0.16694054\n",
            "Iteration 733, loss = 0.16676207\n",
            "Iteration 734, loss = 0.16658300\n",
            "Iteration 735, loss = 0.16640465\n",
            "Iteration 736, loss = 0.16622667\n",
            "Iteration 737, loss = 0.16605115\n",
            "Iteration 738, loss = 0.16587519\n",
            "Iteration 739, loss = 0.16570025\n",
            "Iteration 740, loss = 0.16552556\n",
            "Iteration 741, loss = 0.16535210\n",
            "Iteration 742, loss = 0.16517849\n",
            "Iteration 743, loss = 0.16500547\n",
            "Iteration 744, loss = 0.16483247\n",
            "Iteration 745, loss = 0.16466003\n",
            "Iteration 746, loss = 0.16448828\n",
            "Iteration 747, loss = 0.16431692\n",
            "Iteration 748, loss = 0.16414528\n",
            "Iteration 749, loss = 0.16397479\n",
            "Iteration 750, loss = 0.16380379\n",
            "Iteration 751, loss = 0.16363300\n",
            "Iteration 752, loss = 0.16346282\n",
            "Iteration 753, loss = 0.16329235\n",
            "Iteration 754, loss = 0.16312291\n",
            "Iteration 755, loss = 0.16295367\n",
            "Iteration 756, loss = 0.16278487\n",
            "Iteration 757, loss = 0.16261577\n",
            "Iteration 758, loss = 0.16244728\n",
            "Iteration 759, loss = 0.16227971\n",
            "Iteration 760, loss = 0.16211211\n",
            "Iteration 761, loss = 0.16194528\n",
            "Iteration 762, loss = 0.16177830\n",
            "Iteration 763, loss = 0.16161212\n",
            "Iteration 764, loss = 0.16144596\n",
            "Iteration 765, loss = 0.16128013\n",
            "Iteration 766, loss = 0.16111479\n",
            "Iteration 767, loss = 0.16095004\n",
            "Iteration 768, loss = 0.16078559\n",
            "Iteration 769, loss = 0.16062114\n",
            "Iteration 770, loss = 0.16045693\n",
            "Iteration 771, loss = 0.16029254\n",
            "Iteration 772, loss = 0.16012872\n",
            "Iteration 773, loss = 0.15996472\n",
            "Iteration 774, loss = 0.15980054\n",
            "Iteration 775, loss = 0.15963723\n",
            "Iteration 776, loss = 0.15947411\n",
            "Iteration 777, loss = 0.15931072\n",
            "Iteration 778, loss = 0.15914769\n",
            "Iteration 779, loss = 0.15898496\n",
            "Iteration 780, loss = 0.15882313\n",
            "Iteration 781, loss = 0.15866063\n",
            "Iteration 782, loss = 0.15849914\n",
            "Iteration 783, loss = 0.15833788\n",
            "Iteration 784, loss = 0.15817693\n",
            "Iteration 785, loss = 0.15801635\n",
            "Iteration 786, loss = 0.15785736\n",
            "Iteration 787, loss = 0.15769995\n",
            "Iteration 788, loss = 0.15754282\n",
            "Iteration 789, loss = 0.15738582\n",
            "Iteration 790, loss = 0.15722891\n",
            "Iteration 791, loss = 0.15707333\n",
            "Iteration 792, loss = 0.15691681\n",
            "Iteration 793, loss = 0.15676095\n",
            "Iteration 794, loss = 0.15660512\n",
            "Iteration 795, loss = 0.15645029\n",
            "Iteration 796, loss = 0.15629546\n",
            "Iteration 797, loss = 0.15614123\n",
            "Iteration 798, loss = 0.15598708\n",
            "Iteration 799, loss = 0.15583328\n",
            "Iteration 800, loss = 0.15568062\n",
            "Iteration 1, loss = 0.70160442\n",
            "Iteration 2, loss = 0.70082597\n",
            "Iteration 3, loss = 0.69973019\n",
            "Iteration 4, loss = 0.69844766\n",
            "Iteration 5, loss = 0.69704301\n",
            "Iteration 6, loss = 0.69551348\n",
            "Iteration 7, loss = 0.69402982\n",
            "Iteration 8, loss = 0.69263599\n",
            "Iteration 9, loss = 0.69150008\n",
            "Iteration 10, loss = 0.69037700\n",
            "Iteration 11, loss = 0.68936266\n",
            "Iteration 12, loss = 0.68856492\n",
            "Iteration 13, loss = 0.68785581\n",
            "Iteration 14, loss = 0.68728103\n",
            "Iteration 15, loss = 0.68694582\n",
            "Iteration 16, loss = 0.68669798\n",
            "Iteration 17, loss = 0.68651246\n",
            "Iteration 18, loss = 0.68637303\n",
            "Iteration 19, loss = 0.68626805\n",
            "Iteration 20, loss = 0.68611614\n",
            "Iteration 21, loss = 0.68596322\n",
            "Iteration 22, loss = 0.68581110\n",
            "Iteration 23, loss = 0.68567903\n",
            "Iteration 24, loss = 0.68557367\n",
            "Iteration 25, loss = 0.68546363\n",
            "Iteration 26, loss = 0.68534913\n",
            "Iteration 27, loss = 0.68523028\n",
            "Iteration 28, loss = 0.68510721\n",
            "Iteration 29, loss = 0.68498021\n",
            "Iteration 30, loss = 0.68484941\n",
            "Iteration 31, loss = 0.68471500\n",
            "Iteration 32, loss = 0.68457722\n",
            "Iteration 33, loss = 0.68443599\n",
            "Iteration 34, loss = 0.68429152\n",
            "Iteration 35, loss = 0.68414395\n",
            "Iteration 36, loss = 0.68399335\n",
            "Iteration 37, loss = 0.68383975\n",
            "Iteration 38, loss = 0.68368319\n",
            "Iteration 39, loss = 0.68352371\n",
            "Iteration 40, loss = 0.68336135\n",
            "Iteration 41, loss = 0.68319617\n",
            "Iteration 42, loss = 0.68302814\n",
            "Iteration 43, loss = 0.68285753\n",
            "Iteration 44, loss = 0.68268417\n",
            "Iteration 45, loss = 0.68250805\n",
            "Iteration 46, loss = 0.68232917\n",
            "Iteration 47, loss = 0.68214753\n",
            "Iteration 48, loss = 0.68196307\n",
            "Iteration 49, loss = 0.68177580\n",
            "Iteration 50, loss = 0.68158571\n",
            "Iteration 51, loss = 0.68139283\n",
            "Iteration 52, loss = 0.68119706\n",
            "Iteration 53, loss = 0.68099843\n",
            "Iteration 54, loss = 0.68079693\n",
            "Iteration 55, loss = 0.68059250\n",
            "Iteration 56, loss = 0.68038515\n",
            "Iteration 57, loss = 0.68017489\n",
            "Iteration 58, loss = 0.67996164\n",
            "Iteration 59, loss = 0.67974535\n",
            "Iteration 60, loss = 0.67952600\n",
            "Iteration 61, loss = 0.67930363\n",
            "Iteration 62, loss = 0.67907812\n",
            "Iteration 63, loss = 0.67884944\n",
            "Iteration 64, loss = 0.67861796\n",
            "Iteration 65, loss = 0.67838318\n",
            "Iteration 66, loss = 0.67814492\n",
            "Iteration 67, loss = 0.67790293\n",
            "Iteration 68, loss = 0.67765758\n",
            "Iteration 69, loss = 0.67740857\n",
            "Iteration 70, loss = 0.67715598\n",
            "Iteration 71, loss = 0.67689986\n",
            "Iteration 72, loss = 0.67664562\n",
            "Iteration 73, loss = 0.67639288\n",
            "Iteration 74, loss = 0.67613615\n",
            "Iteration 75, loss = 0.67587541\n",
            "Iteration 76, loss = 0.67561066\n",
            "Iteration 77, loss = 0.67534187\n",
            "Iteration 78, loss = 0.67506901\n",
            "Iteration 79, loss = 0.67479206\n",
            "Iteration 80, loss = 0.67451126\n",
            "Iteration 81, loss = 0.67422554\n",
            "Iteration 82, loss = 0.67393589\n",
            "Iteration 83, loss = 0.67364210\n",
            "Iteration 84, loss = 0.67334390\n",
            "Iteration 85, loss = 0.67304124\n",
            "Iteration 86, loss = 0.67273495\n",
            "Iteration 87, loss = 0.67242339\n",
            "Iteration 88, loss = 0.67210703\n",
            "Iteration 89, loss = 0.67178634\n",
            "Iteration 90, loss = 0.67146161\n",
            "Iteration 91, loss = 0.67113109\n",
            "Iteration 92, loss = 0.67079277\n",
            "Iteration 93, loss = 0.67044523\n",
            "Iteration 94, loss = 0.67009215\n",
            "Iteration 95, loss = 0.66973376\n",
            "Iteration 96, loss = 0.66937014\n",
            "Iteration 97, loss = 0.66900077\n",
            "Iteration 98, loss = 0.66862586\n",
            "Iteration 99, loss = 0.66824585\n",
            "Iteration 100, loss = 0.66786147\n",
            "Iteration 101, loss = 0.66747123\n",
            "Iteration 102, loss = 0.66707585\n",
            "Iteration 103, loss = 0.66667499\n",
            "Iteration 104, loss = 0.66626885\n",
            "Iteration 105, loss = 0.66585655\n",
            "Iteration 106, loss = 0.66543435\n",
            "Iteration 107, loss = 0.66500681\n",
            "Iteration 108, loss = 0.66457321\n",
            "Iteration 109, loss = 0.66413356\n",
            "Iteration 110, loss = 0.66368797\n",
            "Iteration 111, loss = 0.66323637\n",
            "Iteration 112, loss = 0.66278315\n",
            "Iteration 113, loss = 0.66232244\n",
            "Iteration 114, loss = 0.66185647\n",
            "Iteration 115, loss = 0.66138107\n",
            "Iteration 116, loss = 0.66089232\n",
            "Iteration 117, loss = 0.66039614\n",
            "Iteration 118, loss = 0.65989296\n",
            "Iteration 119, loss = 0.65938258\n",
            "Iteration 120, loss = 0.65886452\n",
            "Iteration 121, loss = 0.65833937\n",
            "Iteration 122, loss = 0.65780313\n",
            "Iteration 123, loss = 0.65724575\n",
            "Iteration 124, loss = 0.65667706\n",
            "Iteration 125, loss = 0.65610244\n",
            "Iteration 126, loss = 0.65552280\n",
            "Iteration 127, loss = 0.65493500\n",
            "Iteration 128, loss = 0.65433927\n",
            "Iteration 129, loss = 0.65373594\n",
            "Iteration 130, loss = 0.65312501\n",
            "Iteration 131, loss = 0.65250384\n",
            "Iteration 132, loss = 0.65185917\n",
            "Iteration 133, loss = 0.65120193\n",
            "Iteration 134, loss = 0.65053541\n",
            "Iteration 135, loss = 0.64985981\n",
            "Iteration 136, loss = 0.64917261\n",
            "Iteration 137, loss = 0.64846273\n",
            "Iteration 138, loss = 0.64771412\n",
            "Iteration 139, loss = 0.64694836\n",
            "Iteration 140, loss = 0.64616911\n",
            "Iteration 141, loss = 0.64537635\n",
            "Iteration 142, loss = 0.64457676\n",
            "Iteration 143, loss = 0.64378104\n",
            "Iteration 144, loss = 0.64298577\n",
            "Iteration 145, loss = 0.64217316\n",
            "Iteration 146, loss = 0.64134493\n",
            "Iteration 147, loss = 0.64050703\n",
            "Iteration 148, loss = 0.63965937\n",
            "Iteration 149, loss = 0.63878773\n",
            "Iteration 150, loss = 0.63790659\n",
            "Iteration 151, loss = 0.63702398\n",
            "Iteration 152, loss = 0.63612433\n",
            "Iteration 153, loss = 0.63519278\n",
            "Iteration 154, loss = 0.63424427\n",
            "Iteration 155, loss = 0.63328001\n",
            "Iteration 156, loss = 0.63229177\n",
            "Iteration 157, loss = 0.63129240\n",
            "Iteration 158, loss = 0.63028245\n",
            "Iteration 159, loss = 0.62926145\n",
            "Iteration 160, loss = 0.62821860\n",
            "Iteration 161, loss = 0.62715945\n",
            "Iteration 162, loss = 0.62610282\n",
            "Iteration 163, loss = 0.62504017\n",
            "Iteration 164, loss = 0.62394839\n",
            "Iteration 165, loss = 0.62282080\n",
            "Iteration 166, loss = 0.62165115\n",
            "Iteration 167, loss = 0.62046758\n",
            "Iteration 168, loss = 0.61925874\n",
            "Iteration 169, loss = 0.61801078\n",
            "Iteration 170, loss = 0.61675028\n",
            "Iteration 171, loss = 0.61545744\n",
            "Iteration 172, loss = 0.61414808\n",
            "Iteration 173, loss = 0.61283935\n",
            "Iteration 174, loss = 0.61151877\n",
            "Iteration 175, loss = 0.61014586\n",
            "Iteration 176, loss = 0.60874142\n",
            "Iteration 177, loss = 0.60730328\n",
            "Iteration 178, loss = 0.60584996\n",
            "Iteration 179, loss = 0.60435759\n",
            "Iteration 180, loss = 0.60283933\n",
            "Iteration 181, loss = 0.60130888\n",
            "Iteration 182, loss = 0.59974840\n",
            "Iteration 183, loss = 0.59816027\n",
            "Iteration 184, loss = 0.59655952\n",
            "Iteration 185, loss = 0.59493631\n",
            "Iteration 186, loss = 0.59328055\n",
            "Iteration 187, loss = 0.59157817\n",
            "Iteration 188, loss = 0.58986728\n",
            "Iteration 189, loss = 0.58810702\n",
            "Iteration 190, loss = 0.58622003\n",
            "Iteration 191, loss = 0.58430847\n",
            "Iteration 192, loss = 0.58237023\n",
            "Iteration 193, loss = 0.58036078\n",
            "Iteration 194, loss = 0.57829671\n",
            "Iteration 195, loss = 0.57608271\n",
            "Iteration 196, loss = 0.57380414\n",
            "Iteration 197, loss = 0.57149932\n",
            "Iteration 198, loss = 0.56916974\n",
            "Iteration 199, loss = 0.56682358\n",
            "Iteration 200, loss = 0.56445964\n",
            "Iteration 201, loss = 0.56209138\n",
            "Iteration 202, loss = 0.55971590\n",
            "Iteration 203, loss = 0.55733177\n",
            "Iteration 204, loss = 0.55494428\n",
            "Iteration 205, loss = 0.55255418\n",
            "Iteration 206, loss = 0.55016265\n",
            "Iteration 207, loss = 0.54777593\n",
            "Iteration 208, loss = 0.54540725\n",
            "Iteration 209, loss = 0.54306512\n",
            "Iteration 210, loss = 0.54072920\n",
            "Iteration 211, loss = 0.53839920\n",
            "Iteration 212, loss = 0.53607540\n",
            "Iteration 213, loss = 0.53378420\n",
            "Iteration 214, loss = 0.53150560\n",
            "Iteration 215, loss = 0.52923799\n",
            "Iteration 216, loss = 0.52697974\n",
            "Iteration 217, loss = 0.52471564\n",
            "Iteration 218, loss = 0.52241891\n",
            "Iteration 219, loss = 0.52012968\n",
            "Iteration 220, loss = 0.51784817\n",
            "Iteration 221, loss = 0.51557268\n",
            "Iteration 222, loss = 0.51330703\n",
            "Iteration 223, loss = 0.51105180\n",
            "Iteration 224, loss = 0.50880576\n",
            "Iteration 225, loss = 0.50657041\n",
            "Iteration 226, loss = 0.50434521\n",
            "Iteration 227, loss = 0.50213799\n",
            "Iteration 228, loss = 0.49994178\n",
            "Iteration 229, loss = 0.49775453\n",
            "Iteration 230, loss = 0.49557507\n",
            "Iteration 231, loss = 0.49317434\n",
            "Iteration 232, loss = 0.49075943\n",
            "Iteration 233, loss = 0.48833666\n",
            "Iteration 234, loss = 0.48590796\n",
            "Iteration 235, loss = 0.48346744\n",
            "Iteration 236, loss = 0.48103749\n",
            "Iteration 237, loss = 0.47861410\n",
            "Iteration 238, loss = 0.47617785\n",
            "Iteration 239, loss = 0.47371035\n",
            "Iteration 240, loss = 0.47125043\n",
            "Iteration 241, loss = 0.46880065\n",
            "Iteration 242, loss = 0.46636572\n",
            "Iteration 243, loss = 0.46397728\n",
            "Iteration 244, loss = 0.46160555\n",
            "Iteration 245, loss = 0.45925230\n",
            "Iteration 246, loss = 0.45691782\n",
            "Iteration 247, loss = 0.45460295\n",
            "Iteration 248, loss = 0.45233815\n",
            "Iteration 249, loss = 0.45010655\n",
            "Iteration 250, loss = 0.44789462\n",
            "Iteration 251, loss = 0.44570153\n",
            "Iteration 252, loss = 0.44353016\n",
            "Iteration 253, loss = 0.44138034\n",
            "Iteration 254, loss = 0.43925033\n",
            "Iteration 255, loss = 0.43714001\n",
            "Iteration 256, loss = 0.43505009\n",
            "Iteration 257, loss = 0.43297914\n",
            "Iteration 258, loss = 0.43092724\n",
            "Iteration 259, loss = 0.42889366\n",
            "Iteration 260, loss = 0.42687849\n",
            "Iteration 261, loss = 0.42488094\n",
            "Iteration 262, loss = 0.42289830\n",
            "Iteration 263, loss = 0.42093323\n",
            "Iteration 264, loss = 0.41898569\n",
            "Iteration 265, loss = 0.41705536\n",
            "Iteration 266, loss = 0.41514111\n",
            "Iteration 267, loss = 0.41324500\n",
            "Iteration 268, loss = 0.41136618\n",
            "Iteration 269, loss = 0.40950719\n",
            "Iteration 270, loss = 0.40766504\n",
            "Iteration 271, loss = 0.40583704\n",
            "Iteration 272, loss = 0.40402545\n",
            "Iteration 273, loss = 0.40222897\n",
            "Iteration 274, loss = 0.40045269\n",
            "Iteration 275, loss = 0.39869422\n",
            "Iteration 276, loss = 0.39695265\n",
            "Iteration 277, loss = 0.39522748\n",
            "Iteration 278, loss = 0.39351835\n",
            "Iteration 279, loss = 0.39182495\n",
            "Iteration 280, loss = 0.39014807\n",
            "Iteration 281, loss = 0.38848750\n",
            "Iteration 282, loss = 0.38683988\n",
            "Iteration 283, loss = 0.38513053\n",
            "Iteration 284, loss = 0.38341275\n",
            "Iteration 285, loss = 0.38170161\n",
            "Iteration 286, loss = 0.37999970\n",
            "Iteration 287, loss = 0.37830721\n",
            "Iteration 288, loss = 0.37662465\n",
            "Iteration 289, loss = 0.37495333\n",
            "Iteration 290, loss = 0.37329381\n",
            "Iteration 291, loss = 0.37164503\n",
            "Iteration 292, loss = 0.37000785\n",
            "Iteration 293, loss = 0.36838316\n",
            "Iteration 294, loss = 0.36677071\n",
            "Iteration 295, loss = 0.36517169\n",
            "Iteration 296, loss = 0.36358603\n",
            "Iteration 297, loss = 0.36201420\n",
            "Iteration 298, loss = 0.36045848\n",
            "Iteration 299, loss = 0.35891866\n",
            "Iteration 300, loss = 0.35739651\n",
            "Iteration 301, loss = 0.35589069\n",
            "Iteration 302, loss = 0.35439998\n",
            "Iteration 303, loss = 0.35292512\n",
            "Iteration 304, loss = 0.35146705\n",
            "Iteration 305, loss = 0.35002420\n",
            "Iteration 306, loss = 0.34859597\n",
            "Iteration 307, loss = 0.34717926\n",
            "Iteration 308, loss = 0.34577098\n",
            "Iteration 309, loss = 0.34437726\n",
            "Iteration 310, loss = 0.34299841\n",
            "Iteration 311, loss = 0.34163292\n",
            "Iteration 312, loss = 0.34028278\n",
            "Iteration 313, loss = 0.33894669\n",
            "Iteration 314, loss = 0.33762436\n",
            "Iteration 315, loss = 0.33631475\n",
            "Iteration 316, loss = 0.33501808\n",
            "Iteration 317, loss = 0.33373306\n",
            "Iteration 318, loss = 0.33246098\n",
            "Iteration 319, loss = 0.33120125\n",
            "Iteration 320, loss = 0.32995330\n",
            "Iteration 321, loss = 0.32871620\n",
            "Iteration 322, loss = 0.32749105\n",
            "Iteration 323, loss = 0.32627768\n",
            "Iteration 324, loss = 0.32507610\n",
            "Iteration 325, loss = 0.32388638\n",
            "Iteration 326, loss = 0.32270847\n",
            "Iteration 327, loss = 0.32154342\n",
            "Iteration 328, loss = 0.32039005\n",
            "Iteration 329, loss = 0.31924765\n",
            "Iteration 330, loss = 0.31811662\n",
            "Iteration 331, loss = 0.31700108\n",
            "Iteration 332, loss = 0.31589807\n",
            "Iteration 333, loss = 0.31480595\n",
            "Iteration 334, loss = 0.31372558\n",
            "Iteration 335, loss = 0.31265616\n",
            "Iteration 336, loss = 0.31159811\n",
            "Iteration 337, loss = 0.31055093\n",
            "Iteration 338, loss = 0.30951390\n",
            "Iteration 339, loss = 0.30848936\n",
            "Iteration 340, loss = 0.30748237\n",
            "Iteration 341, loss = 0.30648502\n",
            "Iteration 342, loss = 0.30549762\n",
            "Iteration 343, loss = 0.30452059\n",
            "Iteration 344, loss = 0.30355096\n",
            "Iteration 345, loss = 0.30259011\n",
            "Iteration 346, loss = 0.30163816\n",
            "Iteration 347, loss = 0.30069505\n",
            "Iteration 348, loss = 0.29976072\n",
            "Iteration 349, loss = 0.29883685\n",
            "Iteration 350, loss = 0.29792128\n",
            "Iteration 351, loss = 0.29701720\n",
            "Iteration 352, loss = 0.29612398\n",
            "Iteration 353, loss = 0.29523897\n",
            "Iteration 354, loss = 0.29436119\n",
            "Iteration 355, loss = 0.29349062\n",
            "Iteration 356, loss = 0.29262766\n",
            "Iteration 357, loss = 0.29177239\n",
            "Iteration 358, loss = 0.29092456\n",
            "Iteration 359, loss = 0.29008407\n",
            "Iteration 360, loss = 0.28925120\n",
            "Iteration 361, loss = 0.28842550\n",
            "Iteration 362, loss = 0.28760691\n",
            "Iteration 363, loss = 0.28679499\n",
            "Iteration 364, loss = 0.28598934\n",
            "Iteration 365, loss = 0.28519035\n",
            "Iteration 366, loss = 0.28439815\n",
            "Iteration 367, loss = 0.28361261\n",
            "Iteration 368, loss = 0.28283340\n",
            "Iteration 369, loss = 0.28206036\n",
            "Iteration 370, loss = 0.28129360\n",
            "Iteration 371, loss = 0.28053315\n",
            "Iteration 372, loss = 0.27977919\n",
            "Iteration 373, loss = 0.27903117\n",
            "Iteration 374, loss = 0.27828790\n",
            "Iteration 375, loss = 0.27754888\n",
            "Iteration 376, loss = 0.27681489\n",
            "Iteration 377, loss = 0.27608608\n",
            "Iteration 378, loss = 0.27536278\n",
            "Iteration 379, loss = 0.27464521\n",
            "Iteration 380, loss = 0.27393473\n",
            "Iteration 381, loss = 0.27323024\n",
            "Iteration 382, loss = 0.27253119\n",
            "Iteration 383, loss = 0.27183735\n",
            "Iteration 384, loss = 0.27114890\n",
            "Iteration 385, loss = 0.27046635\n",
            "Iteration 386, loss = 0.26978879\n",
            "Iteration 387, loss = 0.26911615\n",
            "Iteration 388, loss = 0.26844837\n",
            "Iteration 389, loss = 0.26778526\n",
            "Iteration 390, loss = 0.26712679\n",
            "Iteration 391, loss = 0.26647290\n",
            "Iteration 392, loss = 0.26582387\n",
            "Iteration 393, loss = 0.26517976\n",
            "Iteration 394, loss = 0.26453996\n",
            "Iteration 395, loss = 0.26390820\n",
            "Iteration 396, loss = 0.26328403\n",
            "Iteration 397, loss = 0.26266470\n",
            "Iteration 398, loss = 0.26204984\n",
            "Iteration 399, loss = 0.26143929\n",
            "Iteration 400, loss = 0.26083297\n",
            "Iteration 401, loss = 0.26023080\n",
            "Iteration 402, loss = 0.25963342\n",
            "Iteration 403, loss = 0.25903995\n",
            "Iteration 404, loss = 0.25845039\n",
            "Iteration 405, loss = 0.25786487\n",
            "Iteration 406, loss = 0.25728193\n",
            "Iteration 407, loss = 0.25670245\n",
            "Iteration 408, loss = 0.25612659\n",
            "Iteration 409, loss = 0.25555433\n",
            "Iteration 410, loss = 0.25498547\n",
            "Iteration 411, loss = 0.25442034\n",
            "Iteration 412, loss = 0.25385938\n",
            "Iteration 413, loss = 0.25330214\n",
            "Iteration 414, loss = 0.25274811\n",
            "Iteration 415, loss = 0.25219773\n",
            "Iteration 416, loss = 0.25165036\n",
            "Iteration 417, loss = 0.25110580\n",
            "Iteration 418, loss = 0.25056445\n",
            "Iteration 419, loss = 0.25002652\n",
            "Iteration 420, loss = 0.24949241\n",
            "Iteration 421, loss = 0.24896143\n",
            "Iteration 422, loss = 0.24843366\n",
            "Iteration 423, loss = 0.24790897\n",
            "Iteration 424, loss = 0.24738832\n",
            "Iteration 425, loss = 0.24687289\n",
            "Iteration 426, loss = 0.24636094\n",
            "Iteration 427, loss = 0.24585193\n",
            "Iteration 428, loss = 0.24534585\n",
            "Iteration 429, loss = 0.24484286\n",
            "Iteration 430, loss = 0.24434271\n",
            "Iteration 431, loss = 0.24384590\n",
            "Iteration 432, loss = 0.24335252\n",
            "Iteration 433, loss = 0.24286278\n",
            "Iteration 434, loss = 0.24237595\n",
            "Iteration 435, loss = 0.24189238\n",
            "Iteration 436, loss = 0.24141164\n",
            "Iteration 437, loss = 0.24093372\n",
            "Iteration 438, loss = 0.24045821\n",
            "Iteration 439, loss = 0.23998575\n",
            "Iteration 440, loss = 0.23951453\n",
            "Iteration 441, loss = 0.23904629\n",
            "Iteration 442, loss = 0.23858078\n",
            "Iteration 443, loss = 0.23811988\n",
            "Iteration 444, loss = 0.23766365\n",
            "Iteration 445, loss = 0.23721011\n",
            "Iteration 446, loss = 0.23675921\n",
            "Iteration 447, loss = 0.23631085\n",
            "Iteration 448, loss = 0.23586509\n",
            "Iteration 449, loss = 0.23542191\n",
            "Iteration 450, loss = 0.23498129\n",
            "Iteration 451, loss = 0.23454324\n",
            "Iteration 452, loss = 0.23410776\n",
            "Iteration 453, loss = 0.23367480\n",
            "Iteration 454, loss = 0.23324436\n",
            "Iteration 455, loss = 0.23281633\n",
            "Iteration 456, loss = 0.23239073\n",
            "Iteration 457, loss = 0.23196752\n",
            "Iteration 458, loss = 0.23154720\n",
            "Iteration 459, loss = 0.23112961\n",
            "Iteration 460, loss = 0.23071440\n",
            "Iteration 461, loss = 0.23030154\n",
            "Iteration 462, loss = 0.22989128\n",
            "Iteration 463, loss = 0.22948335\n",
            "Iteration 464, loss = 0.22907800\n",
            "Iteration 465, loss = 0.22867476\n",
            "Iteration 466, loss = 0.22827388\n",
            "Iteration 467, loss = 0.22787523\n",
            "Iteration 468, loss = 0.22747931\n",
            "Iteration 469, loss = 0.22708561\n",
            "Iteration 470, loss = 0.22669399\n",
            "Iteration 471, loss = 0.22630469\n",
            "Iteration 472, loss = 0.22591729\n",
            "Iteration 473, loss = 0.22553418\n",
            "Iteration 474, loss = 0.22515438\n",
            "Iteration 475, loss = 0.22477696\n",
            "Iteration 476, loss = 0.22440137\n",
            "Iteration 477, loss = 0.22402799\n",
            "Iteration 478, loss = 0.22365677\n",
            "Iteration 479, loss = 0.22328799\n",
            "Iteration 480, loss = 0.22292077\n",
            "Iteration 481, loss = 0.22255561\n",
            "Iteration 482, loss = 0.22219238\n",
            "Iteration 483, loss = 0.22183152\n",
            "Iteration 484, loss = 0.22147243\n",
            "Iteration 485, loss = 0.22111531\n",
            "Iteration 486, loss = 0.22076030\n",
            "Iteration 487, loss = 0.22040750\n",
            "Iteration 488, loss = 0.22005680\n",
            "Iteration 489, loss = 0.21970821\n",
            "Iteration 490, loss = 0.21936140\n",
            "Iteration 491, loss = 0.21901684\n",
            "Iteration 492, loss = 0.21867403\n",
            "Iteration 493, loss = 0.21833304\n",
            "Iteration 494, loss = 0.21799395\n",
            "Iteration 495, loss = 0.21765663\n",
            "Iteration 496, loss = 0.21732077\n",
            "Iteration 497, loss = 0.21698656\n",
            "Iteration 498, loss = 0.21665308\n",
            "Iteration 499, loss = 0.21632169\n",
            "Iteration 500, loss = 0.21599191\n",
            "Iteration 501, loss = 0.21566276\n",
            "Iteration 502, loss = 0.21533190\n",
            "Iteration 503, loss = 0.21500219\n",
            "Iteration 504, loss = 0.21467320\n",
            "Iteration 505, loss = 0.21434538\n",
            "Iteration 506, loss = 0.21401873\n",
            "Iteration 507, loss = 0.21369266\n",
            "Iteration 508, loss = 0.21336846\n",
            "Iteration 509, loss = 0.21304504\n",
            "Iteration 510, loss = 0.21272383\n",
            "Iteration 511, loss = 0.21240331\n",
            "Iteration 512, loss = 0.21208448\n",
            "Iteration 513, loss = 0.21176685\n",
            "Iteration 514, loss = 0.21145062\n",
            "Iteration 515, loss = 0.21113642\n",
            "Iteration 516, loss = 0.21082307\n",
            "Iteration 517, loss = 0.21051109\n",
            "Iteration 518, loss = 0.21020069\n",
            "Iteration 519, loss = 0.20989178\n",
            "Iteration 520, loss = 0.20958403\n",
            "Iteration 521, loss = 0.20927802\n",
            "Iteration 522, loss = 0.20897326\n",
            "Iteration 523, loss = 0.20866956\n",
            "Iteration 524, loss = 0.20836733\n",
            "Iteration 525, loss = 0.20806767\n",
            "Iteration 526, loss = 0.20776905\n",
            "Iteration 527, loss = 0.20747269\n",
            "Iteration 528, loss = 0.20717867\n",
            "Iteration 529, loss = 0.20688581\n",
            "Iteration 530, loss = 0.20659484\n",
            "Iteration 531, loss = 0.20630520\n",
            "Iteration 532, loss = 0.20601681\n",
            "Iteration 533, loss = 0.20572964\n",
            "Iteration 534, loss = 0.20544397\n",
            "Iteration 535, loss = 0.20515911\n",
            "Iteration 536, loss = 0.20487623\n",
            "Iteration 537, loss = 0.20459515\n",
            "Iteration 538, loss = 0.20431524\n",
            "Iteration 539, loss = 0.20403634\n",
            "Iteration 540, loss = 0.20375897\n",
            "Iteration 541, loss = 0.20348251\n",
            "Iteration 542, loss = 0.20320703\n",
            "Iteration 543, loss = 0.20293274\n",
            "Iteration 544, loss = 0.20266061\n",
            "Iteration 545, loss = 0.20238944\n",
            "Iteration 546, loss = 0.20211966\n",
            "Iteration 547, loss = 0.20185172\n",
            "Iteration 548, loss = 0.20158503\n",
            "Iteration 549, loss = 0.20131983\n",
            "Iteration 550, loss = 0.20105574\n",
            "Iteration 551, loss = 0.20079241\n",
            "Iteration 552, loss = 0.20052986\n",
            "Iteration 553, loss = 0.20026841\n",
            "Iteration 554, loss = 0.20000744\n",
            "Iteration 555, loss = 0.19974796\n",
            "Iteration 556, loss = 0.19948984\n",
            "Iteration 557, loss = 0.19923256\n",
            "Iteration 558, loss = 0.19897602\n",
            "Iteration 559, loss = 0.19872092\n",
            "Iteration 560, loss = 0.19846675\n",
            "Iteration 561, loss = 0.19821343\n",
            "Iteration 562, loss = 0.19796134\n",
            "Iteration 563, loss = 0.19770992\n",
            "Iteration 564, loss = 0.19745923\n",
            "Iteration 565, loss = 0.19720982\n",
            "Iteration 566, loss = 0.19696174\n",
            "Iteration 567, loss = 0.19671438\n",
            "Iteration 568, loss = 0.19646815\n",
            "Iteration 569, loss = 0.19622284\n",
            "Iteration 570, loss = 0.19597838\n",
            "Iteration 571, loss = 0.19573480\n",
            "Iteration 572, loss = 0.19549205\n",
            "Iteration 573, loss = 0.19525053\n",
            "Iteration 574, loss = 0.19501035\n",
            "Iteration 575, loss = 0.19477100\n",
            "Iteration 576, loss = 0.19453240\n",
            "Iteration 577, loss = 0.19429475\n",
            "Iteration 578, loss = 0.19405802\n",
            "Iteration 579, loss = 0.19382180\n",
            "Iteration 580, loss = 0.19358658\n",
            "Iteration 581, loss = 0.19335270\n",
            "Iteration 582, loss = 0.19311912\n",
            "Iteration 583, loss = 0.19288715\n",
            "Iteration 584, loss = 0.19265569\n",
            "Iteration 585, loss = 0.19242571\n",
            "Iteration 586, loss = 0.19219649\n",
            "Iteration 587, loss = 0.19196846\n",
            "Iteration 588, loss = 0.19174078\n",
            "Iteration 589, loss = 0.19151370\n",
            "Iteration 590, loss = 0.19128826\n",
            "Iteration 591, loss = 0.19106283\n",
            "Iteration 592, loss = 0.19083894\n",
            "Iteration 593, loss = 0.19061575\n",
            "Iteration 594, loss = 0.19039303\n",
            "Iteration 595, loss = 0.19017178\n",
            "Iteration 596, loss = 0.18995199\n",
            "Iteration 597, loss = 0.18973319\n",
            "Iteration 598, loss = 0.18951540\n",
            "Iteration 599, loss = 0.18929866\n",
            "Iteration 600, loss = 0.18908245\n",
            "Iteration 601, loss = 0.18886756\n",
            "Iteration 602, loss = 0.18865351\n",
            "Iteration 603, loss = 0.18844022\n",
            "Iteration 604, loss = 0.18822798\n",
            "Iteration 605, loss = 0.18801641\n",
            "Iteration 606, loss = 0.18780555\n",
            "Iteration 607, loss = 0.18759548\n",
            "Iteration 608, loss = 0.18738579\n",
            "Iteration 609, loss = 0.18717662\n",
            "Iteration 610, loss = 0.18696833\n",
            "Iteration 611, loss = 0.18676054\n",
            "Iteration 612, loss = 0.18655336\n",
            "Iteration 613, loss = 0.18634798\n",
            "Iteration 614, loss = 0.18614309\n",
            "Iteration 615, loss = 0.18593889\n",
            "Iteration 616, loss = 0.18573613\n",
            "Iteration 617, loss = 0.18553358\n",
            "Iteration 618, loss = 0.18533192\n",
            "Iteration 619, loss = 0.18513060\n",
            "Iteration 620, loss = 0.18493027\n",
            "Iteration 621, loss = 0.18473097\n",
            "Iteration 622, loss = 0.18453237\n",
            "Iteration 623, loss = 0.18433493\n",
            "Iteration 624, loss = 0.18413787\n",
            "Iteration 625, loss = 0.18394121\n",
            "Iteration 626, loss = 0.18374564\n",
            "Iteration 627, loss = 0.18355026\n",
            "Iteration 628, loss = 0.18335537\n",
            "Iteration 629, loss = 0.18316132\n",
            "Iteration 630, loss = 0.18296752\n",
            "Iteration 631, loss = 0.18277435\n",
            "Iteration 632, loss = 0.18258182\n",
            "Iteration 633, loss = 0.18238979\n",
            "Iteration 634, loss = 0.18219849\n",
            "Iteration 635, loss = 0.18200706\n",
            "Iteration 636, loss = 0.18181719\n",
            "Iteration 637, loss = 0.18162735\n",
            "Iteration 638, loss = 0.18143799\n",
            "Iteration 639, loss = 0.18124956\n",
            "Iteration 640, loss = 0.18106100\n",
            "Iteration 641, loss = 0.18087289\n",
            "Iteration 642, loss = 0.18068553\n",
            "Iteration 643, loss = 0.18049856\n",
            "Iteration 644, loss = 0.18031225\n",
            "Iteration 645, loss = 0.18012660\n",
            "Iteration 646, loss = 0.17994113\n",
            "Iteration 647, loss = 0.17975658\n",
            "Iteration 648, loss = 0.17957181\n",
            "Iteration 649, loss = 0.17938861\n",
            "Iteration 650, loss = 0.17920514\n",
            "Iteration 651, loss = 0.17902209\n",
            "Iteration 652, loss = 0.17883971\n",
            "Iteration 653, loss = 0.17865796\n",
            "Iteration 654, loss = 0.17847666\n",
            "Iteration 655, loss = 0.17829630\n",
            "Iteration 656, loss = 0.17811564\n",
            "Iteration 657, loss = 0.17793616\n",
            "Iteration 658, loss = 0.17775657\n",
            "Iteration 659, loss = 0.17757718\n",
            "Iteration 660, loss = 0.17739857\n",
            "Iteration 661, loss = 0.17722019\n",
            "Iteration 662, loss = 0.17704248\n",
            "Iteration 663, loss = 0.17686530\n",
            "Iteration 664, loss = 0.17668817\n",
            "Iteration 665, loss = 0.17651209\n",
            "Iteration 666, loss = 0.17633603\n",
            "Iteration 667, loss = 0.17616094\n",
            "Iteration 668, loss = 0.17598569\n",
            "Iteration 669, loss = 0.17581102\n",
            "Iteration 670, loss = 0.17563642\n",
            "Iteration 671, loss = 0.17546382\n",
            "Iteration 672, loss = 0.17529096\n",
            "Iteration 673, loss = 0.17511840\n",
            "Iteration 674, loss = 0.17494627\n",
            "Iteration 675, loss = 0.17477382\n",
            "Iteration 676, loss = 0.17460199\n",
            "Iteration 677, loss = 0.17443014\n",
            "Iteration 678, loss = 0.17425838\n",
            "Iteration 679, loss = 0.17408742\n",
            "Iteration 680, loss = 0.17391622\n",
            "Iteration 681, loss = 0.17374561\n",
            "Iteration 682, loss = 0.17357542\n",
            "Iteration 683, loss = 0.17340505\n",
            "Iteration 684, loss = 0.17323550\n",
            "Iteration 685, loss = 0.17306606\n",
            "Iteration 686, loss = 0.17289707\n",
            "Iteration 687, loss = 0.17272877\n",
            "Iteration 688, loss = 0.17256076\n",
            "Iteration 689, loss = 0.17239286\n",
            "Iteration 690, loss = 0.17222567\n",
            "Iteration 691, loss = 0.17205843\n",
            "Iteration 692, loss = 0.17189156\n",
            "Iteration 693, loss = 0.17172538\n",
            "Iteration 694, loss = 0.17155918\n",
            "Iteration 695, loss = 0.17139369\n",
            "Iteration 696, loss = 0.17122869\n",
            "Iteration 697, loss = 0.17106360\n",
            "Iteration 698, loss = 0.17089915\n",
            "Iteration 699, loss = 0.17073470\n",
            "Iteration 700, loss = 0.17057050\n",
            "Iteration 701, loss = 0.17040744\n",
            "Iteration 702, loss = 0.17024371\n",
            "Iteration 703, loss = 0.17008144\n",
            "Iteration 704, loss = 0.16991893\n",
            "Iteration 705, loss = 0.16975678\n",
            "Iteration 706, loss = 0.16959539\n",
            "Iteration 707, loss = 0.16943395\n",
            "Iteration 708, loss = 0.16927273\n",
            "Iteration 709, loss = 0.16911212\n",
            "Iteration 710, loss = 0.16895108\n",
            "Iteration 711, loss = 0.16879057\n",
            "Iteration 712, loss = 0.16863044\n",
            "Iteration 713, loss = 0.16847139\n",
            "Iteration 714, loss = 0.16831199\n",
            "Iteration 715, loss = 0.16815360\n",
            "Iteration 716, loss = 0.16799519\n",
            "Iteration 717, loss = 0.16783709\n",
            "Iteration 718, loss = 0.16767990\n",
            "Iteration 719, loss = 0.16752261\n",
            "Iteration 720, loss = 0.16736587\n",
            "Iteration 721, loss = 0.16720910\n",
            "Iteration 722, loss = 0.16705418\n",
            "Iteration 723, loss = 0.16689881\n",
            "Iteration 724, loss = 0.16674367\n",
            "Iteration 725, loss = 0.16658902\n",
            "Iteration 726, loss = 0.16643478\n",
            "Iteration 727, loss = 0.16628057\n",
            "Iteration 728, loss = 0.16612673\n",
            "Iteration 729, loss = 0.16597331\n",
            "Iteration 730, loss = 0.16581986\n",
            "Iteration 731, loss = 0.16566697\n",
            "Iteration 732, loss = 0.16551442\n",
            "Iteration 733, loss = 0.16536206\n",
            "Iteration 734, loss = 0.16521040\n",
            "Iteration 735, loss = 0.16505909\n",
            "Iteration 736, loss = 0.16490818\n",
            "Iteration 737, loss = 0.16475750\n",
            "Iteration 738, loss = 0.16460697\n",
            "Iteration 739, loss = 0.16445662\n",
            "Iteration 740, loss = 0.16430692\n",
            "Iteration 741, loss = 0.16415694\n",
            "Iteration 742, loss = 0.16400731\n",
            "Iteration 743, loss = 0.16385794\n",
            "Iteration 744, loss = 0.16370825\n",
            "Iteration 745, loss = 0.16355914\n",
            "Iteration 746, loss = 0.16341027\n",
            "Iteration 747, loss = 0.16326148\n",
            "Iteration 748, loss = 0.16311616\n",
            "Iteration 749, loss = 0.16297055\n",
            "Iteration 750, loss = 0.16282415\n",
            "Iteration 751, loss = 0.16267779\n",
            "Iteration 752, loss = 0.16253105\n",
            "Iteration 753, loss = 0.16238466\n",
            "Iteration 754, loss = 0.16223766\n",
            "Iteration 755, loss = 0.16209096\n",
            "Iteration 756, loss = 0.16194606\n",
            "Iteration 757, loss = 0.16180105\n",
            "Iteration 758, loss = 0.16165618\n",
            "Iteration 759, loss = 0.16151167\n",
            "Iteration 760, loss = 0.16136742\n",
            "Iteration 761, loss = 0.16122322\n",
            "Iteration 762, loss = 0.16107957\n",
            "Iteration 763, loss = 0.16093591\n",
            "Iteration 764, loss = 0.16079212\n",
            "Iteration 765, loss = 0.16064912\n",
            "Iteration 766, loss = 0.16050622\n",
            "Iteration 767, loss = 0.16036309\n",
            "Iteration 768, loss = 0.16022088\n",
            "Iteration 769, loss = 0.16007850\n",
            "Iteration 770, loss = 0.15993623\n",
            "Iteration 771, loss = 0.15979454\n",
            "Iteration 772, loss = 0.15965305\n",
            "Iteration 773, loss = 0.15951191\n",
            "Iteration 774, loss = 0.15937104\n",
            "Iteration 775, loss = 0.15923028\n",
            "Iteration 776, loss = 0.15909080\n",
            "Iteration 777, loss = 0.15895102\n",
            "Iteration 778, loss = 0.15881148\n",
            "Iteration 779, loss = 0.15867199\n",
            "Iteration 780, loss = 0.15853262\n",
            "Iteration 781, loss = 0.15839302\n",
            "Iteration 782, loss = 0.15825433\n",
            "Iteration 783, loss = 0.15811479\n",
            "Iteration 784, loss = 0.15797589\n",
            "Iteration 785, loss = 0.15783767\n",
            "Iteration 786, loss = 0.15769976\n",
            "Iteration 787, loss = 0.15756154\n",
            "Iteration 788, loss = 0.15742467\n",
            "Iteration 789, loss = 0.15728731\n",
            "Iteration 790, loss = 0.15714969\n",
            "Iteration 791, loss = 0.15701343\n",
            "Iteration 792, loss = 0.15687612\n",
            "Iteration 793, loss = 0.15674021\n",
            "Iteration 794, loss = 0.15660452\n",
            "Iteration 795, loss = 0.15646874\n",
            "Iteration 796, loss = 0.15633329\n",
            "Iteration 797, loss = 0.15619858\n",
            "Iteration 798, loss = 0.15606311\n",
            "Iteration 799, loss = 0.15592896\n",
            "Iteration 800, loss = 0.15579429\n",
            "Iteration 1, loss = 0.70242127\n",
            "Iteration 2, loss = 0.70183836\n",
            "Iteration 3, loss = 0.70101282\n",
            "Iteration 4, loss = 0.70002594\n",
            "Iteration 5, loss = 0.69896464\n",
            "Iteration 6, loss = 0.69781255\n",
            "Iteration 7, loss = 0.69669124\n",
            "Iteration 8, loss = 0.69575509\n",
            "Iteration 9, loss = 0.69491673\n",
            "Iteration 10, loss = 0.69419687\n",
            "Iteration 11, loss = 0.69349484\n",
            "Iteration 12, loss = 0.69287087\n",
            "Iteration 13, loss = 0.69238398\n",
            "Iteration 14, loss = 0.69194059\n",
            "Iteration 15, loss = 0.69153446\n",
            "Iteration 16, loss = 0.69115444\n",
            "Iteration 17, loss = 0.69082892\n",
            "Iteration 18, loss = 0.69061360\n",
            "Iteration 19, loss = 0.69045212\n",
            "Iteration 20, loss = 0.69034783\n",
            "Iteration 21, loss = 0.69027004\n",
            "Iteration 22, loss = 0.69019554\n",
            "Iteration 23, loss = 0.69010207\n",
            "Iteration 24, loss = 0.69000762\n",
            "Iteration 25, loss = 0.68991417\n",
            "Iteration 26, loss = 0.68982159\n",
            "Iteration 27, loss = 0.68973902\n",
            "Iteration 28, loss = 0.68966487\n",
            "Iteration 29, loss = 0.68958952\n",
            "Iteration 30, loss = 0.68951295\n",
            "Iteration 31, loss = 0.68943520\n",
            "Iteration 32, loss = 0.68935633\n",
            "Iteration 33, loss = 0.68927638\n",
            "Iteration 34, loss = 0.68919537\n",
            "Iteration 35, loss = 0.68911334\n",
            "Iteration 36, loss = 0.68903028\n",
            "Iteration 37, loss = 0.68894622\n",
            "Iteration 38, loss = 0.68886118\n",
            "Iteration 39, loss = 0.68877520\n",
            "Iteration 40, loss = 0.68868829\n",
            "Iteration 41, loss = 0.68860046\n",
            "Iteration 42, loss = 0.68851171\n",
            "Iteration 43, loss = 0.68842212\n",
            "Iteration 44, loss = 0.68833166\n",
            "Iteration 45, loss = 0.68824031\n",
            "Iteration 46, loss = 0.68814810\n",
            "Iteration 47, loss = 0.68805503\n",
            "Iteration 48, loss = 0.68796120\n",
            "Iteration 49, loss = 0.68786654\n",
            "Iteration 50, loss = 0.68777101\n",
            "Iteration 51, loss = 0.68767459\n",
            "Iteration 52, loss = 0.68757731\n",
            "Iteration 53, loss = 0.68747913\n",
            "Iteration 54, loss = 0.68738005\n",
            "Iteration 55, loss = 0.68728008\n",
            "Iteration 56, loss = 0.68717919\n",
            "Iteration 57, loss = 0.68707736\n",
            "Iteration 58, loss = 0.68697461\n",
            "Iteration 59, loss = 0.68687091\n",
            "Iteration 60, loss = 0.68676625\n",
            "Iteration 61, loss = 0.68666065\n",
            "Iteration 62, loss = 0.68655409\n",
            "Iteration 63, loss = 0.68645016\n",
            "Iteration 64, loss = 0.68634666\n",
            "Iteration 65, loss = 0.68624223\n",
            "Iteration 66, loss = 0.68613690\n",
            "Iteration 67, loss = 0.68603064\n",
            "Iteration 68, loss = 0.68592340\n",
            "Iteration 69, loss = 0.68581523\n",
            "Iteration 70, loss = 0.68570611\n",
            "Iteration 71, loss = 0.68559594\n",
            "Iteration 72, loss = 0.68548470\n",
            "Iteration 73, loss = 0.68537240\n",
            "Iteration 74, loss = 0.68525902\n",
            "Iteration 75, loss = 0.68514450\n",
            "Iteration 76, loss = 0.68502883\n",
            "Iteration 77, loss = 0.68491182\n",
            "Iteration 78, loss = 0.68479359\n",
            "Iteration 79, loss = 0.68467418\n",
            "Iteration 80, loss = 0.68455360\n",
            "Iteration 81, loss = 0.68443181\n",
            "Iteration 82, loss = 0.68430881\n",
            "Iteration 83, loss = 0.68418458\n",
            "Iteration 84, loss = 0.68405912\n",
            "Iteration 85, loss = 0.68393243\n",
            "Iteration 86, loss = 0.68380433\n",
            "Iteration 87, loss = 0.68367431\n",
            "Iteration 88, loss = 0.68354287\n",
            "Iteration 89, loss = 0.68341039\n",
            "Iteration 90, loss = 0.68327944\n",
            "Iteration 91, loss = 0.68314906\n",
            "Iteration 92, loss = 0.68301707\n",
            "Iteration 93, loss = 0.68288349\n",
            "Iteration 94, loss = 0.68274834\n",
            "Iteration 95, loss = 0.68261160\n",
            "Iteration 96, loss = 0.68247581\n",
            "Iteration 97, loss = 0.68233677\n",
            "Iteration 98, loss = 0.68219500\n",
            "Iteration 99, loss = 0.68205062\n",
            "Iteration 100, loss = 0.68190372\n",
            "Iteration 101, loss = 0.68175726\n",
            "Iteration 102, loss = 0.68160918\n",
            "Iteration 103, loss = 0.68145947\n",
            "Iteration 104, loss = 0.68130420\n",
            "Iteration 105, loss = 0.68114329\n",
            "Iteration 106, loss = 0.68098547\n",
            "Iteration 107, loss = 0.68082522\n",
            "Iteration 108, loss = 0.68066248\n",
            "Iteration 109, loss = 0.68049771\n",
            "Iteration 110, loss = 0.68033131\n",
            "Iteration 111, loss = 0.68016277\n",
            "Iteration 112, loss = 0.67999285\n",
            "Iteration 113, loss = 0.67982135\n",
            "Iteration 114, loss = 0.67964801\n",
            "Iteration 115, loss = 0.67947267\n",
            "Iteration 116, loss = 0.67929552\n",
            "Iteration 117, loss = 0.67911608\n",
            "Iteration 118, loss = 0.67893452\n",
            "Iteration 119, loss = 0.67874748\n",
            "Iteration 120, loss = 0.67855736\n",
            "Iteration 121, loss = 0.67836539\n",
            "Iteration 122, loss = 0.67817092\n",
            "Iteration 123, loss = 0.67797402\n",
            "Iteration 124, loss = 0.67777301\n",
            "Iteration 125, loss = 0.67756701\n",
            "Iteration 126, loss = 0.67735818\n",
            "Iteration 127, loss = 0.67714662\n",
            "Iteration 128, loss = 0.67693233\n",
            "Iteration 129, loss = 0.67671554\n",
            "Iteration 130, loss = 0.67649166\n",
            "Iteration 131, loss = 0.67626081\n",
            "Iteration 132, loss = 0.67602635\n",
            "Iteration 133, loss = 0.67578832\n",
            "Iteration 134, loss = 0.67553164\n",
            "Iteration 135, loss = 0.67525960\n",
            "Iteration 136, loss = 0.67497275\n",
            "Iteration 137, loss = 0.67467844\n",
            "Iteration 138, loss = 0.67437712\n",
            "Iteration 139, loss = 0.67406932\n",
            "Iteration 140, loss = 0.67375556\n",
            "Iteration 141, loss = 0.67343668\n",
            "Iteration 142, loss = 0.67311291\n",
            "Iteration 143, loss = 0.67278851\n",
            "Iteration 144, loss = 0.67246240\n",
            "Iteration 145, loss = 0.67213594\n",
            "Iteration 146, loss = 0.67180031\n",
            "Iteration 147, loss = 0.67143833\n",
            "Iteration 148, loss = 0.67106986\n",
            "Iteration 149, loss = 0.67070343\n",
            "Iteration 150, loss = 0.67034089\n",
            "Iteration 151, loss = 0.66997475\n",
            "Iteration 152, loss = 0.66961819\n",
            "Iteration 153, loss = 0.66926018\n",
            "Iteration 154, loss = 0.66889943\n",
            "Iteration 155, loss = 0.66852889\n",
            "Iteration 156, loss = 0.66815334\n",
            "Iteration 157, loss = 0.66777392\n",
            "Iteration 158, loss = 0.66739043\n",
            "Iteration 159, loss = 0.66700314\n",
            "Iteration 160, loss = 0.66661190\n",
            "Iteration 161, loss = 0.66621731\n",
            "Iteration 162, loss = 0.66581937\n",
            "Iteration 163, loss = 0.66541800\n",
            "Iteration 164, loss = 0.66500911\n",
            "Iteration 165, loss = 0.66457537\n",
            "Iteration 166, loss = 0.66413625\n",
            "Iteration 167, loss = 0.66369216\n",
            "Iteration 168, loss = 0.66324376\n",
            "Iteration 169, loss = 0.66279333\n",
            "Iteration 170, loss = 0.66233855\n",
            "Iteration 171, loss = 0.66187916\n",
            "Iteration 172, loss = 0.66141551\n",
            "Iteration 173, loss = 0.66094793\n",
            "Iteration 174, loss = 0.66047665\n",
            "Iteration 175, loss = 0.66000175\n",
            "Iteration 176, loss = 0.65952318\n",
            "Iteration 177, loss = 0.65903719\n",
            "Iteration 178, loss = 0.65854303\n",
            "Iteration 179, loss = 0.65804454\n",
            "Iteration 180, loss = 0.65754217\n",
            "Iteration 181, loss = 0.65703588\n",
            "Iteration 182, loss = 0.65652583\n",
            "Iteration 183, loss = 0.65601290\n",
            "Iteration 184, loss = 0.65549638\n",
            "Iteration 185, loss = 0.65497623\n",
            "Iteration 186, loss = 0.65445743\n",
            "Iteration 187, loss = 0.65393505\n",
            "Iteration 188, loss = 0.65340131\n",
            "Iteration 189, loss = 0.65283216\n",
            "Iteration 190, loss = 0.65222510\n",
            "Iteration 191, loss = 0.65160834\n",
            "Iteration 192, loss = 0.65097659\n",
            "Iteration 193, loss = 0.65032880\n",
            "Iteration 194, loss = 0.64967174\n",
            "Iteration 195, loss = 0.64900566\n",
            "Iteration 196, loss = 0.64832467\n",
            "Iteration 197, loss = 0.64763876\n",
            "Iteration 198, loss = 0.64694421\n",
            "Iteration 199, loss = 0.64624181\n",
            "Iteration 200, loss = 0.64552776\n",
            "Iteration 201, loss = 0.64480532\n",
            "Iteration 202, loss = 0.64408434\n",
            "Iteration 203, loss = 0.64335742\n",
            "Iteration 204, loss = 0.64262475\n",
            "Iteration 205, loss = 0.64188510\n",
            "Iteration 206, loss = 0.64110084\n",
            "Iteration 207, loss = 0.64028880\n",
            "Iteration 208, loss = 0.63945313\n",
            "Iteration 209, loss = 0.63860121\n",
            "Iteration 210, loss = 0.63774030\n",
            "Iteration 211, loss = 0.63687119\n",
            "Iteration 212, loss = 0.63596712\n",
            "Iteration 213, loss = 0.63503770\n",
            "Iteration 214, loss = 0.63409570\n",
            "Iteration 215, loss = 0.63310673\n",
            "Iteration 216, loss = 0.63209616\n",
            "Iteration 217, loss = 0.63105468\n",
            "Iteration 218, loss = 0.63001277\n",
            "Iteration 219, loss = 0.62896732\n",
            "Iteration 220, loss = 0.62784877\n",
            "Iteration 221, loss = 0.62671192\n",
            "Iteration 222, loss = 0.62552929\n",
            "Iteration 223, loss = 0.62433203\n",
            "Iteration 224, loss = 0.62313171\n",
            "Iteration 225, loss = 0.62189287\n",
            "Iteration 226, loss = 0.62059002\n",
            "Iteration 227, loss = 0.61927184\n",
            "Iteration 228, loss = 0.61793709\n",
            "Iteration 229, loss = 0.61653910\n",
            "Iteration 230, loss = 0.61510435\n",
            "Iteration 231, loss = 0.61363305\n",
            "Iteration 232, loss = 0.61211280\n",
            "Iteration 233, loss = 0.61056890\n",
            "Iteration 234, loss = 0.60901197\n",
            "Iteration 235, loss = 0.60743373\n",
            "Iteration 236, loss = 0.60577287\n",
            "Iteration 237, loss = 0.60401760\n",
            "Iteration 238, loss = 0.60224297\n",
            "Iteration 239, loss = 0.60045232\n",
            "Iteration 240, loss = 0.59865078\n",
            "Iteration 241, loss = 0.59680694\n",
            "Iteration 242, loss = 0.59494258\n",
            "Iteration 243, loss = 0.59310506\n",
            "Iteration 244, loss = 0.59127961\n",
            "Iteration 245, loss = 0.58942947\n",
            "Iteration 246, loss = 0.58758093\n",
            "Iteration 247, loss = 0.58575127\n",
            "Iteration 248, loss = 0.58392609\n",
            "Iteration 249, loss = 0.58210551\n",
            "Iteration 250, loss = 0.58028997\n",
            "Iteration 251, loss = 0.57846006\n",
            "Iteration 252, loss = 0.57643975\n",
            "Iteration 253, loss = 0.57441468\n",
            "Iteration 254, loss = 0.57238091\n",
            "Iteration 255, loss = 0.57033897\n",
            "Iteration 256, loss = 0.56829624\n",
            "Iteration 257, loss = 0.56626115\n",
            "Iteration 258, loss = 0.56427186\n",
            "Iteration 259, loss = 0.56229710\n",
            "Iteration 260, loss = 0.56032960\n",
            "Iteration 261, loss = 0.55840144\n",
            "Iteration 262, loss = 0.55648230\n",
            "Iteration 263, loss = 0.55457148\n",
            "Iteration 264, loss = 0.55267089\n",
            "Iteration 265, loss = 0.55076555\n",
            "Iteration 266, loss = 0.54880347\n",
            "Iteration 267, loss = 0.54684349\n",
            "Iteration 268, loss = 0.54488839\n",
            "Iteration 269, loss = 0.54294103\n",
            "Iteration 270, loss = 0.54100273\n",
            "Iteration 271, loss = 0.53907382\n",
            "Iteration 272, loss = 0.53715495\n",
            "Iteration 273, loss = 0.53524581\n",
            "Iteration 274, loss = 0.53334658\n",
            "Iteration 275, loss = 0.53145881\n",
            "Iteration 276, loss = 0.52958177\n",
            "Iteration 277, loss = 0.52771497\n",
            "Iteration 278, loss = 0.52585919\n",
            "Iteration 279, loss = 0.52401375\n",
            "Iteration 280, loss = 0.52218008\n",
            "Iteration 281, loss = 0.52035872\n",
            "Iteration 282, loss = 0.51855007\n",
            "Iteration 283, loss = 0.51675290\n",
            "Iteration 284, loss = 0.51496494\n",
            "Iteration 285, loss = 0.51318923\n",
            "Iteration 286, loss = 0.51135856\n",
            "Iteration 287, loss = 0.50949045\n",
            "Iteration 288, loss = 0.50761847\n",
            "Iteration 289, loss = 0.50574508\n",
            "Iteration 290, loss = 0.50389823\n",
            "Iteration 291, loss = 0.50209177\n",
            "Iteration 292, loss = 0.50029448\n",
            "Iteration 293, loss = 0.49850809\n",
            "Iteration 294, loss = 0.49673560\n",
            "Iteration 295, loss = 0.49497525\n",
            "Iteration 296, loss = 0.49322647\n",
            "Iteration 297, loss = 0.49149006\n",
            "Iteration 298, loss = 0.48976578\n",
            "Iteration 299, loss = 0.48805288\n",
            "Iteration 300, loss = 0.48635121\n",
            "Iteration 301, loss = 0.48465858\n",
            "Iteration 302, loss = 0.48297548\n",
            "Iteration 303, loss = 0.48130270\n",
            "Iteration 304, loss = 0.47964066\n",
            "Iteration 305, loss = 0.47799021\n",
            "Iteration 306, loss = 0.47635060\n",
            "Iteration 307, loss = 0.47472171\n",
            "Iteration 308, loss = 0.47310359\n",
            "Iteration 309, loss = 0.47149542\n",
            "Iteration 310, loss = 0.46989677\n",
            "Iteration 311, loss = 0.46830773\n",
            "Iteration 312, loss = 0.46672858\n",
            "Iteration 313, loss = 0.46516085\n",
            "Iteration 314, loss = 0.46360118\n",
            "Iteration 315, loss = 0.46204943\n",
            "Iteration 316, loss = 0.46050628\n",
            "Iteration 317, loss = 0.45897211\n",
            "Iteration 318, loss = 0.45744630\n",
            "Iteration 319, loss = 0.45593347\n",
            "Iteration 320, loss = 0.45443297\n",
            "Iteration 321, loss = 0.45294184\n",
            "Iteration 322, loss = 0.45145849\n",
            "Iteration 323, loss = 0.44998281\n",
            "Iteration 324, loss = 0.44851528\n",
            "Iteration 325, loss = 0.44705738\n",
            "Iteration 326, loss = 0.44561079\n",
            "Iteration 327, loss = 0.44416997\n",
            "Iteration 328, loss = 0.44273705\n",
            "Iteration 329, loss = 0.44131292\n",
            "Iteration 330, loss = 0.43989957\n",
            "Iteration 331, loss = 0.43849385\n",
            "Iteration 332, loss = 0.43709885\n",
            "Iteration 333, loss = 0.43571628\n",
            "Iteration 334, loss = 0.43434919\n",
            "Iteration 335, loss = 0.43299210\n",
            "Iteration 336, loss = 0.43164383\n",
            "Iteration 337, loss = 0.43030220\n",
            "Iteration 338, loss = 0.42896521\n",
            "Iteration 339, loss = 0.42763412\n",
            "Iteration 340, loss = 0.42630811\n",
            "Iteration 341, loss = 0.42499042\n",
            "Iteration 342, loss = 0.42368101\n",
            "Iteration 343, loss = 0.42238056\n",
            "Iteration 344, loss = 0.42108806\n",
            "Iteration 345, loss = 0.41980305\n",
            "Iteration 346, loss = 0.41852551\n",
            "Iteration 347, loss = 0.41725716\n",
            "Iteration 348, loss = 0.41599690\n",
            "Iteration 349, loss = 0.41474465\n",
            "Iteration 350, loss = 0.41349935\n",
            "Iteration 351, loss = 0.41226363\n",
            "Iteration 352, loss = 0.41103538\n",
            "Iteration 353, loss = 0.40981535\n",
            "Iteration 354, loss = 0.40860215\n",
            "Iteration 355, loss = 0.40739983\n",
            "Iteration 356, loss = 0.40620485\n",
            "Iteration 357, loss = 0.40501719\n",
            "Iteration 358, loss = 0.40383788\n",
            "Iteration 359, loss = 0.40266582\n",
            "Iteration 360, loss = 0.40150641\n",
            "Iteration 361, loss = 0.40035423\n",
            "Iteration 362, loss = 0.39920878\n",
            "Iteration 363, loss = 0.39806993\n",
            "Iteration 364, loss = 0.39693817\n",
            "Iteration 365, loss = 0.39581320\n",
            "Iteration 366, loss = 0.39469540\n",
            "Iteration 367, loss = 0.39358434\n",
            "Iteration 368, loss = 0.39248184\n",
            "Iteration 369, loss = 0.39138586\n",
            "Iteration 370, loss = 0.39029608\n",
            "Iteration 371, loss = 0.38921203\n",
            "Iteration 372, loss = 0.38813389\n",
            "Iteration 373, loss = 0.38706174\n",
            "Iteration 374, loss = 0.38599643\n",
            "Iteration 375, loss = 0.38493779\n",
            "Iteration 376, loss = 0.38388511\n",
            "Iteration 377, loss = 0.38283911\n",
            "Iteration 378, loss = 0.38180417\n",
            "Iteration 379, loss = 0.38077589\n",
            "Iteration 380, loss = 0.37975314\n",
            "Iteration 381, loss = 0.37873684\n",
            "Iteration 382, loss = 0.37772560\n",
            "Iteration 383, loss = 0.37672098\n",
            "Iteration 384, loss = 0.37572257\n",
            "Iteration 385, loss = 0.37472925\n",
            "Iteration 386, loss = 0.37374128\n",
            "Iteration 387, loss = 0.37275883\n",
            "Iteration 388, loss = 0.37178176\n",
            "Iteration 389, loss = 0.37081127\n",
            "Iteration 390, loss = 0.36984560\n",
            "Iteration 391, loss = 0.36888556\n",
            "Iteration 392, loss = 0.36792838\n",
            "Iteration 393, loss = 0.36697551\n",
            "Iteration 394, loss = 0.36602914\n",
            "Iteration 395, loss = 0.36508833\n",
            "Iteration 396, loss = 0.36415253\n",
            "Iteration 397, loss = 0.36322142\n",
            "Iteration 398, loss = 0.36229694\n",
            "Iteration 399, loss = 0.36137761\n",
            "Iteration 400, loss = 0.36046384\n",
            "Iteration 401, loss = 0.35955905\n",
            "Iteration 402, loss = 0.35865866\n",
            "Iteration 403, loss = 0.35776311\n",
            "Iteration 404, loss = 0.35687230\n",
            "Iteration 405, loss = 0.35598696\n",
            "Iteration 406, loss = 0.35510574\n",
            "Iteration 407, loss = 0.35422974\n",
            "Iteration 408, loss = 0.35335857\n",
            "Iteration 409, loss = 0.35249216\n",
            "Iteration 410, loss = 0.35163082\n",
            "Iteration 411, loss = 0.35077418\n",
            "Iteration 412, loss = 0.34992160\n",
            "Iteration 413, loss = 0.34907315\n",
            "Iteration 414, loss = 0.34822784\n",
            "Iteration 415, loss = 0.34738670\n",
            "Iteration 416, loss = 0.34654939\n",
            "Iteration 417, loss = 0.34571651\n",
            "Iteration 418, loss = 0.34488697\n",
            "Iteration 419, loss = 0.34406254\n",
            "Iteration 420, loss = 0.34324096\n",
            "Iteration 421, loss = 0.34242400\n",
            "Iteration 422, loss = 0.34160987\n",
            "Iteration 423, loss = 0.34079903\n",
            "Iteration 424, loss = 0.33999261\n",
            "Iteration 425, loss = 0.33919613\n",
            "Iteration 426, loss = 0.33840282\n",
            "Iteration 427, loss = 0.33761359\n",
            "Iteration 428, loss = 0.33682825\n",
            "Iteration 429, loss = 0.33604700\n",
            "Iteration 430, loss = 0.33526882\n",
            "Iteration 431, loss = 0.33449366\n",
            "Iteration 432, loss = 0.33372215\n",
            "Iteration 433, loss = 0.33295388\n",
            "Iteration 434, loss = 0.33218873\n",
            "Iteration 435, loss = 0.33142765\n",
            "Iteration 436, loss = 0.33067116\n",
            "Iteration 437, loss = 0.32991550\n",
            "Iteration 438, loss = 0.32916192\n",
            "Iteration 439, loss = 0.32841271\n",
            "Iteration 440, loss = 0.32766603\n",
            "Iteration 441, loss = 0.32692240\n",
            "Iteration 442, loss = 0.32618114\n",
            "Iteration 443, loss = 0.32544283\n",
            "Iteration 444, loss = 0.32470654\n",
            "Iteration 445, loss = 0.32397312\n",
            "Iteration 446, loss = 0.32324208\n",
            "Iteration 447, loss = 0.32251313\n",
            "Iteration 448, loss = 0.32179514\n",
            "Iteration 449, loss = 0.32108141\n",
            "Iteration 450, loss = 0.32036956\n",
            "Iteration 451, loss = 0.31966013\n",
            "Iteration 452, loss = 0.31895187\n",
            "Iteration 453, loss = 0.31824631\n",
            "Iteration 454, loss = 0.31754232\n",
            "Iteration 455, loss = 0.31684092\n",
            "Iteration 456, loss = 0.31614232\n",
            "Iteration 457, loss = 0.31544581\n",
            "Iteration 458, loss = 0.31475180\n",
            "Iteration 459, loss = 0.31406018\n",
            "Iteration 460, loss = 0.31337053\n",
            "Iteration 461, loss = 0.31268375\n",
            "Iteration 462, loss = 0.31199781\n",
            "Iteration 463, loss = 0.31131357\n",
            "Iteration 464, loss = 0.31063131\n",
            "Iteration 465, loss = 0.30995577\n",
            "Iteration 466, loss = 0.30928261\n",
            "Iteration 467, loss = 0.30861234\n",
            "Iteration 468, loss = 0.30794398\n",
            "Iteration 469, loss = 0.30727797\n",
            "Iteration 470, loss = 0.30661394\n",
            "Iteration 471, loss = 0.30595149\n",
            "Iteration 472, loss = 0.30529171\n",
            "Iteration 473, loss = 0.30463405\n",
            "Iteration 474, loss = 0.30397798\n",
            "Iteration 475, loss = 0.30332664\n",
            "Iteration 476, loss = 0.30268577\n",
            "Iteration 477, loss = 0.30204682\n",
            "Iteration 478, loss = 0.30141086\n",
            "Iteration 479, loss = 0.30077641\n",
            "Iteration 480, loss = 0.30014483\n",
            "Iteration 481, loss = 0.29952514\n",
            "Iteration 482, loss = 0.29891494\n",
            "Iteration 483, loss = 0.29830693\n",
            "Iteration 484, loss = 0.29770263\n",
            "Iteration 485, loss = 0.29710263\n",
            "Iteration 486, loss = 0.29650654\n",
            "Iteration 487, loss = 0.29591329\n",
            "Iteration 488, loss = 0.29532245\n",
            "Iteration 489, loss = 0.29473330\n",
            "Iteration 490, loss = 0.29414664\n",
            "Iteration 491, loss = 0.29356262\n",
            "Iteration 492, loss = 0.29298092\n",
            "Iteration 493, loss = 0.29240222\n",
            "Iteration 494, loss = 0.29182664\n",
            "Iteration 495, loss = 0.29125339\n",
            "Iteration 496, loss = 0.29068243\n",
            "Iteration 497, loss = 0.29011638\n",
            "Iteration 498, loss = 0.28955351\n",
            "Iteration 499, loss = 0.28899306\n",
            "Iteration 500, loss = 0.28843507\n",
            "Iteration 501, loss = 0.28787926\n",
            "Iteration 502, loss = 0.28732581\n",
            "Iteration 503, loss = 0.28677385\n",
            "Iteration 504, loss = 0.28622703\n",
            "Iteration 505, loss = 0.28559531\n",
            "Iteration 506, loss = 0.28475915\n",
            "Iteration 507, loss = 0.28385734\n",
            "Iteration 508, loss = 0.28290428\n",
            "Iteration 509, loss = 0.28191970\n",
            "Iteration 510, loss = 0.28091772\n",
            "Iteration 511, loss = 0.27990810\n",
            "Iteration 512, loss = 0.27890174\n",
            "Iteration 513, loss = 0.27791016\n",
            "Iteration 514, loss = 0.27694322\n",
            "Iteration 515, loss = 0.27600843\n",
            "Iteration 516, loss = 0.27510584\n",
            "Iteration 517, loss = 0.27423797\n",
            "Iteration 518, loss = 0.27340457\n",
            "Iteration 519, loss = 0.27260711\n",
            "Iteration 520, loss = 0.27184572\n",
            "Iteration 521, loss = 0.27111744\n",
            "Iteration 522, loss = 0.27042057\n",
            "Iteration 523, loss = 0.26975366\n",
            "Iteration 524, loss = 0.26911255\n",
            "Iteration 525, loss = 0.26849679\n",
            "Iteration 526, loss = 0.26790134\n",
            "Iteration 527, loss = 0.26732513\n",
            "Iteration 528, loss = 0.26676724\n",
            "Iteration 529, loss = 0.26622469\n",
            "Iteration 530, loss = 0.26569680\n",
            "Iteration 531, loss = 0.26518191\n",
            "Iteration 532, loss = 0.26467802\n",
            "Iteration 533, loss = 0.26418384\n",
            "Iteration 534, loss = 0.26370042\n",
            "Iteration 535, loss = 0.26322587\n",
            "Iteration 536, loss = 0.26275983\n",
            "Iteration 537, loss = 0.26230099\n",
            "Iteration 538, loss = 0.26184864\n",
            "Iteration 539, loss = 0.26140324\n",
            "Iteration 540, loss = 0.26096501\n",
            "Iteration 541, loss = 0.26053296\n",
            "Iteration 542, loss = 0.26010702\n",
            "Iteration 543, loss = 0.25968716\n",
            "Iteration 544, loss = 0.25927131\n",
            "Iteration 545, loss = 0.25886127\n",
            "Iteration 546, loss = 0.25845589\n",
            "Iteration 547, loss = 0.25805507\n",
            "Iteration 548, loss = 0.25766153\n",
            "Iteration 549, loss = 0.25727267\n",
            "Iteration 550, loss = 0.25688777\n",
            "Iteration 551, loss = 0.25650631\n",
            "Iteration 552, loss = 0.25612859\n",
            "Iteration 553, loss = 0.25575454\n",
            "Iteration 554, loss = 0.25538378\n",
            "Iteration 555, loss = 0.25501613\n",
            "Iteration 556, loss = 0.25465275\n",
            "Iteration 557, loss = 0.25429197\n",
            "Iteration 558, loss = 0.25393374\n",
            "Iteration 559, loss = 0.25358056\n",
            "Iteration 560, loss = 0.25323023\n",
            "Iteration 561, loss = 0.25288184\n",
            "Iteration 562, loss = 0.25253586\n",
            "Iteration 563, loss = 0.25219216\n",
            "Iteration 564, loss = 0.25185077\n",
            "Iteration 565, loss = 0.25151126\n",
            "Iteration 566, loss = 0.25117416\n",
            "Iteration 567, loss = 0.25083883\n",
            "Iteration 568, loss = 0.25050573\n",
            "Iteration 569, loss = 0.25017498\n",
            "Iteration 570, loss = 0.24984530\n",
            "Iteration 571, loss = 0.24951803\n",
            "Iteration 572, loss = 0.24919186\n",
            "Iteration 573, loss = 0.24886760\n",
            "Iteration 574, loss = 0.24854579\n",
            "Iteration 575, loss = 0.24822515\n",
            "Iteration 576, loss = 0.24790621\n",
            "Iteration 577, loss = 0.24758911\n",
            "Iteration 578, loss = 0.24727392\n",
            "Iteration 579, loss = 0.24695974\n",
            "Iteration 580, loss = 0.24664802\n",
            "Iteration 581, loss = 0.24633723\n",
            "Iteration 582, loss = 0.24602773\n",
            "Iteration 583, loss = 0.24572023\n",
            "Iteration 584, loss = 0.24541424\n",
            "Iteration 585, loss = 0.24511030\n",
            "Iteration 586, loss = 0.24480812\n",
            "Iteration 587, loss = 0.24450697\n",
            "Iteration 588, loss = 0.24420758\n",
            "Iteration 589, loss = 0.24390813\n",
            "Iteration 590, loss = 0.24361009\n",
            "Iteration 591, loss = 0.24331357\n",
            "Iteration 592, loss = 0.24301787\n",
            "Iteration 593, loss = 0.24272336\n",
            "Iteration 594, loss = 0.24243067\n",
            "Iteration 595, loss = 0.24213809\n",
            "Iteration 596, loss = 0.24184807\n",
            "Iteration 597, loss = 0.24155827\n",
            "Iteration 598, loss = 0.24127074\n",
            "Iteration 599, loss = 0.24098359\n",
            "Iteration 600, loss = 0.24069812\n",
            "Iteration 601, loss = 0.24041347\n",
            "Iteration 602, loss = 0.24013006\n",
            "Iteration 603, loss = 0.23984802\n",
            "Iteration 604, loss = 0.23956689\n",
            "Iteration 605, loss = 0.23928678\n",
            "Iteration 606, loss = 0.23900798\n",
            "Iteration 607, loss = 0.23872971\n",
            "Iteration 608, loss = 0.23845366\n",
            "Iteration 609, loss = 0.23817824\n",
            "Iteration 610, loss = 0.23790387\n",
            "Iteration 611, loss = 0.23763125\n",
            "Iteration 612, loss = 0.23735865\n",
            "Iteration 613, loss = 0.23708760\n",
            "Iteration 614, loss = 0.23681673\n",
            "Iteration 615, loss = 0.23654679\n",
            "Iteration 616, loss = 0.23627874\n",
            "Iteration 617, loss = 0.23601053\n",
            "Iteration 618, loss = 0.23574363\n",
            "Iteration 619, loss = 0.23547805\n",
            "Iteration 620, loss = 0.23521284\n",
            "Iteration 621, loss = 0.23494936\n",
            "Iteration 622, loss = 0.23468534\n",
            "Iteration 623, loss = 0.23442287\n",
            "Iteration 624, loss = 0.23416060\n",
            "Iteration 625, loss = 0.23389877\n",
            "Iteration 626, loss = 0.23363895\n",
            "Iteration 627, loss = 0.23337833\n",
            "Iteration 628, loss = 0.23311975\n",
            "Iteration 629, loss = 0.23286109\n",
            "Iteration 630, loss = 0.23260354\n",
            "Iteration 631, loss = 0.23234697\n",
            "Iteration 632, loss = 0.23209152\n",
            "Iteration 633, loss = 0.23183726\n",
            "Iteration 634, loss = 0.23158309\n",
            "Iteration 635, loss = 0.23133028\n",
            "Iteration 636, loss = 0.23107788\n",
            "Iteration 637, loss = 0.23082590\n",
            "Iteration 638, loss = 0.23057550\n",
            "Iteration 639, loss = 0.23032538\n",
            "Iteration 640, loss = 0.23007705\n",
            "Iteration 641, loss = 0.22982901\n",
            "Iteration 642, loss = 0.22958151\n",
            "Iteration 643, loss = 0.22933503\n",
            "Iteration 644, loss = 0.22908895\n",
            "Iteration 645, loss = 0.22884326\n",
            "Iteration 646, loss = 0.22859826\n",
            "Iteration 647, loss = 0.22835403\n",
            "Iteration 648, loss = 0.22811048\n",
            "Iteration 649, loss = 0.22786731\n",
            "Iteration 650, loss = 0.22762522\n",
            "Iteration 651, loss = 0.22738319\n",
            "Iteration 652, loss = 0.22714210\n",
            "Iteration 653, loss = 0.22690150\n",
            "Iteration 654, loss = 0.22666139\n",
            "Iteration 655, loss = 0.22642244\n",
            "Iteration 656, loss = 0.22618339\n",
            "Iteration 657, loss = 0.22594575\n",
            "Iteration 658, loss = 0.22570755\n",
            "Iteration 659, loss = 0.22547059\n",
            "Iteration 660, loss = 0.22523373\n",
            "Iteration 661, loss = 0.22499677\n",
            "Iteration 662, loss = 0.22476123\n",
            "Iteration 663, loss = 0.22452546\n",
            "Iteration 664, loss = 0.22429131\n",
            "Iteration 665, loss = 0.22405757\n",
            "Iteration 666, loss = 0.22382464\n",
            "Iteration 667, loss = 0.22359271\n",
            "Iteration 668, loss = 0.22336065\n",
            "Iteration 669, loss = 0.22313060\n",
            "Iteration 670, loss = 0.22289962\n",
            "Iteration 671, loss = 0.22266954\n",
            "Iteration 672, loss = 0.22244014\n",
            "Iteration 673, loss = 0.22221167\n",
            "Iteration 674, loss = 0.22198333\n",
            "Iteration 675, loss = 0.22175624\n",
            "Iteration 676, loss = 0.22152897\n",
            "Iteration 677, loss = 0.22130304\n",
            "Iteration 678, loss = 0.22107707\n",
            "Iteration 679, loss = 0.22085182\n",
            "Iteration 680, loss = 0.22062726\n",
            "Iteration 681, loss = 0.22040341\n",
            "Iteration 682, loss = 0.22017996\n",
            "Iteration 683, loss = 0.21995635\n",
            "Iteration 684, loss = 0.21973350\n",
            "Iteration 685, loss = 0.21951053\n",
            "Iteration 686, loss = 0.21928797\n",
            "Iteration 687, loss = 0.21906620\n",
            "Iteration 688, loss = 0.21884324\n",
            "Iteration 689, loss = 0.21862107\n",
            "Iteration 690, loss = 0.21839836\n",
            "Iteration 691, loss = 0.21817683\n",
            "Iteration 692, loss = 0.21795496\n",
            "Iteration 693, loss = 0.21773385\n",
            "Iteration 694, loss = 0.21751319\n",
            "Iteration 695, loss = 0.21729293\n",
            "Iteration 696, loss = 0.21707323\n",
            "Iteration 697, loss = 0.21685452\n",
            "Iteration 698, loss = 0.21663562\n",
            "Iteration 699, loss = 0.21641774\n",
            "Iteration 700, loss = 0.21619924\n",
            "Iteration 701, loss = 0.21598190\n",
            "Iteration 702, loss = 0.21576498\n",
            "Iteration 703, loss = 0.21554828\n",
            "Iteration 704, loss = 0.21533344\n",
            "Iteration 705, loss = 0.21511916\n",
            "Iteration 706, loss = 0.21490540\n",
            "Iteration 707, loss = 0.21469234\n",
            "Iteration 708, loss = 0.21447928\n",
            "Iteration 709, loss = 0.21426682\n",
            "Iteration 710, loss = 0.21405578\n",
            "Iteration 711, loss = 0.21384432\n",
            "Iteration 712, loss = 0.21363322\n",
            "Iteration 713, loss = 0.21342300\n",
            "Iteration 714, loss = 0.21321252\n",
            "Iteration 715, loss = 0.21300270\n",
            "Iteration 716, loss = 0.21279332\n",
            "Iteration 717, loss = 0.21258464\n",
            "Iteration 718, loss = 0.21237599\n",
            "Iteration 719, loss = 0.21216827\n",
            "Iteration 720, loss = 0.21196077\n",
            "Iteration 721, loss = 0.21175470\n",
            "Iteration 722, loss = 0.21154816\n",
            "Iteration 723, loss = 0.21134335\n",
            "Iteration 724, loss = 0.21113839\n",
            "Iteration 725, loss = 0.21093421\n",
            "Iteration 726, loss = 0.21073005\n",
            "Iteration 727, loss = 0.21052664\n",
            "Iteration 728, loss = 0.21032310\n",
            "Iteration 729, loss = 0.21012051\n",
            "Iteration 730, loss = 0.20991820\n",
            "Iteration 731, loss = 0.20971653\n",
            "Iteration 732, loss = 0.20951491\n",
            "Iteration 733, loss = 0.20931393\n",
            "Iteration 734, loss = 0.20911369\n",
            "Iteration 735, loss = 0.20891317\n",
            "Iteration 736, loss = 0.20871378\n",
            "Iteration 737, loss = 0.20851397\n",
            "Iteration 738, loss = 0.20831531\n",
            "Iteration 739, loss = 0.20811646\n",
            "Iteration 740, loss = 0.20791859\n",
            "Iteration 741, loss = 0.20772104\n",
            "Iteration 742, loss = 0.20752386\n",
            "Iteration 743, loss = 0.20732667\n",
            "Iteration 744, loss = 0.20713056\n",
            "Iteration 745, loss = 0.20693386\n",
            "Iteration 746, loss = 0.20673808\n",
            "Iteration 747, loss = 0.20654302\n",
            "Iteration 748, loss = 0.20634761\n",
            "Iteration 749, loss = 0.20615296\n",
            "Iteration 750, loss = 0.20595857\n",
            "Iteration 751, loss = 0.20576449\n",
            "Iteration 752, loss = 0.20557083\n",
            "Iteration 753, loss = 0.20537768\n",
            "Iteration 754, loss = 0.20518441\n",
            "Iteration 755, loss = 0.20499186\n",
            "Iteration 756, loss = 0.20479921\n",
            "Iteration 757, loss = 0.20460724\n",
            "Iteration 758, loss = 0.20441519\n",
            "Iteration 759, loss = 0.20422388\n",
            "Iteration 760, loss = 0.20403257\n",
            "Iteration 761, loss = 0.20384203\n",
            "Iteration 762, loss = 0.20365113\n",
            "Iteration 763, loss = 0.20346102\n",
            "Iteration 764, loss = 0.20327073\n",
            "Iteration 765, loss = 0.20308142\n",
            "Iteration 766, loss = 0.20289159\n",
            "Iteration 767, loss = 0.20270296\n",
            "Iteration 768, loss = 0.20251356\n",
            "Iteration 769, loss = 0.20232468\n",
            "Iteration 770, loss = 0.20213640\n",
            "Iteration 771, loss = 0.20194789\n",
            "Iteration 772, loss = 0.20176003\n",
            "Iteration 773, loss = 0.20157315\n",
            "Iteration 774, loss = 0.20138650\n",
            "Iteration 775, loss = 0.20120047\n",
            "Iteration 776, loss = 0.20101443\n",
            "Iteration 777, loss = 0.20082885\n",
            "Iteration 778, loss = 0.20064369\n",
            "Iteration 779, loss = 0.20045837\n",
            "Iteration 780, loss = 0.20027380\n",
            "Iteration 781, loss = 0.20008979\n",
            "Iteration 782, loss = 0.19990683\n",
            "Iteration 783, loss = 0.19972443\n",
            "Iteration 784, loss = 0.19954167\n",
            "Iteration 785, loss = 0.19936017\n",
            "Iteration 786, loss = 0.19917761\n",
            "Iteration 787, loss = 0.19899576\n",
            "Iteration 788, loss = 0.19881359\n",
            "Iteration 789, loss = 0.19863209\n",
            "Iteration 790, loss = 0.19845014\n",
            "Iteration 791, loss = 0.19826959\n",
            "Iteration 792, loss = 0.19808790\n",
            "Iteration 793, loss = 0.19790790\n",
            "Iteration 794, loss = 0.19772706\n",
            "Iteration 795, loss = 0.19754719\n",
            "Iteration 796, loss = 0.19736664\n",
            "Iteration 797, loss = 0.19718695\n",
            "Iteration 798, loss = 0.19700638\n",
            "Iteration 799, loss = 0.19682712\n",
            "Iteration 800, loss = 0.19664716\n",
            "Iteration 1, loss = 0.70477802\n",
            "Iteration 2, loss = 0.70405787\n",
            "Iteration 3, loss = 0.70304569\n",
            "Iteration 4, loss = 0.70185080\n",
            "Iteration 5, loss = 0.70046377\n",
            "Iteration 6, loss = 0.69894006\n",
            "Iteration 7, loss = 0.69751780\n",
            "Iteration 8, loss = 0.69621080\n",
            "Iteration 9, loss = 0.69506114\n",
            "Iteration 10, loss = 0.69391842\n",
            "Iteration 11, loss = 0.69283040\n",
            "Iteration 12, loss = 0.69193906\n",
            "Iteration 13, loss = 0.69112666\n",
            "Iteration 14, loss = 0.69044190\n",
            "Iteration 15, loss = 0.68990281\n",
            "Iteration 16, loss = 0.68957778\n",
            "Iteration 17, loss = 0.68937709\n",
            "Iteration 18, loss = 0.68926418\n",
            "Iteration 19, loss = 0.68913264\n",
            "Iteration 20, loss = 0.68900401\n",
            "Iteration 21, loss = 0.68891060\n",
            "Iteration 22, loss = 0.68885716\n",
            "Iteration 23, loss = 0.68880146\n",
            "Iteration 24, loss = 0.68874359\n",
            "Iteration 25, loss = 0.68869740\n",
            "Iteration 26, loss = 0.68864860\n",
            "Iteration 27, loss = 0.68859537\n",
            "Iteration 28, loss = 0.68853800\n",
            "Iteration 29, loss = 0.68847680\n",
            "Iteration 30, loss = 0.68841208\n",
            "Iteration 31, loss = 0.68834413\n",
            "Iteration 32, loss = 0.68827319\n",
            "Iteration 33, loss = 0.68819962\n",
            "Iteration 34, loss = 0.68812348\n",
            "Iteration 35, loss = 0.68804492\n",
            "Iteration 36, loss = 0.68796411\n",
            "Iteration 37, loss = 0.68788121\n",
            "Iteration 38, loss = 0.68779634\n",
            "Iteration 39, loss = 0.68770964\n",
            "Iteration 40, loss = 0.68762125\n",
            "Iteration 41, loss = 0.68753122\n",
            "Iteration 42, loss = 0.68743960\n",
            "Iteration 43, loss = 0.68734646\n",
            "Iteration 44, loss = 0.68725188\n",
            "Iteration 45, loss = 0.68715590\n",
            "Iteration 46, loss = 0.68705859\n",
            "Iteration 47, loss = 0.68695995\n",
            "Iteration 48, loss = 0.68686005\n",
            "Iteration 49, loss = 0.68675895\n",
            "Iteration 50, loss = 0.68665663\n",
            "Iteration 51, loss = 0.68655315\n",
            "Iteration 52, loss = 0.68645243\n",
            "Iteration 53, loss = 0.68635440\n",
            "Iteration 54, loss = 0.68625561\n",
            "Iteration 55, loss = 0.68615602\n",
            "Iteration 56, loss = 0.68605560\n",
            "Iteration 57, loss = 0.68595436\n",
            "Iteration 58, loss = 0.68585227\n",
            "Iteration 59, loss = 0.68574926\n",
            "Iteration 60, loss = 0.68564533\n",
            "Iteration 61, loss = 0.68554045\n",
            "Iteration 62, loss = 0.68543463\n",
            "Iteration 63, loss = 0.68532787\n",
            "Iteration 64, loss = 0.68522006\n",
            "Iteration 65, loss = 0.68511119\n",
            "Iteration 66, loss = 0.68500129\n",
            "Iteration 67, loss = 0.68489258\n",
            "Iteration 68, loss = 0.68478506\n",
            "Iteration 69, loss = 0.68467637\n",
            "Iteration 70, loss = 0.68456672\n",
            "Iteration 71, loss = 0.68445592\n",
            "Iteration 72, loss = 0.68434391\n",
            "Iteration 73, loss = 0.68423066\n",
            "Iteration 74, loss = 0.68411622\n",
            "Iteration 75, loss = 0.68400055\n",
            "Iteration 76, loss = 0.68388362\n",
            "Iteration 77, loss = 0.68376542\n",
            "Iteration 78, loss = 0.68364593\n",
            "Iteration 79, loss = 0.68352508\n",
            "Iteration 80, loss = 0.68340292\n",
            "Iteration 81, loss = 0.68327941\n",
            "Iteration 82, loss = 0.68315449\n",
            "Iteration 83, loss = 0.68302836\n",
            "Iteration 84, loss = 0.68290101\n",
            "Iteration 85, loss = 0.68277228\n",
            "Iteration 86, loss = 0.68264220\n",
            "Iteration 87, loss = 0.68251063\n",
            "Iteration 88, loss = 0.68237704\n",
            "Iteration 89, loss = 0.68224176\n",
            "Iteration 90, loss = 0.68210490\n",
            "Iteration 91, loss = 0.68196634\n",
            "Iteration 92, loss = 0.68182622\n",
            "Iteration 93, loss = 0.68168451\n",
            "Iteration 94, loss = 0.68154120\n",
            "Iteration 95, loss = 0.68139626\n",
            "Iteration 96, loss = 0.68124969\n",
            "Iteration 97, loss = 0.68110027\n",
            "Iteration 98, loss = 0.68094496\n",
            "Iteration 99, loss = 0.68078733\n",
            "Iteration 100, loss = 0.68062743\n",
            "Iteration 101, loss = 0.68046528\n",
            "Iteration 102, loss = 0.68030095\n",
            "Iteration 103, loss = 0.68013445\n",
            "Iteration 104, loss = 0.67996581\n",
            "Iteration 105, loss = 0.67979508\n",
            "Iteration 106, loss = 0.67962224\n",
            "Iteration 107, loss = 0.67944752\n",
            "Iteration 108, loss = 0.67927095\n",
            "Iteration 109, loss = 0.67909240\n",
            "Iteration 110, loss = 0.67891195\n",
            "Iteration 111, loss = 0.67872992\n",
            "Iteration 112, loss = 0.67854584\n",
            "Iteration 113, loss = 0.67835932\n",
            "Iteration 114, loss = 0.67817073\n",
            "Iteration 115, loss = 0.67798013\n",
            "Iteration 116, loss = 0.67778751\n",
            "Iteration 117, loss = 0.67759284\n",
            "Iteration 118, loss = 0.67739629\n",
            "Iteration 119, loss = 0.67719764\n",
            "Iteration 120, loss = 0.67699883\n",
            "Iteration 121, loss = 0.67680139\n",
            "Iteration 122, loss = 0.67660127\n",
            "Iteration 123, loss = 0.67639771\n",
            "Iteration 124, loss = 0.67619160\n",
            "Iteration 125, loss = 0.67598295\n",
            "Iteration 126, loss = 0.67577190\n",
            "Iteration 127, loss = 0.67555835\n",
            "Iteration 128, loss = 0.67534241\n",
            "Iteration 129, loss = 0.67512408\n",
            "Iteration 130, loss = 0.67490163\n",
            "Iteration 131, loss = 0.67467499\n",
            "Iteration 132, loss = 0.67444210\n",
            "Iteration 133, loss = 0.67420279\n",
            "Iteration 134, loss = 0.67395462\n",
            "Iteration 135, loss = 0.67370205\n",
            "Iteration 136, loss = 0.67344534\n",
            "Iteration 137, loss = 0.67318468\n",
            "Iteration 138, loss = 0.67291790\n",
            "Iteration 139, loss = 0.67264413\n",
            "Iteration 140, loss = 0.67236566\n",
            "Iteration 141, loss = 0.67208276\n",
            "Iteration 142, loss = 0.67179570\n",
            "Iteration 143, loss = 0.67150456\n",
            "Iteration 144, loss = 0.67121005\n",
            "Iteration 145, loss = 0.67090956\n",
            "Iteration 146, loss = 0.67060025\n",
            "Iteration 147, loss = 0.67027837\n",
            "Iteration 148, loss = 0.66994510\n",
            "Iteration 149, loss = 0.66960316\n",
            "Iteration 150, loss = 0.66925138\n",
            "Iteration 151, loss = 0.66889353\n",
            "Iteration 152, loss = 0.66852986\n",
            "Iteration 153, loss = 0.66816392\n",
            "Iteration 154, loss = 0.66779629\n",
            "Iteration 155, loss = 0.66741425\n",
            "Iteration 156, loss = 0.66701040\n",
            "Iteration 157, loss = 0.66661039\n",
            "Iteration 158, loss = 0.66620561\n",
            "Iteration 159, loss = 0.66580063\n",
            "Iteration 160, loss = 0.66538813\n",
            "Iteration 161, loss = 0.66496531\n",
            "Iteration 162, loss = 0.66453917\n",
            "Iteration 163, loss = 0.66410952\n",
            "Iteration 164, loss = 0.66367408\n",
            "Iteration 165, loss = 0.66323358\n",
            "Iteration 166, loss = 0.66278823\n",
            "Iteration 167, loss = 0.66232705\n",
            "Iteration 168, loss = 0.66184632\n",
            "Iteration 169, loss = 0.66135882\n",
            "Iteration 170, loss = 0.66086524\n",
            "Iteration 171, loss = 0.66036624\n",
            "Iteration 172, loss = 0.65987270\n",
            "Iteration 173, loss = 0.65937707\n",
            "Iteration 174, loss = 0.65887523\n",
            "Iteration 175, loss = 0.65836124\n",
            "Iteration 176, loss = 0.65784250\n",
            "Iteration 177, loss = 0.65731310\n",
            "Iteration 178, loss = 0.65677324\n",
            "Iteration 179, loss = 0.65622724\n",
            "Iteration 180, loss = 0.65566526\n",
            "Iteration 181, loss = 0.65509954\n",
            "Iteration 182, loss = 0.65452738\n",
            "Iteration 183, loss = 0.65394980\n",
            "Iteration 184, loss = 0.65336691\n",
            "Iteration 185, loss = 0.65277923\n",
            "Iteration 186, loss = 0.65218396\n",
            "Iteration 187, loss = 0.65156912\n",
            "Iteration 188, loss = 0.65094645\n",
            "Iteration 189, loss = 0.65031241\n",
            "Iteration 190, loss = 0.64966313\n",
            "Iteration 191, loss = 0.64898180\n",
            "Iteration 192, loss = 0.64828787\n",
            "Iteration 193, loss = 0.64758402\n",
            "Iteration 194, loss = 0.64687133\n",
            "Iteration 195, loss = 0.64615016\n",
            "Iteration 196, loss = 0.64541972\n",
            "Iteration 197, loss = 0.64468164\n",
            "Iteration 198, loss = 0.64391351\n",
            "Iteration 199, loss = 0.64312654\n",
            "Iteration 200, loss = 0.64233094\n",
            "Iteration 201, loss = 0.64152720\n",
            "Iteration 202, loss = 0.64071591\n",
            "Iteration 203, loss = 0.63990924\n",
            "Iteration 204, loss = 0.63909500\n",
            "Iteration 205, loss = 0.63827537\n",
            "Iteration 206, loss = 0.63743827\n",
            "Iteration 207, loss = 0.63652306\n",
            "Iteration 208, loss = 0.63551393\n",
            "Iteration 209, loss = 0.63446081\n",
            "Iteration 210, loss = 0.63338636\n",
            "Iteration 211, loss = 0.63229309\n",
            "Iteration 212, loss = 0.63118284\n",
            "Iteration 213, loss = 0.63004808\n",
            "Iteration 214, loss = 0.62889954\n",
            "Iteration 215, loss = 0.62773920\n",
            "Iteration 216, loss = 0.62656867\n",
            "Iteration 217, loss = 0.62537313\n",
            "Iteration 218, loss = 0.62416516\n",
            "Iteration 219, loss = 0.62297385\n",
            "Iteration 220, loss = 0.62178322\n",
            "Iteration 221, loss = 0.62056163\n",
            "Iteration 222, loss = 0.61931910\n",
            "Iteration 223, loss = 0.61803473\n",
            "Iteration 224, loss = 0.61673803\n",
            "Iteration 225, loss = 0.61539683\n",
            "Iteration 226, loss = 0.61403305\n",
            "Iteration 227, loss = 0.61268091\n",
            "Iteration 228, loss = 0.61124074\n",
            "Iteration 229, loss = 0.60977400\n",
            "Iteration 230, loss = 0.60829474\n",
            "Iteration 231, loss = 0.60680521\n",
            "Iteration 232, loss = 0.60531927\n",
            "Iteration 233, loss = 0.60386284\n",
            "Iteration 234, loss = 0.60240258\n",
            "Iteration 235, loss = 0.60092540\n",
            "Iteration 236, loss = 0.59940338\n",
            "Iteration 237, loss = 0.59785298\n",
            "Iteration 238, loss = 0.59627695\n",
            "Iteration 239, loss = 0.59467934\n",
            "Iteration 240, loss = 0.59308801\n",
            "Iteration 241, loss = 0.59149456\n",
            "Iteration 242, loss = 0.58989970\n",
            "Iteration 243, loss = 0.58830772\n",
            "Iteration 244, loss = 0.58671522\n",
            "Iteration 245, loss = 0.58512271\n",
            "Iteration 246, loss = 0.58353121\n",
            "Iteration 247, loss = 0.58194119\n",
            "Iteration 248, loss = 0.58035226\n",
            "Iteration 249, loss = 0.57876627\n",
            "Iteration 250, loss = 0.57719516\n",
            "Iteration 251, loss = 0.57563083\n",
            "Iteration 252, loss = 0.57407002\n",
            "Iteration 253, loss = 0.57251931\n",
            "Iteration 254, loss = 0.57097626\n",
            "Iteration 255, loss = 0.56943679\n",
            "Iteration 256, loss = 0.56790122\n",
            "Iteration 257, loss = 0.56636817\n",
            "Iteration 258, loss = 0.56484152\n",
            "Iteration 259, loss = 0.56332096\n",
            "Iteration 260, loss = 0.56180434\n",
            "Iteration 261, loss = 0.56029023\n",
            "Iteration 262, loss = 0.55877995\n",
            "Iteration 263, loss = 0.55727337\n",
            "Iteration 264, loss = 0.55577159\n",
            "Iteration 265, loss = 0.55423276\n",
            "Iteration 266, loss = 0.55268280\n",
            "Iteration 267, loss = 0.55113222\n",
            "Iteration 268, loss = 0.54958066\n",
            "Iteration 269, loss = 0.54802970\n",
            "Iteration 270, loss = 0.54648147\n",
            "Iteration 271, loss = 0.54493630\n",
            "Iteration 272, loss = 0.54339284\n",
            "Iteration 273, loss = 0.54185125\n",
            "Iteration 274, loss = 0.54029217\n",
            "Iteration 275, loss = 0.53869937\n",
            "Iteration 276, loss = 0.53710615\n",
            "Iteration 277, loss = 0.53551183\n",
            "Iteration 278, loss = 0.53391695\n",
            "Iteration 279, loss = 0.53232349\n",
            "Iteration 280, loss = 0.53073117\n",
            "Iteration 281, loss = 0.52914138\n",
            "Iteration 282, loss = 0.52755380\n",
            "Iteration 283, loss = 0.52596780\n",
            "Iteration 284, loss = 0.52438467\n",
            "Iteration 285, loss = 0.52280523\n",
            "Iteration 286, loss = 0.52122797\n",
            "Iteration 287, loss = 0.51965608\n",
            "Iteration 288, loss = 0.51809013\n",
            "Iteration 289, loss = 0.51652803\n",
            "Iteration 290, loss = 0.51497012\n",
            "Iteration 291, loss = 0.51341665\n",
            "Iteration 292, loss = 0.51186779\n",
            "Iteration 293, loss = 0.51032270\n",
            "Iteration 294, loss = 0.50878416\n",
            "Iteration 295, loss = 0.50725039\n",
            "Iteration 296, loss = 0.50572202\n",
            "Iteration 297, loss = 0.50419874\n",
            "Iteration 298, loss = 0.50268299\n",
            "Iteration 299, loss = 0.50117244\n",
            "Iteration 300, loss = 0.49966678\n",
            "Iteration 301, loss = 0.49816640\n",
            "Iteration 302, loss = 0.49667045\n",
            "Iteration 303, loss = 0.49518054\n",
            "Iteration 304, loss = 0.49369588\n",
            "Iteration 305, loss = 0.49221673\n",
            "Iteration 306, loss = 0.49074301\n",
            "Iteration 307, loss = 0.48927536\n",
            "Iteration 308, loss = 0.48781442\n",
            "Iteration 309, loss = 0.48635904\n",
            "Iteration 310, loss = 0.48490968\n",
            "Iteration 311, loss = 0.48346791\n",
            "Iteration 312, loss = 0.48203695\n",
            "Iteration 313, loss = 0.48061272\n",
            "Iteration 314, loss = 0.47919529\n",
            "Iteration 315, loss = 0.47778473\n",
            "Iteration 316, loss = 0.47638011\n",
            "Iteration 317, loss = 0.47498227\n",
            "Iteration 318, loss = 0.47359226\n",
            "Iteration 319, loss = 0.47220983\n",
            "Iteration 320, loss = 0.47083321\n",
            "Iteration 321, loss = 0.46942787\n",
            "Iteration 322, loss = 0.46796144\n",
            "Iteration 323, loss = 0.46649518\n",
            "Iteration 324, loss = 0.46502947\n",
            "Iteration 325, loss = 0.46356845\n",
            "Iteration 326, loss = 0.46210991\n",
            "Iteration 327, loss = 0.46065389\n",
            "Iteration 328, loss = 0.45920065\n",
            "Iteration 329, loss = 0.45775055\n",
            "Iteration 330, loss = 0.45630347\n",
            "Iteration 331, loss = 0.45486016\n",
            "Iteration 332, loss = 0.45342230\n",
            "Iteration 333, loss = 0.45199076\n",
            "Iteration 334, loss = 0.45056629\n",
            "Iteration 335, loss = 0.44914908\n",
            "Iteration 336, loss = 0.44773856\n",
            "Iteration 337, loss = 0.44631181\n",
            "Iteration 338, loss = 0.44483941\n",
            "Iteration 339, loss = 0.44335456\n",
            "Iteration 340, loss = 0.44185854\n",
            "Iteration 341, loss = 0.44035894\n",
            "Iteration 342, loss = 0.43885814\n",
            "Iteration 343, loss = 0.43735502\n",
            "Iteration 344, loss = 0.43585229\n",
            "Iteration 345, loss = 0.43435574\n",
            "Iteration 346, loss = 0.43286097\n",
            "Iteration 347, loss = 0.43136906\n",
            "Iteration 348, loss = 0.42988209\n",
            "Iteration 349, loss = 0.42840364\n",
            "Iteration 350, loss = 0.42693269\n",
            "Iteration 351, loss = 0.42547080\n",
            "Iteration 352, loss = 0.42401743\n",
            "Iteration 353, loss = 0.42257445\n",
            "Iteration 354, loss = 0.42114959\n",
            "Iteration 355, loss = 0.41973425\n",
            "Iteration 356, loss = 0.41832971\n",
            "Iteration 357, loss = 0.41693664\n",
            "Iteration 358, loss = 0.41555593\n",
            "Iteration 359, loss = 0.41418559\n",
            "Iteration 360, loss = 0.41282822\n",
            "Iteration 361, loss = 0.41148052\n",
            "Iteration 362, loss = 0.41013995\n",
            "Iteration 363, loss = 0.40880912\n",
            "Iteration 364, loss = 0.40748830\n",
            "Iteration 365, loss = 0.40617828\n",
            "Iteration 366, loss = 0.40487819\n",
            "Iteration 367, loss = 0.40358784\n",
            "Iteration 368, loss = 0.40230619\n",
            "Iteration 369, loss = 0.40103396\n",
            "Iteration 370, loss = 0.39977221\n",
            "Iteration 371, loss = 0.39851953\n",
            "Iteration 372, loss = 0.39727666\n",
            "Iteration 373, loss = 0.39604563\n",
            "Iteration 374, loss = 0.39482497\n",
            "Iteration 375, loss = 0.39361508\n",
            "Iteration 376, loss = 0.39242203\n",
            "Iteration 377, loss = 0.39124843\n",
            "Iteration 378, loss = 0.39010111\n",
            "Iteration 379, loss = 0.38897062\n",
            "Iteration 380, loss = 0.38785082\n",
            "Iteration 381, loss = 0.38674087\n",
            "Iteration 382, loss = 0.38564009\n",
            "Iteration 383, loss = 0.38454822\n",
            "Iteration 384, loss = 0.38346675\n",
            "Iteration 385, loss = 0.38239507\n",
            "Iteration 386, loss = 0.38133306\n",
            "Iteration 387, loss = 0.38027819\n",
            "Iteration 388, loss = 0.37923125\n",
            "Iteration 389, loss = 0.37819181\n",
            "Iteration 390, loss = 0.37715942\n",
            "Iteration 391, loss = 0.37613410\n",
            "Iteration 392, loss = 0.37511603\n",
            "Iteration 393, loss = 0.37410567\n",
            "Iteration 394, loss = 0.37310162\n",
            "Iteration 395, loss = 0.37210430\n",
            "Iteration 396, loss = 0.37111397\n",
            "Iteration 397, loss = 0.37013036\n",
            "Iteration 398, loss = 0.36915402\n",
            "Iteration 399, loss = 0.36818513\n",
            "Iteration 400, loss = 0.36722221\n",
            "Iteration 401, loss = 0.36626551\n",
            "Iteration 402, loss = 0.36531537\n",
            "Iteration 403, loss = 0.36437208\n",
            "Iteration 404, loss = 0.36343479\n",
            "Iteration 405, loss = 0.36250327\n",
            "Iteration 406, loss = 0.36157714\n",
            "Iteration 407, loss = 0.36065633\n",
            "Iteration 408, loss = 0.35974191\n",
            "Iteration 409, loss = 0.35883265\n",
            "Iteration 410, loss = 0.35793057\n",
            "Iteration 411, loss = 0.35703371\n",
            "Iteration 412, loss = 0.35614657\n",
            "Iteration 413, loss = 0.35526623\n",
            "Iteration 414, loss = 0.35439129\n",
            "Iteration 415, loss = 0.35352116\n",
            "Iteration 416, loss = 0.35265633\n",
            "Iteration 417, loss = 0.35179615\n",
            "Iteration 418, loss = 0.35094118\n",
            "Iteration 419, loss = 0.35009275\n",
            "Iteration 420, loss = 0.34925398\n",
            "Iteration 421, loss = 0.34841977\n",
            "Iteration 422, loss = 0.34759001\n",
            "Iteration 423, loss = 0.34676421\n",
            "Iteration 424, loss = 0.34594189\n",
            "Iteration 425, loss = 0.34512308\n",
            "Iteration 426, loss = 0.34430911\n",
            "Iteration 427, loss = 0.34349892\n",
            "Iteration 428, loss = 0.34269509\n",
            "Iteration 429, loss = 0.34189514\n",
            "Iteration 430, loss = 0.34110154\n",
            "Iteration 431, loss = 0.34031206\n",
            "Iteration 432, loss = 0.33953305\n",
            "Iteration 433, loss = 0.33875932\n",
            "Iteration 434, loss = 0.33798947\n",
            "Iteration 435, loss = 0.33722352\n",
            "Iteration 436, loss = 0.33646161\n",
            "Iteration 437, loss = 0.33570386\n",
            "Iteration 438, loss = 0.33494831\n",
            "Iteration 439, loss = 0.33419642\n",
            "Iteration 440, loss = 0.33344813\n",
            "Iteration 441, loss = 0.33270610\n",
            "Iteration 442, loss = 0.33197156\n",
            "Iteration 443, loss = 0.33124074\n",
            "Iteration 444, loss = 0.33051375\n",
            "Iteration 445, loss = 0.32979383\n",
            "Iteration 446, loss = 0.32907891\n",
            "Iteration 447, loss = 0.32836806\n",
            "Iteration 448, loss = 0.32766217\n",
            "Iteration 449, loss = 0.32696163\n",
            "Iteration 450, loss = 0.32626502\n",
            "Iteration 451, loss = 0.32557203\n",
            "Iteration 452, loss = 0.32488256\n",
            "Iteration 453, loss = 0.32419677\n",
            "Iteration 454, loss = 0.32351260\n",
            "Iteration 455, loss = 0.32283102\n",
            "Iteration 456, loss = 0.32215260\n",
            "Iteration 457, loss = 0.32147744\n",
            "Iteration 458, loss = 0.32080553\n",
            "Iteration 459, loss = 0.32013723\n",
            "Iteration 460, loss = 0.31947214\n",
            "Iteration 461, loss = 0.31881124\n",
            "Iteration 462, loss = 0.31804391\n",
            "Iteration 463, loss = 0.31697582\n",
            "Iteration 464, loss = 0.31582180\n",
            "Iteration 465, loss = 0.31460515\n",
            "Iteration 466, loss = 0.31335405\n",
            "Iteration 467, loss = 0.31207263\n",
            "Iteration 468, loss = 0.31078828\n",
            "Iteration 469, loss = 0.30950304\n",
            "Iteration 470, loss = 0.30822814\n",
            "Iteration 471, loss = 0.30697330\n",
            "Iteration 472, loss = 0.30574641\n",
            "Iteration 473, loss = 0.30455543\n",
            "Iteration 474, loss = 0.30340620\n",
            "Iteration 475, loss = 0.30230209\n",
            "Iteration 476, loss = 0.30124481\n",
            "Iteration 477, loss = 0.30020430\n",
            "Iteration 478, loss = 0.29920024\n",
            "Iteration 479, loss = 0.29823844\n",
            "Iteration 480, loss = 0.29733260\n",
            "Iteration 481, loss = 0.29648073\n",
            "Iteration 482, loss = 0.29566046\n",
            "Iteration 483, loss = 0.29487051\n",
            "Iteration 484, loss = 0.29410855\n",
            "Iteration 485, loss = 0.29337438\n",
            "Iteration 486, loss = 0.29266251\n",
            "Iteration 487, loss = 0.29197105\n",
            "Iteration 488, loss = 0.29129742\n",
            "Iteration 489, loss = 0.29064020\n",
            "Iteration 490, loss = 0.28999824\n",
            "Iteration 491, loss = 0.28936891\n",
            "Iteration 492, loss = 0.28875239\n",
            "Iteration 493, loss = 0.28814789\n",
            "Iteration 494, loss = 0.28755591\n",
            "Iteration 495, loss = 0.28697428\n",
            "Iteration 496, loss = 0.28640042\n",
            "Iteration 497, loss = 0.28583419\n",
            "Iteration 498, loss = 0.28527540\n",
            "Iteration 499, loss = 0.28472328\n",
            "Iteration 500, loss = 0.28417903\n",
            "Iteration 501, loss = 0.28364151\n",
            "Iteration 502, loss = 0.28311019\n",
            "Iteration 503, loss = 0.28258497\n",
            "Iteration 504, loss = 0.28206550\n",
            "Iteration 505, loss = 0.28155142\n",
            "Iteration 506, loss = 0.28104318\n",
            "Iteration 507, loss = 0.28054014\n",
            "Iteration 508, loss = 0.28004341\n",
            "Iteration 509, loss = 0.27955386\n",
            "Iteration 510, loss = 0.27906728\n",
            "Iteration 511, loss = 0.27858487\n",
            "Iteration 512, loss = 0.27810754\n",
            "Iteration 513, loss = 0.27763532\n",
            "Iteration 514, loss = 0.27716989\n",
            "Iteration 515, loss = 0.27670975\n",
            "Iteration 516, loss = 0.27625677\n",
            "Iteration 517, loss = 0.27580862\n",
            "Iteration 518, loss = 0.27536605\n",
            "Iteration 519, loss = 0.27492924\n",
            "Iteration 520, loss = 0.27449827\n",
            "Iteration 521, loss = 0.27407291\n",
            "Iteration 522, loss = 0.27365287\n",
            "Iteration 523, loss = 0.27323724\n",
            "Iteration 524, loss = 0.27282619\n",
            "Iteration 525, loss = 0.27241843\n",
            "Iteration 526, loss = 0.27201452\n",
            "Iteration 527, loss = 0.27161422\n",
            "Iteration 528, loss = 0.27121658\n",
            "Iteration 529, loss = 0.27082222\n",
            "Iteration 530, loss = 0.27043074\n",
            "Iteration 531, loss = 0.27004351\n",
            "Iteration 532, loss = 0.26966030\n",
            "Iteration 533, loss = 0.26927994\n",
            "Iteration 534, loss = 0.26890293\n",
            "Iteration 535, loss = 0.26853054\n",
            "Iteration 536, loss = 0.26815980\n",
            "Iteration 537, loss = 0.26779353\n",
            "Iteration 538, loss = 0.26743007\n",
            "Iteration 539, loss = 0.26706957\n",
            "Iteration 540, loss = 0.26671152\n",
            "Iteration 541, loss = 0.26635657\n",
            "Iteration 542, loss = 0.26600344\n",
            "Iteration 543, loss = 0.26565290\n",
            "Iteration 544, loss = 0.26530491\n",
            "Iteration 545, loss = 0.26495918\n",
            "Iteration 546, loss = 0.26461566\n",
            "Iteration 547, loss = 0.26427465\n",
            "Iteration 548, loss = 0.26393621\n",
            "Iteration 549, loss = 0.26360089\n",
            "Iteration 550, loss = 0.26326732\n",
            "Iteration 551, loss = 0.26293565\n",
            "Iteration 552, loss = 0.26260545\n",
            "Iteration 553, loss = 0.26227745\n",
            "Iteration 554, loss = 0.26195172\n",
            "Iteration 555, loss = 0.26162856\n",
            "Iteration 556, loss = 0.26130697\n",
            "Iteration 557, loss = 0.26098752\n",
            "Iteration 558, loss = 0.26067021\n",
            "Iteration 559, loss = 0.26035491\n",
            "Iteration 560, loss = 0.26004204\n",
            "Iteration 561, loss = 0.25973103\n",
            "Iteration 562, loss = 0.25942195\n",
            "Iteration 563, loss = 0.25911449\n",
            "Iteration 564, loss = 0.25880870\n",
            "Iteration 565, loss = 0.25850426\n",
            "Iteration 566, loss = 0.25820196\n",
            "Iteration 567, loss = 0.25790197\n",
            "Iteration 568, loss = 0.25760309\n",
            "Iteration 569, loss = 0.25730656\n",
            "Iteration 570, loss = 0.25701286\n",
            "Iteration 571, loss = 0.25672050\n",
            "Iteration 572, loss = 0.25642972\n",
            "Iteration 573, loss = 0.25614055\n",
            "Iteration 574, loss = 0.25585368\n",
            "Iteration 575, loss = 0.25556751\n",
            "Iteration 576, loss = 0.25528393\n",
            "Iteration 577, loss = 0.25500258\n",
            "Iteration 578, loss = 0.25472337\n",
            "Iteration 579, loss = 0.25444558\n",
            "Iteration 580, loss = 0.25417035\n",
            "Iteration 581, loss = 0.25389694\n",
            "Iteration 582, loss = 0.25362498\n",
            "Iteration 583, loss = 0.25335420\n",
            "Iteration 584, loss = 0.25308519\n",
            "Iteration 585, loss = 0.25281701\n",
            "Iteration 586, loss = 0.25255064\n",
            "Iteration 587, loss = 0.25228523\n",
            "Iteration 588, loss = 0.25202240\n",
            "Iteration 589, loss = 0.25176105\n",
            "Iteration 590, loss = 0.25150143\n",
            "Iteration 591, loss = 0.25124384\n",
            "Iteration 592, loss = 0.25098749\n",
            "Iteration 593, loss = 0.25073255\n",
            "Iteration 594, loss = 0.25047837\n",
            "Iteration 595, loss = 0.25022526\n",
            "Iteration 596, loss = 0.24997264\n",
            "Iteration 597, loss = 0.24972202\n",
            "Iteration 598, loss = 0.24947209\n",
            "Iteration 599, loss = 0.24922342\n",
            "Iteration 600, loss = 0.24897586\n",
            "Iteration 601, loss = 0.24872885\n",
            "Iteration 602, loss = 0.24848342\n",
            "Iteration 603, loss = 0.24823846\n",
            "Iteration 604, loss = 0.24799509\n",
            "Iteration 605, loss = 0.24775293\n",
            "Iteration 606, loss = 0.24751180\n",
            "Iteration 607, loss = 0.24727213\n",
            "Iteration 608, loss = 0.24703277\n",
            "Iteration 609, loss = 0.24679511\n",
            "Iteration 610, loss = 0.24655762\n",
            "Iteration 611, loss = 0.24632169\n",
            "Iteration 612, loss = 0.24608642\n",
            "Iteration 613, loss = 0.24585209\n",
            "Iteration 614, loss = 0.24561885\n",
            "Iteration 615, loss = 0.24538626\n",
            "Iteration 616, loss = 0.24515485\n",
            "Iteration 617, loss = 0.24492420\n",
            "Iteration 618, loss = 0.24469499\n",
            "Iteration 619, loss = 0.24446585\n",
            "Iteration 620, loss = 0.24423821\n",
            "Iteration 621, loss = 0.24401091\n",
            "Iteration 622, loss = 0.24378429\n",
            "Iteration 623, loss = 0.24355938\n",
            "Iteration 624, loss = 0.24333433\n",
            "Iteration 625, loss = 0.24311019\n",
            "Iteration 626, loss = 0.24288712\n",
            "Iteration 627, loss = 0.24266501\n",
            "Iteration 628, loss = 0.24244340\n",
            "Iteration 629, loss = 0.24222302\n",
            "Iteration 630, loss = 0.24200313\n",
            "Iteration 631, loss = 0.24178411\n",
            "Iteration 632, loss = 0.24156616\n",
            "Iteration 633, loss = 0.24134923\n",
            "Iteration 634, loss = 0.24113342\n",
            "Iteration 635, loss = 0.24091834\n",
            "Iteration 636, loss = 0.24070411\n",
            "Iteration 637, loss = 0.24049076\n",
            "Iteration 638, loss = 0.24027760\n",
            "Iteration 639, loss = 0.24006590\n",
            "Iteration 640, loss = 0.23985382\n",
            "Iteration 641, loss = 0.23964358\n",
            "Iteration 642, loss = 0.23943293\n",
            "Iteration 643, loss = 0.23922422\n",
            "Iteration 644, loss = 0.23901597\n",
            "Iteration 645, loss = 0.23880824\n",
            "Iteration 646, loss = 0.23860169\n",
            "Iteration 647, loss = 0.23839509\n",
            "Iteration 648, loss = 0.23818949\n",
            "Iteration 649, loss = 0.23798445\n",
            "Iteration 650, loss = 0.23777981\n",
            "Iteration 651, loss = 0.23757620\n",
            "Iteration 652, loss = 0.23737290\n",
            "Iteration 653, loss = 0.23717040\n",
            "Iteration 654, loss = 0.23696905\n",
            "Iteration 655, loss = 0.23676775\n",
            "Iteration 656, loss = 0.23656768\n",
            "Iteration 657, loss = 0.23636762\n",
            "Iteration 658, loss = 0.23616836\n",
            "Iteration 659, loss = 0.23596939\n",
            "Iteration 660, loss = 0.23577079\n",
            "Iteration 661, loss = 0.23557377\n",
            "Iteration 662, loss = 0.23537599\n",
            "Iteration 663, loss = 0.23517980\n",
            "Iteration 664, loss = 0.23498360\n",
            "Iteration 665, loss = 0.23478809\n",
            "Iteration 666, loss = 0.23459434\n",
            "Iteration 667, loss = 0.23440031\n",
            "Iteration 668, loss = 0.23420832\n",
            "Iteration 669, loss = 0.23401545\n",
            "Iteration 670, loss = 0.23382468\n",
            "Iteration 671, loss = 0.23363383\n",
            "Iteration 672, loss = 0.23344365\n",
            "Iteration 673, loss = 0.23325459\n",
            "Iteration 674, loss = 0.23306536\n",
            "Iteration 675, loss = 0.23287781\n",
            "Iteration 676, loss = 0.23269046\n",
            "Iteration 677, loss = 0.23250315\n",
            "Iteration 678, loss = 0.23231756\n",
            "Iteration 679, loss = 0.23213158\n",
            "Iteration 680, loss = 0.23194666\n",
            "Iteration 681, loss = 0.23176158\n",
            "Iteration 682, loss = 0.23157776\n",
            "Iteration 683, loss = 0.23139416\n",
            "Iteration 684, loss = 0.23121093\n",
            "Iteration 685, loss = 0.23102802\n",
            "Iteration 686, loss = 0.23084549\n",
            "Iteration 687, loss = 0.23066355\n",
            "Iteration 688, loss = 0.23048150\n",
            "Iteration 689, loss = 0.23030042\n",
            "Iteration 690, loss = 0.23011955\n",
            "Iteration 691, loss = 0.22993896\n",
            "Iteration 692, loss = 0.22975922\n",
            "Iteration 693, loss = 0.22957895\n",
            "Iteration 694, loss = 0.22939917\n",
            "Iteration 695, loss = 0.22921941\n",
            "Iteration 696, loss = 0.22904044\n",
            "Iteration 697, loss = 0.22886139\n",
            "Iteration 698, loss = 0.22868254\n",
            "Iteration 699, loss = 0.22850351\n",
            "Iteration 700, loss = 0.22832414\n",
            "Iteration 701, loss = 0.22814581\n",
            "Iteration 702, loss = 0.22796700\n",
            "Iteration 703, loss = 0.22778877\n",
            "Iteration 704, loss = 0.22761073\n",
            "Iteration 705, loss = 0.22743275\n",
            "Iteration 706, loss = 0.22725552\n",
            "Iteration 707, loss = 0.22707798\n",
            "Iteration 708, loss = 0.22690067\n",
            "Iteration 709, loss = 0.22672435\n",
            "Iteration 710, loss = 0.22654782\n",
            "Iteration 711, loss = 0.22637223\n",
            "Iteration 712, loss = 0.22619650\n",
            "Iteration 713, loss = 0.22602232\n",
            "Iteration 714, loss = 0.22584782\n",
            "Iteration 715, loss = 0.22567461\n",
            "Iteration 716, loss = 0.22550116\n",
            "Iteration 717, loss = 0.22532796\n",
            "Iteration 718, loss = 0.22515540\n",
            "Iteration 719, loss = 0.22498297\n",
            "Iteration 720, loss = 0.22481132\n",
            "Iteration 721, loss = 0.22463930\n",
            "Iteration 722, loss = 0.22446842\n",
            "Iteration 723, loss = 0.22429711\n",
            "Iteration 724, loss = 0.22412563\n",
            "Iteration 725, loss = 0.22395470\n",
            "Iteration 726, loss = 0.22378390\n",
            "Iteration 727, loss = 0.22361377\n",
            "Iteration 728, loss = 0.22344346\n",
            "Iteration 729, loss = 0.22327433\n",
            "Iteration 730, loss = 0.22310510\n",
            "Iteration 731, loss = 0.22293623\n",
            "Iteration 732, loss = 0.22276794\n",
            "Iteration 733, loss = 0.22259940\n",
            "Iteration 734, loss = 0.22243186\n",
            "Iteration 735, loss = 0.22226358\n",
            "Iteration 736, loss = 0.22209683\n",
            "Iteration 737, loss = 0.22192955\n",
            "Iteration 738, loss = 0.22176244\n",
            "Iteration 739, loss = 0.22159654\n",
            "Iteration 740, loss = 0.22143011\n",
            "Iteration 741, loss = 0.22126455\n",
            "Iteration 742, loss = 0.22109932\n",
            "Iteration 743, loss = 0.22093462\n",
            "Iteration 744, loss = 0.22077024\n",
            "Iteration 745, loss = 0.22060609\n",
            "Iteration 746, loss = 0.22044236\n",
            "Iteration 747, loss = 0.22027845\n",
            "Iteration 748, loss = 0.22011537\n",
            "Iteration 749, loss = 0.21995188\n",
            "Iteration 750, loss = 0.21978932\n",
            "Iteration 751, loss = 0.21962635\n",
            "Iteration 752, loss = 0.21946389\n",
            "Iteration 753, loss = 0.21930163\n",
            "Iteration 754, loss = 0.21913947\n",
            "Iteration 755, loss = 0.21897817\n",
            "Iteration 756, loss = 0.21881608\n",
            "Iteration 757, loss = 0.21865498\n",
            "Iteration 758, loss = 0.21849355\n",
            "Iteration 759, loss = 0.21833279\n",
            "Iteration 760, loss = 0.21817209\n",
            "Iteration 761, loss = 0.21801125\n",
            "Iteration 762, loss = 0.21785140\n",
            "Iteration 763, loss = 0.21769099\n",
            "Iteration 764, loss = 0.21753187\n",
            "Iteration 765, loss = 0.21737219\n",
            "Iteration 766, loss = 0.21721250\n",
            "Iteration 767, loss = 0.21705425\n",
            "Iteration 768, loss = 0.21689491\n",
            "Iteration 769, loss = 0.21673688\n",
            "Iteration 770, loss = 0.21657834\n",
            "Iteration 771, loss = 0.21641984\n",
            "Iteration 772, loss = 0.21626212\n",
            "Iteration 773, loss = 0.21610431\n",
            "Iteration 774, loss = 0.21594729\n",
            "Iteration 775, loss = 0.21578970\n",
            "Iteration 776, loss = 0.21563296\n",
            "Iteration 777, loss = 0.21547632\n",
            "Iteration 778, loss = 0.21532018\n",
            "Iteration 779, loss = 0.21516454\n",
            "Iteration 780, loss = 0.21500845\n",
            "Iteration 781, loss = 0.21485224\n",
            "Iteration 782, loss = 0.21469655\n",
            "Iteration 783, loss = 0.21454054\n",
            "Iteration 784, loss = 0.21438519\n",
            "Iteration 785, loss = 0.21422951\n",
            "Iteration 786, loss = 0.21407459\n",
            "Iteration 787, loss = 0.21391925\n",
            "Iteration 788, loss = 0.21376411\n",
            "Iteration 789, loss = 0.21360980\n",
            "Iteration 790, loss = 0.21345517\n",
            "Iteration 791, loss = 0.21330137\n",
            "Iteration 792, loss = 0.21314828\n",
            "Iteration 793, loss = 0.21299477\n",
            "Iteration 794, loss = 0.21284298\n",
            "Iteration 795, loss = 0.21268987\n",
            "Iteration 796, loss = 0.21253855\n",
            "Iteration 797, loss = 0.21238630\n",
            "Iteration 798, loss = 0.21223437\n",
            "Iteration 799, loss = 0.21208342\n",
            "Iteration 800, loss = 0.21193125\n",
            "Iteration 1, loss = 0.70442430\n",
            "Iteration 2, loss = 0.70378864\n",
            "Iteration 3, loss = 0.70288780\n",
            "Iteration 4, loss = 0.70183623\n",
            "Iteration 5, loss = 0.70069786\n",
            "Iteration 6, loss = 0.69942745\n",
            "Iteration 7, loss = 0.69817844\n",
            "Iteration 8, loss = 0.69692293\n",
            "Iteration 9, loss = 0.69586668\n",
            "Iteration 10, loss = 0.69497037\n",
            "Iteration 11, loss = 0.69406993\n",
            "Iteration 12, loss = 0.69329302\n",
            "Iteration 13, loss = 0.69266815\n",
            "Iteration 14, loss = 0.69211108\n",
            "Iteration 15, loss = 0.69167183\n",
            "Iteration 16, loss = 0.69134039\n",
            "Iteration 17, loss = 0.69107477\n",
            "Iteration 18, loss = 0.69082387\n",
            "Iteration 19, loss = 0.69066307\n",
            "Iteration 20, loss = 0.69051564\n",
            "Iteration 21, loss = 0.69036549\n",
            "Iteration 22, loss = 0.69019920\n",
            "Iteration 23, loss = 0.69003161\n",
            "Iteration 24, loss = 0.68988371\n",
            "Iteration 25, loss = 0.68978292\n",
            "Iteration 26, loss = 0.68969264\n",
            "Iteration 27, loss = 0.68960366\n",
            "Iteration 28, loss = 0.68951582\n",
            "Iteration 29, loss = 0.68942880\n",
            "Iteration 30, loss = 0.68934250\n",
            "Iteration 31, loss = 0.68925681\n",
            "Iteration 32, loss = 0.68918112\n",
            "Iteration 33, loss = 0.68910413\n",
            "Iteration 34, loss = 0.68902620\n",
            "Iteration 35, loss = 0.68894732\n",
            "Iteration 36, loss = 0.68886750\n",
            "Iteration 37, loss = 0.68878678\n",
            "Iteration 38, loss = 0.68870518\n",
            "Iteration 39, loss = 0.68862273\n",
            "Iteration 40, loss = 0.68853947\n",
            "Iteration 41, loss = 0.68845541\n",
            "Iteration 42, loss = 0.68837056\n",
            "Iteration 43, loss = 0.68828494\n",
            "Iteration 44, loss = 0.68819838\n",
            "Iteration 45, loss = 0.68811099\n",
            "Iteration 46, loss = 0.68802285\n",
            "Iteration 47, loss = 0.68793392\n",
            "Iteration 48, loss = 0.68784422\n",
            "Iteration 49, loss = 0.68775381\n",
            "Iteration 50, loss = 0.68766268\n",
            "Iteration 51, loss = 0.68757183\n",
            "Iteration 52, loss = 0.68748583\n",
            "Iteration 53, loss = 0.68739872\n",
            "Iteration 54, loss = 0.68731054\n",
            "Iteration 55, loss = 0.68722135\n",
            "Iteration 56, loss = 0.68713123\n",
            "Iteration 57, loss = 0.68704748\n",
            "Iteration 58, loss = 0.68696234\n",
            "Iteration 59, loss = 0.68687590\n",
            "Iteration 60, loss = 0.68678986\n",
            "Iteration 61, loss = 0.68670318\n",
            "Iteration 62, loss = 0.68661588\n",
            "Iteration 63, loss = 0.68652794\n",
            "Iteration 64, loss = 0.68643931\n",
            "Iteration 65, loss = 0.68634998\n",
            "Iteration 66, loss = 0.68625994\n",
            "Iteration 67, loss = 0.68616916\n",
            "Iteration 68, loss = 0.68607761\n",
            "Iteration 69, loss = 0.68598527\n",
            "Iteration 70, loss = 0.68589212\n",
            "Iteration 71, loss = 0.68579817\n",
            "Iteration 72, loss = 0.68570360\n",
            "Iteration 73, loss = 0.68560805\n",
            "Iteration 74, loss = 0.68551162\n",
            "Iteration 75, loss = 0.68541434\n",
            "Iteration 76, loss = 0.68531613\n",
            "Iteration 77, loss = 0.68521697\n",
            "Iteration 78, loss = 0.68511708\n",
            "Iteration 79, loss = 0.68502047\n",
            "Iteration 80, loss = 0.68492283\n",
            "Iteration 81, loss = 0.68482440\n",
            "Iteration 82, loss = 0.68472532\n",
            "Iteration 83, loss = 0.68462501\n",
            "Iteration 84, loss = 0.68452445\n",
            "Iteration 85, loss = 0.68442224\n",
            "Iteration 86, loss = 0.68431956\n",
            "Iteration 87, loss = 0.68421492\n",
            "Iteration 88, loss = 0.68410984\n",
            "Iteration 89, loss = 0.68400321\n",
            "Iteration 90, loss = 0.68389536\n",
            "Iteration 91, loss = 0.68378694\n",
            "Iteration 92, loss = 0.68367637\n",
            "Iteration 93, loss = 0.68356559\n",
            "Iteration 94, loss = 0.68345291\n",
            "Iteration 95, loss = 0.68333885\n",
            "Iteration 96, loss = 0.68322403\n",
            "Iteration 97, loss = 0.68310559\n",
            "Iteration 98, loss = 0.68298060\n",
            "Iteration 99, loss = 0.68284998\n",
            "Iteration 100, loss = 0.68271681\n",
            "Iteration 101, loss = 0.68258182\n",
            "Iteration 102, loss = 0.68244413\n",
            "Iteration 103, loss = 0.68230447\n",
            "Iteration 104, loss = 0.68216273\n",
            "Iteration 105, loss = 0.68201874\n",
            "Iteration 106, loss = 0.68187344\n",
            "Iteration 107, loss = 0.68172520\n",
            "Iteration 108, loss = 0.68157608\n",
            "Iteration 109, loss = 0.68142458\n",
            "Iteration 110, loss = 0.68127109\n",
            "Iteration 111, loss = 0.68111563\n",
            "Iteration 112, loss = 0.68095904\n",
            "Iteration 113, loss = 0.68080017\n",
            "Iteration 114, loss = 0.68064005\n",
            "Iteration 115, loss = 0.68047764\n",
            "Iteration 116, loss = 0.68031397\n",
            "Iteration 117, loss = 0.68014833\n",
            "Iteration 118, loss = 0.67998082\n",
            "Iteration 119, loss = 0.67981200\n",
            "Iteration 120, loss = 0.67964096\n",
            "Iteration 121, loss = 0.67946799\n",
            "Iteration 122, loss = 0.67929357\n",
            "Iteration 123, loss = 0.67911696\n",
            "Iteration 124, loss = 0.67893344\n",
            "Iteration 125, loss = 0.67874635\n",
            "Iteration 126, loss = 0.67855394\n",
            "Iteration 127, loss = 0.67835249\n",
            "Iteration 128, loss = 0.67814791\n",
            "Iteration 129, loss = 0.67794007\n",
            "Iteration 130, loss = 0.67772912\n",
            "Iteration 131, loss = 0.67751525\n",
            "Iteration 132, loss = 0.67729898\n",
            "Iteration 133, loss = 0.67708032\n",
            "Iteration 134, loss = 0.67685670\n",
            "Iteration 135, loss = 0.67661852\n",
            "Iteration 136, loss = 0.67636977\n",
            "Iteration 137, loss = 0.67610819\n",
            "Iteration 138, loss = 0.67584101\n",
            "Iteration 139, loss = 0.67555708\n",
            "Iteration 140, loss = 0.67526327\n",
            "Iteration 141, loss = 0.67496342\n",
            "Iteration 142, loss = 0.67465752\n",
            "Iteration 143, loss = 0.67434596\n",
            "Iteration 144, loss = 0.67402906\n",
            "Iteration 145, loss = 0.67371972\n",
            "Iteration 146, loss = 0.67341000\n",
            "Iteration 147, loss = 0.67309310\n",
            "Iteration 148, loss = 0.67277073\n",
            "Iteration 149, loss = 0.67243322\n",
            "Iteration 150, loss = 0.67207677\n",
            "Iteration 151, loss = 0.67172701\n",
            "Iteration 152, loss = 0.67136638\n",
            "Iteration 153, loss = 0.67101105\n",
            "Iteration 154, loss = 0.67065834\n",
            "Iteration 155, loss = 0.67030245\n",
            "Iteration 156, loss = 0.66994351\n",
            "Iteration 157, loss = 0.66957035\n",
            "Iteration 158, loss = 0.66917838\n",
            "Iteration 159, loss = 0.66876911\n",
            "Iteration 160, loss = 0.66836098\n",
            "Iteration 161, loss = 0.66794220\n",
            "Iteration 162, loss = 0.66748263\n",
            "Iteration 163, loss = 0.66701177\n",
            "Iteration 164, loss = 0.66651467\n",
            "Iteration 165, loss = 0.66596291\n",
            "Iteration 166, loss = 0.66539619\n",
            "Iteration 167, loss = 0.66476412\n",
            "Iteration 168, loss = 0.66409221\n",
            "Iteration 169, loss = 0.66340275\n",
            "Iteration 170, loss = 0.66271565\n",
            "Iteration 171, loss = 0.66201542\n",
            "Iteration 172, loss = 0.66130590\n",
            "Iteration 173, loss = 0.66060072\n",
            "Iteration 174, loss = 0.65989895\n",
            "Iteration 175, loss = 0.65918287\n",
            "Iteration 176, loss = 0.65843948\n",
            "Iteration 177, loss = 0.65767952\n",
            "Iteration 178, loss = 0.65692701\n",
            "Iteration 179, loss = 0.65617059\n",
            "Iteration 180, loss = 0.65541604\n",
            "Iteration 181, loss = 0.65465482\n",
            "Iteration 182, loss = 0.65384956\n",
            "Iteration 183, loss = 0.65299826\n",
            "Iteration 184, loss = 0.65213799\n",
            "Iteration 185, loss = 0.65126631\n",
            "Iteration 186, loss = 0.65034659\n",
            "Iteration 187, loss = 0.64935026\n",
            "Iteration 188, loss = 0.64832229\n",
            "Iteration 189, loss = 0.64727168\n",
            "Iteration 190, loss = 0.64620660\n",
            "Iteration 191, loss = 0.64512870\n",
            "Iteration 192, loss = 0.64403990\n",
            "Iteration 193, loss = 0.64294552\n",
            "Iteration 194, loss = 0.64185270\n",
            "Iteration 195, loss = 0.64076036\n",
            "Iteration 196, loss = 0.63962693\n",
            "Iteration 197, loss = 0.63845410\n",
            "Iteration 198, loss = 0.63723319\n",
            "Iteration 199, loss = 0.63596950\n",
            "Iteration 200, loss = 0.63469788\n",
            "Iteration 201, loss = 0.63339946\n",
            "Iteration 202, loss = 0.63206730\n",
            "Iteration 203, loss = 0.63072123\n",
            "Iteration 204, loss = 0.62936299\n",
            "Iteration 205, loss = 0.62797905\n",
            "Iteration 206, loss = 0.62658374\n",
            "Iteration 207, loss = 0.62518317\n",
            "Iteration 208, loss = 0.62377811\n",
            "Iteration 209, loss = 0.62236383\n",
            "Iteration 210, loss = 0.62091255\n",
            "Iteration 211, loss = 0.61944949\n",
            "Iteration 212, loss = 0.61798212\n",
            "Iteration 213, loss = 0.61651152\n",
            "Iteration 214, loss = 0.61501189\n",
            "Iteration 215, loss = 0.61348737\n",
            "Iteration 216, loss = 0.61198093\n",
            "Iteration 217, loss = 0.61049870\n",
            "Iteration 218, loss = 0.60901625\n",
            "Iteration 219, loss = 0.60753721\n",
            "Iteration 220, loss = 0.60605856\n",
            "Iteration 221, loss = 0.60458006\n",
            "Iteration 222, loss = 0.60310395\n",
            "Iteration 223, loss = 0.60163041\n",
            "Iteration 224, loss = 0.60016119\n",
            "Iteration 225, loss = 0.59869443\n",
            "Iteration 226, loss = 0.59722738\n",
            "Iteration 227, loss = 0.59570719\n",
            "Iteration 228, loss = 0.59418719\n",
            "Iteration 229, loss = 0.59266765\n",
            "Iteration 230, loss = 0.59115047\n",
            "Iteration 231, loss = 0.58963637\n",
            "Iteration 232, loss = 0.58813950\n",
            "Iteration 233, loss = 0.58665199\n",
            "Iteration 234, loss = 0.58516842\n",
            "Iteration 235, loss = 0.58369046\n",
            "Iteration 236, loss = 0.58221536\n",
            "Iteration 237, loss = 0.58074417\n",
            "Iteration 238, loss = 0.57925725\n",
            "Iteration 239, loss = 0.57771807\n",
            "Iteration 240, loss = 0.57617657\n",
            "Iteration 241, loss = 0.57463267\n",
            "Iteration 242, loss = 0.57308964\n",
            "Iteration 243, loss = 0.57154845\n",
            "Iteration 244, loss = 0.56997019\n",
            "Iteration 245, loss = 0.56838502\n",
            "Iteration 246, loss = 0.56679740\n",
            "Iteration 247, loss = 0.56520819\n",
            "Iteration 248, loss = 0.56361872\n",
            "Iteration 249, loss = 0.56203161\n",
            "Iteration 250, loss = 0.56044549\n",
            "Iteration 251, loss = 0.55880606\n",
            "Iteration 252, loss = 0.55705768\n",
            "Iteration 253, loss = 0.55529305\n",
            "Iteration 254, loss = 0.55351332\n",
            "Iteration 255, loss = 0.55172373\n",
            "Iteration 256, loss = 0.54992751\n",
            "Iteration 257, loss = 0.54812620\n",
            "Iteration 258, loss = 0.54632241\n",
            "Iteration 259, loss = 0.54451999\n",
            "Iteration 260, loss = 0.54271768\n",
            "Iteration 261, loss = 0.54092967\n",
            "Iteration 262, loss = 0.53917775\n",
            "Iteration 263, loss = 0.53743697\n",
            "Iteration 264, loss = 0.53570353\n",
            "Iteration 265, loss = 0.53397785\n",
            "Iteration 266, loss = 0.53226300\n",
            "Iteration 267, loss = 0.53055624\n",
            "Iteration 268, loss = 0.52885939\n",
            "Iteration 269, loss = 0.52717272\n",
            "Iteration 270, loss = 0.52542232\n",
            "Iteration 271, loss = 0.52366613\n",
            "Iteration 272, loss = 0.52192784\n",
            "Iteration 273, loss = 0.52019607\n",
            "Iteration 274, loss = 0.51847254\n",
            "Iteration 275, loss = 0.51675720\n",
            "Iteration 276, loss = 0.51504988\n",
            "Iteration 277, loss = 0.51335071\n",
            "Iteration 278, loss = 0.51167025\n",
            "Iteration 279, loss = 0.50999892\n",
            "Iteration 280, loss = 0.50829496\n",
            "Iteration 281, loss = 0.50653160\n",
            "Iteration 282, loss = 0.50476979\n",
            "Iteration 283, loss = 0.50301025\n",
            "Iteration 284, loss = 0.50125412\n",
            "Iteration 285, loss = 0.49950068\n",
            "Iteration 286, loss = 0.49774914\n",
            "Iteration 287, loss = 0.49600397\n",
            "Iteration 288, loss = 0.49426617\n",
            "Iteration 289, loss = 0.49253472\n",
            "Iteration 290, loss = 0.49081143\n",
            "Iteration 291, loss = 0.48909539\n",
            "Iteration 292, loss = 0.48738677\n",
            "Iteration 293, loss = 0.48568704\n",
            "Iteration 294, loss = 0.48399507\n",
            "Iteration 295, loss = 0.48231722\n",
            "Iteration 296, loss = 0.48066286\n",
            "Iteration 297, loss = 0.47901774\n",
            "Iteration 298, loss = 0.47737937\n",
            "Iteration 299, loss = 0.47574898\n",
            "Iteration 300, loss = 0.47412854\n",
            "Iteration 301, loss = 0.47251591\n",
            "Iteration 302, loss = 0.47091116\n",
            "Iteration 303, loss = 0.46931494\n",
            "Iteration 304, loss = 0.46772633\n",
            "Iteration 305, loss = 0.46614774\n",
            "Iteration 306, loss = 0.46457785\n",
            "Iteration 307, loss = 0.46301731\n",
            "Iteration 308, loss = 0.46146467\n",
            "Iteration 309, loss = 0.45992254\n",
            "Iteration 310, loss = 0.45839112\n",
            "Iteration 311, loss = 0.45686943\n",
            "Iteration 312, loss = 0.45535827\n",
            "Iteration 313, loss = 0.45386332\n",
            "Iteration 314, loss = 0.45237826\n",
            "Iteration 315, loss = 0.45090097\n",
            "Iteration 316, loss = 0.44943350\n",
            "Iteration 317, loss = 0.44797585\n",
            "Iteration 318, loss = 0.44652696\n",
            "Iteration 319, loss = 0.44508864\n",
            "Iteration 320, loss = 0.44366029\n",
            "Iteration 321, loss = 0.44223773\n",
            "Iteration 322, loss = 0.44082384\n",
            "Iteration 323, loss = 0.43941934\n",
            "Iteration 324, loss = 0.43802297\n",
            "Iteration 325, loss = 0.43663572\n",
            "Iteration 326, loss = 0.43525660\n",
            "Iteration 327, loss = 0.43389004\n",
            "Iteration 328, loss = 0.43253809\n",
            "Iteration 329, loss = 0.43119378\n",
            "Iteration 330, loss = 0.42985768\n",
            "Iteration 331, loss = 0.42852825\n",
            "Iteration 332, loss = 0.42721294\n",
            "Iteration 333, loss = 0.42590653\n",
            "Iteration 334, loss = 0.42460918\n",
            "Iteration 335, loss = 0.42332252\n",
            "Iteration 336, loss = 0.42204390\n",
            "Iteration 337, loss = 0.42077361\n",
            "Iteration 338, loss = 0.41951205\n",
            "Iteration 339, loss = 0.41825907\n",
            "Iteration 340, loss = 0.41701529\n",
            "Iteration 341, loss = 0.41578045\n",
            "Iteration 342, loss = 0.41455395\n",
            "Iteration 343, loss = 0.41333588\n",
            "Iteration 344, loss = 0.41212475\n",
            "Iteration 345, loss = 0.41092223\n",
            "Iteration 346, loss = 0.40972844\n",
            "Iteration 347, loss = 0.40854493\n",
            "Iteration 348, loss = 0.40736753\n",
            "Iteration 349, loss = 0.40619828\n",
            "Iteration 350, loss = 0.40503763\n",
            "Iteration 351, loss = 0.40388479\n",
            "Iteration 352, loss = 0.40273976\n",
            "Iteration 353, loss = 0.40160266\n",
            "Iteration 354, loss = 0.40047371\n",
            "Iteration 355, loss = 0.39935214\n",
            "Iteration 356, loss = 0.39823627\n",
            "Iteration 357, loss = 0.39712697\n",
            "Iteration 358, loss = 0.39602359\n",
            "Iteration 359, loss = 0.39492734\n",
            "Iteration 360, loss = 0.39383795\n",
            "Iteration 361, loss = 0.39275589\n",
            "Iteration 362, loss = 0.39168107\n",
            "Iteration 363, loss = 0.39061377\n",
            "Iteration 364, loss = 0.38955421\n",
            "Iteration 365, loss = 0.38850159\n",
            "Iteration 366, loss = 0.38745792\n",
            "Iteration 367, loss = 0.38642193\n",
            "Iteration 368, loss = 0.38539410\n",
            "Iteration 369, loss = 0.38437324\n",
            "Iteration 370, loss = 0.38335958\n",
            "Iteration 371, loss = 0.38235374\n",
            "Iteration 372, loss = 0.38135489\n",
            "Iteration 373, loss = 0.38036322\n",
            "Iteration 374, loss = 0.37937808\n",
            "Iteration 375, loss = 0.37839972\n",
            "Iteration 376, loss = 0.37742915\n",
            "Iteration 377, loss = 0.37646573\n",
            "Iteration 378, loss = 0.37550803\n",
            "Iteration 379, loss = 0.37455623\n",
            "Iteration 380, loss = 0.37361032\n",
            "Iteration 381, loss = 0.37267006\n",
            "Iteration 382, loss = 0.37173522\n",
            "Iteration 383, loss = 0.37080605\n",
            "Iteration 384, loss = 0.36988232\n",
            "Iteration 385, loss = 0.36896469\n",
            "Iteration 386, loss = 0.36805247\n",
            "Iteration 387, loss = 0.36714530\n",
            "Iteration 388, loss = 0.36624517\n",
            "Iteration 389, loss = 0.36534939\n",
            "Iteration 390, loss = 0.36445815\n",
            "Iteration 391, loss = 0.36357186\n",
            "Iteration 392, loss = 0.36269089\n",
            "Iteration 393, loss = 0.36181508\n",
            "Iteration 394, loss = 0.36094378\n",
            "Iteration 395, loss = 0.36008121\n",
            "Iteration 396, loss = 0.35922443\n",
            "Iteration 397, loss = 0.35837262\n",
            "Iteration 398, loss = 0.35752534\n",
            "Iteration 399, loss = 0.35668229\n",
            "Iteration 400, loss = 0.35584293\n",
            "Iteration 401, loss = 0.35500772\n",
            "Iteration 402, loss = 0.35417779\n",
            "Iteration 403, loss = 0.35335249\n",
            "Iteration 404, loss = 0.35253328\n",
            "Iteration 405, loss = 0.35171793\n",
            "Iteration 406, loss = 0.35090613\n",
            "Iteration 407, loss = 0.35009840\n",
            "Iteration 408, loss = 0.34929477\n",
            "Iteration 409, loss = 0.34849480\n",
            "Iteration 410, loss = 0.34769844\n",
            "Iteration 411, loss = 0.34690529\n",
            "Iteration 412, loss = 0.34611568\n",
            "Iteration 413, loss = 0.34532962\n",
            "Iteration 414, loss = 0.34454763\n",
            "Iteration 415, loss = 0.34377099\n",
            "Iteration 416, loss = 0.34299775\n",
            "Iteration 417, loss = 0.34222634\n",
            "Iteration 418, loss = 0.34145754\n",
            "Iteration 419, loss = 0.34069100\n",
            "Iteration 420, loss = 0.33992805\n",
            "Iteration 421, loss = 0.33916813\n",
            "Iteration 422, loss = 0.33841111\n",
            "Iteration 423, loss = 0.33765566\n",
            "Iteration 424, loss = 0.33690461\n",
            "Iteration 425, loss = 0.33615839\n",
            "Iteration 426, loss = 0.33541435\n",
            "Iteration 427, loss = 0.33467378\n",
            "Iteration 428, loss = 0.33393465\n",
            "Iteration 429, loss = 0.33319713\n",
            "Iteration 430, loss = 0.33246313\n",
            "Iteration 431, loss = 0.33173192\n",
            "Iteration 432, loss = 0.33100416\n",
            "Iteration 433, loss = 0.33027947\n",
            "Iteration 434, loss = 0.32955710\n",
            "Iteration 435, loss = 0.32883658\n",
            "Iteration 436, loss = 0.32811873\n",
            "Iteration 437, loss = 0.32740315\n",
            "Iteration 438, loss = 0.32669007\n",
            "Iteration 439, loss = 0.32597935\n",
            "Iteration 440, loss = 0.32527149\n",
            "Iteration 441, loss = 0.32456608\n",
            "Iteration 442, loss = 0.32386349\n",
            "Iteration 443, loss = 0.32316395\n",
            "Iteration 444, loss = 0.32246605\n",
            "Iteration 445, loss = 0.32177044\n",
            "Iteration 446, loss = 0.32107706\n",
            "Iteration 447, loss = 0.32038590\n",
            "Iteration 448, loss = 0.31969641\n",
            "Iteration 449, loss = 0.31900908\n",
            "Iteration 450, loss = 0.31832404\n",
            "Iteration 451, loss = 0.31764070\n",
            "Iteration 452, loss = 0.31695933\n",
            "Iteration 453, loss = 0.31616378\n",
            "Iteration 454, loss = 0.31503113\n",
            "Iteration 455, loss = 0.31380542\n",
            "Iteration 456, loss = 0.31248825\n",
            "Iteration 457, loss = 0.31109724\n",
            "Iteration 458, loss = 0.30965566\n",
            "Iteration 459, loss = 0.30817762\n",
            "Iteration 460, loss = 0.30667510\n",
            "Iteration 461, loss = 0.30516388\n",
            "Iteration 462, loss = 0.30365807\n",
            "Iteration 463, loss = 0.30216918\n",
            "Iteration 464, loss = 0.30070860\n",
            "Iteration 465, loss = 0.29928462\n",
            "Iteration 466, loss = 0.29791489\n",
            "Iteration 467, loss = 0.29660403\n",
            "Iteration 468, loss = 0.29535293\n",
            "Iteration 469, loss = 0.29416605\n",
            "Iteration 470, loss = 0.29303474\n",
            "Iteration 471, loss = 0.29195873\n",
            "Iteration 472, loss = 0.29093090\n",
            "Iteration 473, loss = 0.28994878\n",
            "Iteration 474, loss = 0.28903100\n",
            "Iteration 475, loss = 0.28816424\n",
            "Iteration 476, loss = 0.28732647\n",
            "Iteration 477, loss = 0.28651536\n",
            "Iteration 478, loss = 0.28572825\n",
            "Iteration 479, loss = 0.28496250\n",
            "Iteration 480, loss = 0.28421675\n",
            "Iteration 481, loss = 0.28349037\n",
            "Iteration 482, loss = 0.28278078\n",
            "Iteration 483, loss = 0.28208598\n",
            "Iteration 484, loss = 0.28140614\n",
            "Iteration 485, loss = 0.28073919\n",
            "Iteration 486, loss = 0.28008466\n",
            "Iteration 487, loss = 0.27944210\n",
            "Iteration 488, loss = 0.27881074\n",
            "Iteration 489, loss = 0.27819052\n",
            "Iteration 490, loss = 0.27758031\n",
            "Iteration 491, loss = 0.27697965\n",
            "Iteration 492, loss = 0.27638959\n",
            "Iteration 493, loss = 0.27580905\n",
            "Iteration 494, loss = 0.27523833\n",
            "Iteration 495, loss = 0.27468015\n",
            "Iteration 496, loss = 0.27412998\n",
            "Iteration 497, loss = 0.27358753\n",
            "Iteration 498, loss = 0.27305300\n",
            "Iteration 499, loss = 0.27252564\n",
            "Iteration 500, loss = 0.27200484\n",
            "Iteration 501, loss = 0.27149021\n",
            "Iteration 502, loss = 0.27098141\n",
            "Iteration 503, loss = 0.27047858\n",
            "Iteration 504, loss = 0.26998143\n",
            "Iteration 505, loss = 0.26948935\n",
            "Iteration 506, loss = 0.26900204\n",
            "Iteration 507, loss = 0.26851935\n",
            "Iteration 508, loss = 0.26804104\n",
            "Iteration 509, loss = 0.26756707\n",
            "Iteration 510, loss = 0.26709679\n",
            "Iteration 511, loss = 0.26663016\n",
            "Iteration 512, loss = 0.26616727\n",
            "Iteration 513, loss = 0.26570877\n",
            "Iteration 514, loss = 0.26525150\n",
            "Iteration 515, loss = 0.26479804\n",
            "Iteration 516, loss = 0.26434736\n",
            "Iteration 517, loss = 0.26390160\n",
            "Iteration 518, loss = 0.26345906\n",
            "Iteration 519, loss = 0.26301943\n",
            "Iteration 520, loss = 0.26258268\n",
            "Iteration 521, loss = 0.26214855\n",
            "Iteration 522, loss = 0.26171749\n",
            "Iteration 523, loss = 0.26128941\n",
            "Iteration 524, loss = 0.26086397\n",
            "Iteration 525, loss = 0.26044137\n",
            "Iteration 526, loss = 0.26002120\n",
            "Iteration 527, loss = 0.25960334\n",
            "Iteration 528, loss = 0.25918776\n",
            "Iteration 529, loss = 0.25877472\n",
            "Iteration 530, loss = 0.25836393\n",
            "Iteration 531, loss = 0.25795439\n",
            "Iteration 532, loss = 0.25754706\n",
            "Iteration 533, loss = 0.25714371\n",
            "Iteration 534, loss = 0.25674367\n",
            "Iteration 535, loss = 0.25634606\n",
            "Iteration 536, loss = 0.25595082\n",
            "Iteration 537, loss = 0.25555814\n",
            "Iteration 538, loss = 0.25516779\n",
            "Iteration 539, loss = 0.25477903\n",
            "Iteration 540, loss = 0.25439236\n",
            "Iteration 541, loss = 0.25400764\n",
            "Iteration 542, loss = 0.25362511\n",
            "Iteration 543, loss = 0.25324448\n",
            "Iteration 544, loss = 0.25286579\n",
            "Iteration 545, loss = 0.25248899\n",
            "Iteration 546, loss = 0.25211397\n",
            "Iteration 547, loss = 0.25174058\n",
            "Iteration 548, loss = 0.25136904\n",
            "Iteration 549, loss = 0.25099966\n",
            "Iteration 550, loss = 0.25063336\n",
            "Iteration 551, loss = 0.25026914\n",
            "Iteration 552, loss = 0.24990709\n",
            "Iteration 553, loss = 0.24954663\n",
            "Iteration 554, loss = 0.24918784\n",
            "Iteration 555, loss = 0.24883103\n",
            "Iteration 556, loss = 0.24847604\n",
            "Iteration 557, loss = 0.24812316\n",
            "Iteration 558, loss = 0.24777239\n",
            "Iteration 559, loss = 0.24742384\n",
            "Iteration 560, loss = 0.24707745\n",
            "Iteration 561, loss = 0.24673257\n",
            "Iteration 562, loss = 0.24638900\n",
            "Iteration 563, loss = 0.24604725\n",
            "Iteration 564, loss = 0.24570732\n",
            "Iteration 565, loss = 0.24536962\n",
            "Iteration 566, loss = 0.24503401\n",
            "Iteration 567, loss = 0.24470028\n",
            "Iteration 568, loss = 0.24436787\n",
            "Iteration 569, loss = 0.24403714\n",
            "Iteration 570, loss = 0.24370742\n",
            "Iteration 571, loss = 0.24337928\n",
            "Iteration 572, loss = 0.24305258\n",
            "Iteration 573, loss = 0.24272727\n",
            "Iteration 574, loss = 0.24240343\n",
            "Iteration 575, loss = 0.24208107\n",
            "Iteration 576, loss = 0.24176037\n",
            "Iteration 577, loss = 0.24144088\n",
            "Iteration 578, loss = 0.24112294\n",
            "Iteration 579, loss = 0.24080681\n",
            "Iteration 580, loss = 0.24049146\n",
            "Iteration 581, loss = 0.24017783\n",
            "Iteration 582, loss = 0.23986550\n",
            "Iteration 583, loss = 0.23955463\n",
            "Iteration 584, loss = 0.23924515\n",
            "Iteration 585, loss = 0.23893698\n",
            "Iteration 586, loss = 0.23863007\n",
            "Iteration 587, loss = 0.23832559\n",
            "Iteration 588, loss = 0.23802314\n",
            "Iteration 589, loss = 0.23772210\n",
            "Iteration 590, loss = 0.23742260\n",
            "Iteration 591, loss = 0.23712425\n",
            "Iteration 592, loss = 0.23682746\n",
            "Iteration 593, loss = 0.23653187\n",
            "Iteration 594, loss = 0.23623753\n",
            "Iteration 595, loss = 0.23594504\n",
            "Iteration 596, loss = 0.23565512\n",
            "Iteration 597, loss = 0.23536629\n",
            "Iteration 598, loss = 0.23507867\n",
            "Iteration 599, loss = 0.23479232\n",
            "Iteration 600, loss = 0.23450719\n",
            "Iteration 601, loss = 0.23422282\n",
            "Iteration 602, loss = 0.23393927\n",
            "Iteration 603, loss = 0.23365689\n",
            "Iteration 604, loss = 0.23337552\n",
            "Iteration 605, loss = 0.23309520\n",
            "Iteration 606, loss = 0.23281616\n",
            "Iteration 607, loss = 0.23253836\n",
            "Iteration 608, loss = 0.23226157\n",
            "Iteration 609, loss = 0.23198590\n",
            "Iteration 610, loss = 0.23171160\n",
            "Iteration 611, loss = 0.23143937\n",
            "Iteration 612, loss = 0.23116814\n",
            "Iteration 613, loss = 0.23089807\n",
            "Iteration 614, loss = 0.23062955\n",
            "Iteration 615, loss = 0.23036218\n",
            "Iteration 616, loss = 0.23009537\n",
            "Iteration 617, loss = 0.22982926\n",
            "Iteration 618, loss = 0.22956442\n",
            "Iteration 619, loss = 0.22930412\n",
            "Iteration 620, loss = 0.22904501\n",
            "Iteration 621, loss = 0.22878690\n",
            "Iteration 622, loss = 0.22853006\n",
            "Iteration 623, loss = 0.22827437\n",
            "Iteration 624, loss = 0.22802124\n",
            "Iteration 625, loss = 0.22776933\n",
            "Iteration 626, loss = 0.22751891\n",
            "Iteration 627, loss = 0.22726955\n",
            "Iteration 628, loss = 0.22702113\n",
            "Iteration 629, loss = 0.22677374\n",
            "Iteration 630, loss = 0.22652751\n",
            "Iteration 631, loss = 0.22628198\n",
            "Iteration 632, loss = 0.22603747\n",
            "Iteration 633, loss = 0.22579405\n",
            "Iteration 634, loss = 0.22555140\n",
            "Iteration 635, loss = 0.22530952\n",
            "Iteration 636, loss = 0.22506878\n",
            "Iteration 637, loss = 0.22482857\n",
            "Iteration 638, loss = 0.22458977\n",
            "Iteration 639, loss = 0.22435179\n",
            "Iteration 640, loss = 0.22411512\n",
            "Iteration 641, loss = 0.22387971\n",
            "Iteration 642, loss = 0.22364521\n",
            "Iteration 643, loss = 0.22341167\n",
            "Iteration 644, loss = 0.22317920\n",
            "Iteration 645, loss = 0.22294755\n",
            "Iteration 646, loss = 0.22271737\n",
            "Iteration 647, loss = 0.22248818\n",
            "Iteration 648, loss = 0.22225968\n",
            "Iteration 649, loss = 0.22203230\n",
            "Iteration 650, loss = 0.22180532\n",
            "Iteration 651, loss = 0.22157947\n",
            "Iteration 652, loss = 0.22135407\n",
            "Iteration 653, loss = 0.22112940\n",
            "Iteration 654, loss = 0.22090569\n",
            "Iteration 655, loss = 0.22068235\n",
            "Iteration 656, loss = 0.22046030\n",
            "Iteration 657, loss = 0.22023976\n",
            "Iteration 658, loss = 0.22002005\n",
            "Iteration 659, loss = 0.21980194\n",
            "Iteration 660, loss = 0.21958440\n",
            "Iteration 661, loss = 0.21936788\n",
            "Iteration 662, loss = 0.21915187\n",
            "Iteration 663, loss = 0.21893666\n",
            "Iteration 664, loss = 0.21872270\n",
            "Iteration 665, loss = 0.21850920\n",
            "Iteration 666, loss = 0.21829679\n",
            "Iteration 667, loss = 0.21808454\n",
            "Iteration 668, loss = 0.21787316\n",
            "Iteration 669, loss = 0.21766164\n",
            "Iteration 670, loss = 0.21744986\n",
            "Iteration 671, loss = 0.21723836\n",
            "Iteration 672, loss = 0.21702745\n",
            "Iteration 673, loss = 0.21681693\n",
            "Iteration 674, loss = 0.21660751\n",
            "Iteration 675, loss = 0.21639839\n",
            "Iteration 676, loss = 0.21619001\n",
            "Iteration 677, loss = 0.21598240\n",
            "Iteration 678, loss = 0.21577558\n",
            "Iteration 679, loss = 0.21556988\n",
            "Iteration 680, loss = 0.21536428\n",
            "Iteration 681, loss = 0.21515939\n",
            "Iteration 682, loss = 0.21495463\n",
            "Iteration 683, loss = 0.21475142\n",
            "Iteration 684, loss = 0.21454842\n",
            "Iteration 685, loss = 0.21434626\n",
            "Iteration 686, loss = 0.21414542\n",
            "Iteration 687, loss = 0.21394461\n",
            "Iteration 688, loss = 0.21374481\n",
            "Iteration 689, loss = 0.21354578\n",
            "Iteration 690, loss = 0.21334742\n",
            "Iteration 691, loss = 0.21314898\n",
            "Iteration 692, loss = 0.21295131\n",
            "Iteration 693, loss = 0.21275423\n",
            "Iteration 694, loss = 0.21255951\n",
            "Iteration 695, loss = 0.21236560\n",
            "Iteration 696, loss = 0.21217233\n",
            "Iteration 697, loss = 0.21197944\n",
            "Iteration 698, loss = 0.21178725\n",
            "Iteration 699, loss = 0.21159566\n",
            "Iteration 700, loss = 0.21140481\n",
            "Iteration 701, loss = 0.21121453\n",
            "Iteration 702, loss = 0.21102457\n",
            "Iteration 703, loss = 0.21083537\n",
            "Iteration 704, loss = 0.21064656\n",
            "Iteration 705, loss = 0.21045822\n",
            "Iteration 706, loss = 0.21027018\n",
            "Iteration 707, loss = 0.21008308\n",
            "Iteration 708, loss = 0.20989595\n",
            "Iteration 709, loss = 0.20970989\n",
            "Iteration 710, loss = 0.20952388\n",
            "Iteration 711, loss = 0.20933852\n",
            "Iteration 712, loss = 0.20915353\n",
            "Iteration 713, loss = 0.20896916\n",
            "Iteration 714, loss = 0.20878525\n",
            "Iteration 715, loss = 0.20860171\n",
            "Iteration 716, loss = 0.20841869\n",
            "Iteration 717, loss = 0.20823610\n",
            "Iteration 718, loss = 0.20805375\n",
            "Iteration 719, loss = 0.20787201\n",
            "Iteration 720, loss = 0.20769027\n",
            "Iteration 721, loss = 0.20750871\n",
            "Iteration 722, loss = 0.20732710\n",
            "Iteration 723, loss = 0.20714607\n",
            "Iteration 724, loss = 0.20696503\n",
            "Iteration 725, loss = 0.20678435\n",
            "Iteration 726, loss = 0.20660409\n",
            "Iteration 727, loss = 0.20642404\n",
            "Iteration 728, loss = 0.20624429\n",
            "Iteration 729, loss = 0.20606543\n",
            "Iteration 730, loss = 0.20588651\n",
            "Iteration 731, loss = 0.20570695\n",
            "Iteration 732, loss = 0.20552804\n",
            "Iteration 733, loss = 0.20534954\n",
            "Iteration 734, loss = 0.20517138\n",
            "Iteration 735, loss = 0.20499393\n",
            "Iteration 736, loss = 0.20481626\n",
            "Iteration 737, loss = 0.20463933\n",
            "Iteration 738, loss = 0.20446222\n",
            "Iteration 739, loss = 0.20428556\n",
            "Iteration 740, loss = 0.20410931\n",
            "Iteration 741, loss = 0.20393331\n",
            "Iteration 742, loss = 0.20375771\n",
            "Iteration 743, loss = 0.20358235\n",
            "Iteration 744, loss = 0.20340708\n",
            "Iteration 745, loss = 0.20323239\n",
            "Iteration 746, loss = 0.20305787\n",
            "Iteration 747, loss = 0.20288382\n",
            "Iteration 748, loss = 0.20270980\n",
            "Iteration 749, loss = 0.20253644\n",
            "Iteration 750, loss = 0.20236325\n",
            "Iteration 751, loss = 0.20219084\n",
            "Iteration 752, loss = 0.20201874\n",
            "Iteration 753, loss = 0.20184708\n",
            "Iteration 754, loss = 0.20167595\n",
            "Iteration 755, loss = 0.20150498\n",
            "Iteration 756, loss = 0.20133472\n",
            "Iteration 757, loss = 0.20116445\n",
            "Iteration 758, loss = 0.20099492\n",
            "Iteration 759, loss = 0.20082545\n",
            "Iteration 760, loss = 0.20065658\n",
            "Iteration 761, loss = 0.20048826\n",
            "Iteration 762, loss = 0.20032028\n",
            "Iteration 763, loss = 0.20015240\n",
            "Iteration 764, loss = 0.19998522\n",
            "Iteration 765, loss = 0.19981811\n",
            "Iteration 766, loss = 0.19965164\n",
            "Iteration 767, loss = 0.19948530\n",
            "Iteration 768, loss = 0.19931962\n",
            "Iteration 769, loss = 0.19915407\n",
            "Iteration 770, loss = 0.19899032\n",
            "Iteration 771, loss = 0.19882662\n",
            "Iteration 772, loss = 0.19866366\n",
            "Iteration 773, loss = 0.19850098\n",
            "Iteration 774, loss = 0.19833871\n",
            "Iteration 775, loss = 0.19817632\n",
            "Iteration 776, loss = 0.19801457\n",
            "Iteration 777, loss = 0.19785285\n",
            "Iteration 778, loss = 0.19769145\n",
            "Iteration 779, loss = 0.19753055\n",
            "Iteration 780, loss = 0.19736971\n",
            "Iteration 781, loss = 0.19720913\n",
            "Iteration 782, loss = 0.19704923\n",
            "Iteration 783, loss = 0.19688939\n",
            "Iteration 784, loss = 0.19672998\n",
            "Iteration 785, loss = 0.19657073\n",
            "Iteration 786, loss = 0.19641185\n",
            "Iteration 787, loss = 0.19625283\n",
            "Iteration 788, loss = 0.19609353\n",
            "Iteration 789, loss = 0.19593421\n",
            "Iteration 790, loss = 0.19577602\n",
            "Iteration 791, loss = 0.19561776\n",
            "Iteration 792, loss = 0.19546012\n",
            "Iteration 793, loss = 0.19530226\n",
            "Iteration 794, loss = 0.19514517\n",
            "Iteration 795, loss = 0.19498882\n",
            "Iteration 796, loss = 0.19483380\n",
            "Iteration 797, loss = 0.19467948\n",
            "Iteration 798, loss = 0.19452553\n",
            "Iteration 799, loss = 0.19437186\n",
            "Iteration 800, loss = 0.19421871\n",
            "0.8618823541771986\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABWAAAAFpCAYAAAASt4EGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAgAAAAAURAALAAAAAFCQX72dlGIhwfjqAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 1728x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksLkWgihlPDb",
        "colab_type": "code",
        "outputId": "281d760a-3b69-45ec-f8c7-8b3749c79c73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        }
      },
      "source": [
        "mf.ROC_(pickle_model, X_test,y_test )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEaCAYAAADtxAsqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhiAIgiCbKBqCIAiCbP8FLNCkvOrwqioAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4f0VC1wLlScS",
        "colab_type": "text"
      },
      "source": [
        "#In order to rebuild a similar model with future versions of scikit-learn, additional metadata should be saved along the pickled model:\n",
        "The training data, e.g. a reference to an immutable snapshot\n",
        "\n",
        "The python source code used to generate the model\n",
        "\n",
        "The versions of scikit-learn and its dependencies\n",
        "\n",
        "The cross validation score obtained on the training data\n",
        "\n",
        "This should make it possible to check that the cross-validation score is in the same range as before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2Tq60iElI4a",
        "colab_type": "code",
        "outputId": "1522c699-4933-4164-dc6b-f0c019b66b2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        }
      },
      "source": [
        "import pickle\n",
        "\n",
        "#from google.colab import files\n",
        "#files.download('your typical text file or what ever.txt')\n",
        "# Create your model here (same as above)\n",
        "#\n",
        "\n",
        "# Save to file in the current working directory\n",
        "#model=tree\n",
        "#pkl_filename = \"/content/drive/My Drive/tree(3-1)-pickle_model.pkl\"\n",
        "#with open(pkl_filename, 'wb') as file:\n",
        "#    pickle.dump(model, file)\n",
        "#Ojo se queda convertido en cero\n",
        "df_UNK=pd.read_excel('/content/drive/My Drive/tree_UNK.xlsx')\n",
        "print(df_UNK.shape)\n",
        "# Load from file\n",
        "pkl_filename = \"/content/drive/My Drive/NN-pickle_model.pkl\"\n",
        "with open(pkl_filename, 'rb') as file:\n",
        "    pickle_model = pickle.load(file)\n",
        "    \n",
        "#Calculate the accuracy score and predict target values from recovered model\n",
        "\n",
        "df_pre=df_UNK.copy()\n",
        "df_pre=df_pre[features_]\n",
        "df_pre.drop(['source_type'], axis=1, inplace=True)\n",
        "print('Dataframe con AGNs a clasificar :', df_pre.shape)\n",
        "print('Caracteristicas del modelo entrenado:', len(features_)-1)\n",
        "print('Â¡Deben ser iguales!')\n",
        "#normalizamos el dtaframe A PREDECIR\n",
        "sc = StandardScaler()\n",
        "sc.fit(df_pre)\n",
        "df_pre_std = sc.transform(df_pre)\n",
        "\n",
        "#X_train, X_test, y_train, y_test = train_test_split(X_std, y_, test_size=0.3)\n",
        "#score = pickle_model.score(X_test, y_test)\n",
        "#print(\"Test score: {0:.2f} %\".format(100 * score))\n",
        "#Snippet_192(pickle_model, X_test, y_test)\n",
        "pickle_model.get_params\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(573, 58)\n",
            "Dataframe con AGNs a clasificar : (573, 44)\n",
            "Caracteristicas del modelo entrenado: 44\n",
            "Â¡Deben ser iguales!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method BaseEstimator.get_params of MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
              "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
              "              hidden_layer_sizes=(45, 90, 2), learning_rate='constant',\n",
              "              learning_rate_init=0.001, max_fun=15000, max_iter=800,\n",
              "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
              "              power_t=0.5, random_state=21, shuffle=True, solver='sgd',\n",
              "              tol=1e-09, validation_fraction=0.1, verbose=10, warm_start=False)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNBk6Bl3vcW8",
        "colab_type": "text"
      },
      "source": [
        "#PREDICCION USANDO EL MODELO SALVADO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJGkXNlwvcD8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# REDECIMOS USANDO EL DATAFRAME INCOGNITA CON EL MODELO ENTRENADO, SALVADO Y VUELTO A CARGAR CON PICKLE_MODEL\n",
        "#-----------------------------------------------------------------#\n",
        "Ypredict = pickle_model.predict(df_pre_std)\n",
        "#-----------------------------------------------------------------#\n",
        "#Ypredict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4jv68T0JNur",
        "colab_type": "code",
        "outputId": "49f24806-8130-4806-f673-6c99855c7797",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        }
      },
      "source": [
        "#df_UNK.head(20)\n",
        "#df_pre.head(5)\n",
        "#X_std.shape\n",
        "#COMPLETAMOS EL DATAFRAME DE LOS BLAZARS DESCONOCIDOS CON NUESTRA PREDICCIÃ“N \n",
        " #-----------------------------------------------------------------# \n",
        "df_UNK['source_type']=Ypredict\n",
        "df_UNK.head(5)\n",
        "#-----------------------------------------------------------------#\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>ra</th>\n",
              "      <th>dec</th>\n",
              "      <th>flux_1_100_gev</th>\n",
              "      <th>spectral_index</th>\n",
              "      <th>spectral_index_error</th>\n",
              "      <th>detection_significance</th>\n",
              "      <th>lii</th>\n",
              "      <th>bii</th>\n",
              "      <th>pivot_energy</th>\n",
              "      <th>flux_density</th>\n",
              "      <th>flux_density_error</th>\n",
              "      <th>energy_flux</th>\n",
              "      <th>energy_flux_error</th>\n",
              "      <th>curve_significance</th>\n",
              "      <th>spectrum_type</th>\n",
              "      <th>powerlaw_index</th>\n",
              "      <th>flux_100_300_mev</th>\n",
              "      <th>flux_100_300_mev_pos_err</th>\n",
              "      <th>flux_100_300_mev_neg_err</th>\n",
              "      <th>nufnu_100_300_mev</th>\n",
              "      <th>sqrt_ts_100_300_mev</th>\n",
              "      <th>flux_0p3_1_gev</th>\n",
              "      <th>flux_0p3_1_gev_pos_err</th>\n",
              "      <th>flux_0p3_1_gev_neg_err</th>\n",
              "      <th>nufnu_0p3_1_gev</th>\n",
              "      <th>sqrt_ts_0p3_1_gev</th>\n",
              "      <th>flux_1_3_gev</th>\n",
              "      <th>flux_1_3_gev_pos_err</th>\n",
              "      <th>flux_1_3_gev_neg_err</th>\n",
              "      <th>nufnu_1_3_gev</th>\n",
              "      <th>sqrt_ts_1_3_gev</th>\n",
              "      <th>flux_3_10_gev</th>\n",
              "      <th>nufnu_3_10_gev</th>\n",
              "      <th>sqrt_ts_3_10_gev</th>\n",
              "      <th>flux_10_100_gev</th>\n",
              "      <th>nufnu_10_100_gev</th>\n",
              "      <th>sqrt_ts_10_100_gev</th>\n",
              "      <th>variability_index</th>\n",
              "      <th>significance_peak</th>\n",
              "      <th>flux_peak</th>\n",
              "      <th>flux_peak_error</th>\n",
              "      <th>time_peak</th>\n",
              "      <th>time_peak_interval</th>\n",
              "      <th>source_type</th>\n",
              "      <th>analysis_flags</th>\n",
              "      <th>HR12</th>\n",
              "      <th>HR23</th>\n",
              "      <th>HR34</th>\n",
              "      <th>hard_slope</th>\n",
              "      <th>soft_slope</th>\n",
              "      <th>P_E_lg</th>\n",
              "      <th>TS_</th>\n",
              "      <th>sig_</th>\n",
              "      <th>gamm_log</th>\n",
              "      <th>Ts_log</th>\n",
              "      <th>sig_log</th>\n",
              "      <th>F100_log</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>117</td>\n",
              "      <td>170.807907</td>\n",
              "      <td>-64.260902</td>\n",
              "      <td>1.268470e-09</td>\n",
              "      <td>2.24084</td>\n",
              "      <td>0.10782</td>\n",
              "      <td>14.025361</td>\n",
              "      <td>293.535187</td>\n",
              "      <td>-3.010400</td>\n",
              "      <td>339.359985</td>\n",
              "      <td>6.514200e-11</td>\n",
              "      <td>7.074700e-12</td>\n",
              "      <td>2.629280e-11</td>\n",
              "      <td>2.118950e-12</td>\n",
              "      <td>4.524</td>\n",
              "      <td>2</td>\n",
              "      <td>2.6114</td>\n",
              "      <td>5.874740e-08</td>\n",
              "      <td>6.184800e-09</td>\n",
              "      <td>-6.154550e-09</td>\n",
              "      <td>1.469530e-11</td>\n",
              "      <td>9.574000</td>\n",
              "      <td>1.306430e-08</td>\n",
              "      <td>1.428180e-09</td>\n",
              "      <td>-1.413830e-09</td>\n",
              "      <td>8.282390e-12</td>\n",
              "      <td>9.663000</td>\n",
              "      <td>1.216690e-09</td>\n",
              "      <td>2.746420e-10</td>\n",
              "      <td>-2.659180e-10</td>\n",
              "      <td>2.492240e-12</td>\n",
              "      <td>4.895000</td>\n",
              "      <td>9.437780e-11</td>\n",
              "      <td>4.955250e-13</td>\n",
              "      <td>1.854000</td>\n",
              "      <td>2.818170e-11</td>\n",
              "      <td>2.611100e-13</td>\n",
              "      <td>2.527</td>\n",
              "      <td>210.782593</td>\n",
              "      <td>9.025927</td>\n",
              "      <td>3.128840e-07</td>\n",
              "      <td>3.964350e-08</td>\n",
              "      <td>54725.683594</td>\n",
              "      <td>2630000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.636151</td>\n",
              "      <td>-0.829607</td>\n",
              "      <td>-0.856029</td>\n",
              "      <td>0.026422</td>\n",
              "      <td>0.193456</td>\n",
              "      <td>2.530661</td>\n",
              "      <td>15.028675</td>\n",
              "      <td>0.322559</td>\n",
              "      <td>0.350411</td>\n",
              "      <td>1.176921</td>\n",
              "      <td>-0.491391</td>\n",
              "      <td>-8.896720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>118</td>\n",
              "      <td>202.240402</td>\n",
              "      <td>-56.130901</td>\n",
              "      <td>6.763560e-09</td>\n",
              "      <td>2.21938</td>\n",
              "      <td>0.03290</td>\n",
              "      <td>46.079670</td>\n",
              "      <td>308.176086</td>\n",
              "      <td>6.358300</td>\n",
              "      <td>582.570007</td>\n",
              "      <td>3.501100e-11</td>\n",
              "      <td>1.167800e-12</td>\n",
              "      <td>7.046150e-11</td>\n",
              "      <td>2.359720e-12</td>\n",
              "      <td>5.998</td>\n",
              "      <td>2</td>\n",
              "      <td>2.3404</td>\n",
              "      <td>9.235340e-08</td>\n",
              "      <td>8.233390e-09</td>\n",
              "      <td>-8.233390e-09</td>\n",
              "      <td>2.237260e-11</td>\n",
              "      <td>11.952000</td>\n",
              "      <td>2.821480e-08</td>\n",
              "      <td>1.211760e-09</td>\n",
              "      <td>-1.211760e-09</td>\n",
              "      <td>1.891400e-11</td>\n",
              "      <td>27.768000</td>\n",
              "      <td>5.441840e-09</td>\n",
              "      <td>2.938120e-10</td>\n",
              "      <td>-2.938120e-10</td>\n",
              "      <td>1.247090e-11</td>\n",
              "      <td>26.823999</td>\n",
              "      <td>9.965950e-10</td>\n",
              "      <td>6.266210e-12</td>\n",
              "      <td>18.434000</td>\n",
              "      <td>1.578590e-10</td>\n",
              "      <td>1.893100e-12</td>\n",
              "      <td>9.420</td>\n",
              "      <td>788.263062</td>\n",
              "      <td>16.583115</td>\n",
              "      <td>3.443660e-07</td>\n",
              "      <td>3.004300e-08</td>\n",
              "      <td>56004.156250</td>\n",
              "      <td>2630000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.531969</td>\n",
              "      <td>-0.676626</td>\n",
              "      <td>-0.690423</td>\n",
              "      <td>0.013797</td>\n",
              "      <td>0.144657</td>\n",
              "      <td>2.765348</td>\n",
              "      <td>17.106525</td>\n",
              "      <td>0.130166</td>\n",
              "      <td>0.346232</td>\n",
              "      <td>1.233162</td>\n",
              "      <td>-0.885503</td>\n",
              "      <td>-8.169825</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>119</td>\n",
              "      <td>159.735199</td>\n",
              "      <td>-53.186100</td>\n",
              "      <td>1.939800e-09</td>\n",
              "      <td>2.31971</td>\n",
              "      <td>0.07696</td>\n",
              "      <td>24.437548</td>\n",
              "      <td>283.751587</td>\n",
              "      <td>4.679100</td>\n",
              "      <td>327.359985</td>\n",
              "      <td>7.086100e-11</td>\n",
              "      <td>3.901800e-12</td>\n",
              "      <td>3.157340e-11</td>\n",
              "      <td>1.598800e-12</td>\n",
              "      <td>5.330</td>\n",
              "      <td>2</td>\n",
              "      <td>2.6188</td>\n",
              "      <td>5.444920e-08</td>\n",
              "      <td>4.922340e-09</td>\n",
              "      <td>-4.922340e-09</td>\n",
              "      <td>1.305460e-11</td>\n",
              "      <td>11.671000</td>\n",
              "      <td>1.373480e-08</td>\n",
              "      <td>9.768080e-10</td>\n",
              "      <td>-9.768080e-10</td>\n",
              "      <td>8.842370e-12</td>\n",
              "      <td>15.994000</td>\n",
              "      <td>1.892330e-09</td>\n",
              "      <td>2.139180e-10</td>\n",
              "      <td>-2.063610e-10</td>\n",
              "      <td>4.104160e-12</td>\n",
              "      <td>11.592000</td>\n",
              "      <td>1.549630e-10</td>\n",
              "      <td>8.952460e-13</td>\n",
              "      <td>4.716000</td>\n",
              "      <td>1.032650e-14</td>\n",
              "      <td>1.008050e-16</td>\n",
              "      <td>0.000</td>\n",
              "      <td>914.668640</td>\n",
              "      <td>22.781076</td>\n",
              "      <td>5.224050e-07</td>\n",
              "      <td>3.202220e-08</td>\n",
              "      <td>56004.156250</td>\n",
              "      <td>2630000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.597125</td>\n",
              "      <td>-0.757815</td>\n",
              "      <td>-0.848617</td>\n",
              "      <td>0.090802</td>\n",
              "      <td>0.160689</td>\n",
              "      <td>2.515026</td>\n",
              "      <td>37.428822</td>\n",
              "      <td>0.218107</td>\n",
              "      <td>0.365434</td>\n",
              "      <td>1.573206</td>\n",
              "      <td>-0.661330</td>\n",
              "      <td>-8.712243</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>120</td>\n",
              "      <td>276.320709</td>\n",
              "      <td>-52.509300</td>\n",
              "      <td>1.993040e-09</td>\n",
              "      <td>2.01742</td>\n",
              "      <td>0.08697</td>\n",
              "      <td>21.572411</td>\n",
              "      <td>342.269989</td>\n",
              "      <td>-17.464701</td>\n",
              "      <td>714.700012</td>\n",
              "      <td>5.416700e-12</td>\n",
              "      <td>3.985900e-13</td>\n",
              "      <td>1.649350e-11</td>\n",
              "      <td>1.321550e-12</td>\n",
              "      <td>3.863</td>\n",
              "      <td>2</td>\n",
              "      <td>2.2269</td>\n",
              "      <td>1.263060e-08</td>\n",
              "      <td>3.067500e-09</td>\n",
              "      <td>-3.020100e-09</td>\n",
              "      <td>3.179230e-12</td>\n",
              "      <td>4.266000</td>\n",
              "      <td>7.096370e-09</td>\n",
              "      <td>6.141310e-10</td>\n",
              "      <td>-6.141310e-10</td>\n",
              "      <td>4.914550e-12</td>\n",
              "      <td>13.680000</td>\n",
              "      <td>1.576740e-09</td>\n",
              "      <td>1.665220e-10</td>\n",
              "      <td>-1.591710e-10</td>\n",
              "      <td>3.672690e-12</td>\n",
              "      <td>13.956000</td>\n",
              "      <td>2.878620e-10</td>\n",
              "      <td>1.822500e-12</td>\n",
              "      <td>8.611000</td>\n",
              "      <td>6.177760e-11</td>\n",
              "      <td>7.242330e-13</td>\n",
              "      <td>5.219</td>\n",
              "      <td>310.308960</td>\n",
              "      <td>13.392247</td>\n",
              "      <td>9.469290e-08</td>\n",
              "      <td>1.144430e-08</td>\n",
              "      <td>55395.359375</td>\n",
              "      <td>2630000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.280541</td>\n",
              "      <td>-0.636407</td>\n",
              "      <td>-0.691235</td>\n",
              "      <td>0.054828</td>\n",
              "      <td>0.355866</td>\n",
              "      <td>2.854124</td>\n",
              "      <td>14.384529</td>\n",
              "      <td>0.179071</td>\n",
              "      <td>0.304796</td>\n",
              "      <td>1.157896</td>\n",
              "      <td>-0.746974</td>\n",
              "      <td>-8.700484</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>121</td>\n",
              "      <td>32.697201</td>\n",
              "      <td>-51.028198</td>\n",
              "      <td>4.740330e-09</td>\n",
              "      <td>2.16971</td>\n",
              "      <td>0.03282</td>\n",
              "      <td>62.947647</td>\n",
              "      <td>276.111694</td>\n",
              "      <td>-61.767799</td>\n",
              "      <td>385.059998</td>\n",
              "      <td>5.930000e-11</td>\n",
              "      <td>1.618200e-12</td>\n",
              "      <td>5.107200e-11</td>\n",
              "      <td>1.523330e-12</td>\n",
              "      <td>4.746</td>\n",
              "      <td>2</td>\n",
              "      <td>2.3031</td>\n",
              "      <td>6.790450e-08</td>\n",
              "      <td>3.436410e-09</td>\n",
              "      <td>-3.436410e-09</td>\n",
              "      <td>1.627360e-11</td>\n",
              "      <td>22.070999</td>\n",
              "      <td>1.822940e-08</td>\n",
              "      <td>6.762120e-10</td>\n",
              "      <td>-6.762120e-10</td>\n",
              "      <td>1.217990e-11</td>\n",
              "      <td>38.094002</td>\n",
              "      <td>4.268660e-09</td>\n",
              "      <td>2.105200e-10</td>\n",
              "      <td>-2.105200e-10</td>\n",
              "      <td>9.830760e-12</td>\n",
              "      <td>38.015999</td>\n",
              "      <td>7.063220e-10</td>\n",
              "      <td>4.505460e-12</td>\n",
              "      <td>20.693001</td>\n",
              "      <td>9.707360e-11</td>\n",
              "      <td>1.242470e-12</td>\n",
              "      <td>8.745</td>\n",
              "      <td>1444.283813</td>\n",
              "      <td>27.729914</td>\n",
              "      <td>4.094160e-07</td>\n",
              "      <td>2.493990e-08</td>\n",
              "      <td>54756.121094</td>\n",
              "      <td>2630000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.576720</td>\n",
              "      <td>-0.620531</td>\n",
              "      <td>-0.716050</td>\n",
              "      <td>0.095520</td>\n",
              "      <td>0.043811</td>\n",
              "      <td>2.585528</td>\n",
              "      <td>22.944206</td>\n",
              "      <td>0.075396</td>\n",
              "      <td>0.336402</td>\n",
              "      <td>1.360673</td>\n",
              "      <td>-1.122652</td>\n",
              "      <td>-8.324191</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0          ra        dec  flux_1_100_gev  spectral_index  \\\n",
              "0         117  170.807907 -64.260902    1.268470e-09         2.24084   \n",
              "1         118  202.240402 -56.130901    6.763560e-09         2.21938   \n",
              "2         119  159.735199 -53.186100    1.939800e-09         2.31971   \n",
              "3         120  276.320709 -52.509300    1.993040e-09         2.01742   \n",
              "4         121   32.697201 -51.028198    4.740330e-09         2.16971   \n",
              "\n",
              "   spectral_index_error  detection_significance         lii        bii  \\\n",
              "0               0.10782               14.025361  293.535187  -3.010400   \n",
              "1               0.03290               46.079670  308.176086   6.358300   \n",
              "2               0.07696               24.437548  283.751587   4.679100   \n",
              "3               0.08697               21.572411  342.269989 -17.464701   \n",
              "4               0.03282               62.947647  276.111694 -61.767799   \n",
              "\n",
              "   pivot_energy  flux_density  flux_density_error   energy_flux  \\\n",
              "0    339.359985  6.514200e-11        7.074700e-12  2.629280e-11   \n",
              "1    582.570007  3.501100e-11        1.167800e-12  7.046150e-11   \n",
              "2    327.359985  7.086100e-11        3.901800e-12  3.157340e-11   \n",
              "3    714.700012  5.416700e-12        3.985900e-13  1.649350e-11   \n",
              "4    385.059998  5.930000e-11        1.618200e-12  5.107200e-11   \n",
              "\n",
              "   energy_flux_error  curve_significance  spectrum_type  powerlaw_index  \\\n",
              "0       2.118950e-12               4.524              2          2.6114   \n",
              "1       2.359720e-12               5.998              2          2.3404   \n",
              "2       1.598800e-12               5.330              2          2.6188   \n",
              "3       1.321550e-12               3.863              2          2.2269   \n",
              "4       1.523330e-12               4.746              2          2.3031   \n",
              "\n",
              "   flux_100_300_mev  flux_100_300_mev_pos_err  flux_100_300_mev_neg_err  \\\n",
              "0      5.874740e-08              6.184800e-09             -6.154550e-09   \n",
              "1      9.235340e-08              8.233390e-09             -8.233390e-09   \n",
              "2      5.444920e-08              4.922340e-09             -4.922340e-09   \n",
              "3      1.263060e-08              3.067500e-09             -3.020100e-09   \n",
              "4      6.790450e-08              3.436410e-09             -3.436410e-09   \n",
              "\n",
              "   nufnu_100_300_mev  sqrt_ts_100_300_mev  flux_0p3_1_gev  \\\n",
              "0       1.469530e-11             9.574000    1.306430e-08   \n",
              "1       2.237260e-11            11.952000    2.821480e-08   \n",
              "2       1.305460e-11            11.671000    1.373480e-08   \n",
              "3       3.179230e-12             4.266000    7.096370e-09   \n",
              "4       1.627360e-11            22.070999    1.822940e-08   \n",
              "\n",
              "   flux_0p3_1_gev_pos_err  flux_0p3_1_gev_neg_err  nufnu_0p3_1_gev  \\\n",
              "0            1.428180e-09           -1.413830e-09     8.282390e-12   \n",
              "1            1.211760e-09           -1.211760e-09     1.891400e-11   \n",
              "2            9.768080e-10           -9.768080e-10     8.842370e-12   \n",
              "3            6.141310e-10           -6.141310e-10     4.914550e-12   \n",
              "4            6.762120e-10           -6.762120e-10     1.217990e-11   \n",
              "\n",
              "   sqrt_ts_0p3_1_gev  flux_1_3_gev  flux_1_3_gev_pos_err  \\\n",
              "0           9.663000  1.216690e-09          2.746420e-10   \n",
              "1          27.768000  5.441840e-09          2.938120e-10   \n",
              "2          15.994000  1.892330e-09          2.139180e-10   \n",
              "3          13.680000  1.576740e-09          1.665220e-10   \n",
              "4          38.094002  4.268660e-09          2.105200e-10   \n",
              "\n",
              "   flux_1_3_gev_neg_err  nufnu_1_3_gev  sqrt_ts_1_3_gev  flux_3_10_gev  \\\n",
              "0         -2.659180e-10   2.492240e-12         4.895000   9.437780e-11   \n",
              "1         -2.938120e-10   1.247090e-11        26.823999   9.965950e-10   \n",
              "2         -2.063610e-10   4.104160e-12        11.592000   1.549630e-10   \n",
              "3         -1.591710e-10   3.672690e-12        13.956000   2.878620e-10   \n",
              "4         -2.105200e-10   9.830760e-12        38.015999   7.063220e-10   \n",
              "\n",
              "   nufnu_3_10_gev  sqrt_ts_3_10_gev  flux_10_100_gev  nufnu_10_100_gev  \\\n",
              "0    4.955250e-13          1.854000     2.818170e-11      2.611100e-13   \n",
              "1    6.266210e-12         18.434000     1.578590e-10      1.893100e-12   \n",
              "2    8.952460e-13          4.716000     1.032650e-14      1.008050e-16   \n",
              "3    1.822500e-12          8.611000     6.177760e-11      7.242330e-13   \n",
              "4    4.505460e-12         20.693001     9.707360e-11      1.242470e-12   \n",
              "\n",
              "   sqrt_ts_10_100_gev  variability_index  significance_peak     flux_peak  \\\n",
              "0               2.527         210.782593           9.025927  3.128840e-07   \n",
              "1               9.420         788.263062          16.583115  3.443660e-07   \n",
              "2               0.000         914.668640          22.781076  5.224050e-07   \n",
              "3               5.219         310.308960          13.392247  9.469290e-08   \n",
              "4               8.745        1444.283813          27.729914  4.094160e-07   \n",
              "\n",
              "   flux_peak_error     time_peak  time_peak_interval  source_type  \\\n",
              "0     3.964350e-08  54725.683594             2630000          0.0   \n",
              "1     3.004300e-08  56004.156250             2630000          0.0   \n",
              "2     3.202220e-08  56004.156250             2630000          0.0   \n",
              "3     1.144430e-08  55395.359375             2630000          0.0   \n",
              "4     2.493990e-08  54756.121094             2630000          0.0   \n",
              "\n",
              "   analysis_flags      HR12      HR23      HR34  hard_slope  soft_slope  \\\n",
              "0               0 -0.636151 -0.829607 -0.856029    0.026422    0.193456   \n",
              "1               0 -0.531969 -0.676626 -0.690423    0.013797    0.144657   \n",
              "2               0 -0.597125 -0.757815 -0.848617    0.090802    0.160689   \n",
              "3               0 -0.280541 -0.636407 -0.691235    0.054828    0.355866   \n",
              "4               0 -0.576720 -0.620531 -0.716050    0.095520    0.043811   \n",
              "\n",
              "     P_E_lg        TS_      sig_  gamm_log    Ts_log   sig_log  F100_log  \n",
              "0  2.530661  15.028675  0.322559  0.350411  1.176921 -0.491391 -8.896720  \n",
              "1  2.765348  17.106525  0.130166  0.346232  1.233162 -0.885503 -8.169825  \n",
              "2  2.515026  37.428822  0.218107  0.365434  1.573206 -0.661330 -8.712243  \n",
              "3  2.854124  14.384529  0.179071  0.304796  1.157896 -0.746974 -8.700484  \n",
              "4  2.585528  22.944206  0.075396  0.336402  1.360673 -1.122652 -8.324191  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7ismsLtxHsv",
        "colab_type": "code",
        "outputId": "7e1a914a-fb35-4381-bc5d-0ba57fa236e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        }
      },
      "source": [
        "# RECUENTO DE CLASIFICACION\n",
        "print ('Bl Lac = 1 -- FSRQ = 0 -- UNKNOWN = 2 ')\n",
        "df0['source_type'].apply(pd.Series).stack().value_counts()\n",
        "#Ypredict\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Bl Lac = 1 -- FSRQ = 0 -- UNKNOWN = 2 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    660\n",
              "2    573\n",
              "0    484\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TC0MdOArT3Tq",
        "colab_type": "code",
        "outputId": "89d1f48c-5280-40fe-bddb-d7a2e125c55e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        }
      },
      "source": [
        "df_UNK.head(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>ra</th>\n",
              "      <th>dec</th>\n",
              "      <th>flux_1_100_gev</th>\n",
              "      <th>spectral_index</th>\n",
              "      <th>spectral_index_error</th>\n",
              "      <th>detection_significance</th>\n",
              "      <th>lii</th>\n",
              "      <th>bii</th>\n",
              "      <th>pivot_energy</th>\n",
              "      <th>flux_density</th>\n",
              "      <th>flux_density_error</th>\n",
              "      <th>energy_flux</th>\n",
              "      <th>energy_flux_error</th>\n",
              "      <th>curve_significance</th>\n",
              "      <th>spectrum_type</th>\n",
              "      <th>powerlaw_index</th>\n",
              "      <th>flux_100_300_mev</th>\n",
              "      <th>flux_100_300_mev_pos_err</th>\n",
              "      <th>flux_100_300_mev_neg_err</th>\n",
              "      <th>nufnu_100_300_mev</th>\n",
              "      <th>sqrt_ts_100_300_mev</th>\n",
              "      <th>flux_0p3_1_gev</th>\n",
              "      <th>flux_0p3_1_gev_pos_err</th>\n",
              "      <th>flux_0p3_1_gev_neg_err</th>\n",
              "      <th>nufnu_0p3_1_gev</th>\n",
              "      <th>sqrt_ts_0p3_1_gev</th>\n",
              "      <th>flux_1_3_gev</th>\n",
              "      <th>flux_1_3_gev_pos_err</th>\n",
              "      <th>flux_1_3_gev_neg_err</th>\n",
              "      <th>nufnu_1_3_gev</th>\n",
              "      <th>sqrt_ts_1_3_gev</th>\n",
              "      <th>flux_3_10_gev</th>\n",
              "      <th>nufnu_3_10_gev</th>\n",
              "      <th>sqrt_ts_3_10_gev</th>\n",
              "      <th>flux_10_100_gev</th>\n",
              "      <th>nufnu_10_100_gev</th>\n",
              "      <th>sqrt_ts_10_100_gev</th>\n",
              "      <th>variability_index</th>\n",
              "      <th>significance_peak</th>\n",
              "      <th>flux_peak</th>\n",
              "      <th>flux_peak_error</th>\n",
              "      <th>time_peak</th>\n",
              "      <th>time_peak_interval</th>\n",
              "      <th>source_type</th>\n",
              "      <th>analysis_flags</th>\n",
              "      <th>HR12</th>\n",
              "      <th>HR23</th>\n",
              "      <th>HR34</th>\n",
              "      <th>hard_slope</th>\n",
              "      <th>soft_slope</th>\n",
              "      <th>P_E_lg</th>\n",
              "      <th>TS_</th>\n",
              "      <th>sig_</th>\n",
              "      <th>gamm_log</th>\n",
              "      <th>Ts_log</th>\n",
              "      <th>sig_log</th>\n",
              "      <th>F100_log</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>117</td>\n",
              "      <td>170.807907</td>\n",
              "      <td>-64.260902</td>\n",
              "      <td>1.268470e-09</td>\n",
              "      <td>2.24084</td>\n",
              "      <td>0.10782</td>\n",
              "      <td>14.025361</td>\n",
              "      <td>293.535187</td>\n",
              "      <td>-3.010400</td>\n",
              "      <td>339.359985</td>\n",
              "      <td>6.514200e-11</td>\n",
              "      <td>7.074700e-12</td>\n",
              "      <td>2.629280e-11</td>\n",
              "      <td>2.118950e-12</td>\n",
              "      <td>4.524</td>\n",
              "      <td>2</td>\n",
              "      <td>2.6114</td>\n",
              "      <td>5.874740e-08</td>\n",
              "      <td>6.184800e-09</td>\n",
              "      <td>-6.154550e-09</td>\n",
              "      <td>1.469530e-11</td>\n",
              "      <td>9.574000</td>\n",
              "      <td>1.306430e-08</td>\n",
              "      <td>1.428180e-09</td>\n",
              "      <td>-1.413830e-09</td>\n",
              "      <td>8.282390e-12</td>\n",
              "      <td>9.663000</td>\n",
              "      <td>1.216690e-09</td>\n",
              "      <td>2.746420e-10</td>\n",
              "      <td>-2.659180e-10</td>\n",
              "      <td>2.492240e-12</td>\n",
              "      <td>4.895000</td>\n",
              "      <td>9.437780e-11</td>\n",
              "      <td>4.955250e-13</td>\n",
              "      <td>1.854000</td>\n",
              "      <td>2.818170e-11</td>\n",
              "      <td>2.611100e-13</td>\n",
              "      <td>2.527</td>\n",
              "      <td>210.782593</td>\n",
              "      <td>9.025927</td>\n",
              "      <td>3.128840e-07</td>\n",
              "      <td>3.964350e-08</td>\n",
              "      <td>54725.683594</td>\n",
              "      <td>2630000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.636151</td>\n",
              "      <td>-0.829607</td>\n",
              "      <td>-0.856029</td>\n",
              "      <td>0.026422</td>\n",
              "      <td>0.193456</td>\n",
              "      <td>2.530661</td>\n",
              "      <td>15.028675</td>\n",
              "      <td>0.322559</td>\n",
              "      <td>0.350411</td>\n",
              "      <td>1.176921</td>\n",
              "      <td>-0.491391</td>\n",
              "      <td>-8.896720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>118</td>\n",
              "      <td>202.240402</td>\n",
              "      <td>-56.130901</td>\n",
              "      <td>6.763560e-09</td>\n",
              "      <td>2.21938</td>\n",
              "      <td>0.03290</td>\n",
              "      <td>46.079670</td>\n",
              "      <td>308.176086</td>\n",
              "      <td>6.358300</td>\n",
              "      <td>582.570007</td>\n",
              "      <td>3.501100e-11</td>\n",
              "      <td>1.167800e-12</td>\n",
              "      <td>7.046150e-11</td>\n",
              "      <td>2.359720e-12</td>\n",
              "      <td>5.998</td>\n",
              "      <td>2</td>\n",
              "      <td>2.3404</td>\n",
              "      <td>9.235340e-08</td>\n",
              "      <td>8.233390e-09</td>\n",
              "      <td>-8.233390e-09</td>\n",
              "      <td>2.237260e-11</td>\n",
              "      <td>11.952000</td>\n",
              "      <td>2.821480e-08</td>\n",
              "      <td>1.211760e-09</td>\n",
              "      <td>-1.211760e-09</td>\n",
              "      <td>1.891400e-11</td>\n",
              "      <td>27.768000</td>\n",
              "      <td>5.441840e-09</td>\n",
              "      <td>2.938120e-10</td>\n",
              "      <td>-2.938120e-10</td>\n",
              "      <td>1.247090e-11</td>\n",
              "      <td>26.823999</td>\n",
              "      <td>9.965950e-10</td>\n",
              "      <td>6.266210e-12</td>\n",
              "      <td>18.434000</td>\n",
              "      <td>1.578590e-10</td>\n",
              "      <td>1.893100e-12</td>\n",
              "      <td>9.420</td>\n",
              "      <td>788.263062</td>\n",
              "      <td>16.583115</td>\n",
              "      <td>3.443660e-07</td>\n",
              "      <td>3.004300e-08</td>\n",
              "      <td>56004.156250</td>\n",
              "      <td>2630000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.531969</td>\n",
              "      <td>-0.676626</td>\n",
              "      <td>-0.690423</td>\n",
              "      <td>0.013797</td>\n",
              "      <td>0.144657</td>\n",
              "      <td>2.765348</td>\n",
              "      <td>17.106525</td>\n",
              "      <td>0.130166</td>\n",
              "      <td>0.346232</td>\n",
              "      <td>1.233162</td>\n",
              "      <td>-0.885503</td>\n",
              "      <td>-8.169825</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>119</td>\n",
              "      <td>159.735199</td>\n",
              "      <td>-53.186100</td>\n",
              "      <td>1.939800e-09</td>\n",
              "      <td>2.31971</td>\n",
              "      <td>0.07696</td>\n",
              "      <td>24.437548</td>\n",
              "      <td>283.751587</td>\n",
              "      <td>4.679100</td>\n",
              "      <td>327.359985</td>\n",
              "      <td>7.086100e-11</td>\n",
              "      <td>3.901800e-12</td>\n",
              "      <td>3.157340e-11</td>\n",
              "      <td>1.598800e-12</td>\n",
              "      <td>5.330</td>\n",
              "      <td>2</td>\n",
              "      <td>2.6188</td>\n",
              "      <td>5.444920e-08</td>\n",
              "      <td>4.922340e-09</td>\n",
              "      <td>-4.922340e-09</td>\n",
              "      <td>1.305460e-11</td>\n",
              "      <td>11.671000</td>\n",
              "      <td>1.373480e-08</td>\n",
              "      <td>9.768080e-10</td>\n",
              "      <td>-9.768080e-10</td>\n",
              "      <td>8.842370e-12</td>\n",
              "      <td>15.994000</td>\n",
              "      <td>1.892330e-09</td>\n",
              "      <td>2.139180e-10</td>\n",
              "      <td>-2.063610e-10</td>\n",
              "      <td>4.104160e-12</td>\n",
              "      <td>11.592000</td>\n",
              "      <td>1.549630e-10</td>\n",
              "      <td>8.952460e-13</td>\n",
              "      <td>4.716000</td>\n",
              "      <td>1.032650e-14</td>\n",
              "      <td>1.008050e-16</td>\n",
              "      <td>0.000</td>\n",
              "      <td>914.668640</td>\n",
              "      <td>22.781076</td>\n",
              "      <td>5.224050e-07</td>\n",
              "      <td>3.202220e-08</td>\n",
              "      <td>56004.156250</td>\n",
              "      <td>2630000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.597125</td>\n",
              "      <td>-0.757815</td>\n",
              "      <td>-0.848617</td>\n",
              "      <td>0.090802</td>\n",
              "      <td>0.160689</td>\n",
              "      <td>2.515026</td>\n",
              "      <td>37.428822</td>\n",
              "      <td>0.218107</td>\n",
              "      <td>0.365434</td>\n",
              "      <td>1.573206</td>\n",
              "      <td>-0.661330</td>\n",
              "      <td>-8.712243</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>120</td>\n",
              "      <td>276.320709</td>\n",
              "      <td>-52.509300</td>\n",
              "      <td>1.993040e-09</td>\n",
              "      <td>2.01742</td>\n",
              "      <td>0.08697</td>\n",
              "      <td>21.572411</td>\n",
              "      <td>342.269989</td>\n",
              "      <td>-17.464701</td>\n",
              "      <td>714.700012</td>\n",
              "      <td>5.416700e-12</td>\n",
              "      <td>3.985900e-13</td>\n",
              "      <td>1.649350e-11</td>\n",
              "      <td>1.321550e-12</td>\n",
              "      <td>3.863</td>\n",
              "      <td>2</td>\n",
              "      <td>2.2269</td>\n",
              "      <td>1.263060e-08</td>\n",
              "      <td>3.067500e-09</td>\n",
              "      <td>-3.020100e-09</td>\n",
              "      <td>3.179230e-12</td>\n",
              "      <td>4.266000</td>\n",
              "      <td>7.096370e-09</td>\n",
              "      <td>6.141310e-10</td>\n",
              "      <td>-6.141310e-10</td>\n",
              "      <td>4.914550e-12</td>\n",
              "      <td>13.680000</td>\n",
              "      <td>1.576740e-09</td>\n",
              "      <td>1.665220e-10</td>\n",
              "      <td>-1.591710e-10</td>\n",
              "      <td>3.672690e-12</td>\n",
              "      <td>13.956000</td>\n",
              "      <td>2.878620e-10</td>\n",
              "      <td>1.822500e-12</td>\n",
              "      <td>8.611000</td>\n",
              "      <td>6.177760e-11</td>\n",
              "      <td>7.242330e-13</td>\n",
              "      <td>5.219</td>\n",
              "      <td>310.308960</td>\n",
              "      <td>13.392247</td>\n",
              "      <td>9.469290e-08</td>\n",
              "      <td>1.144430e-08</td>\n",
              "      <td>55395.359375</td>\n",
              "      <td>2630000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.280541</td>\n",
              "      <td>-0.636407</td>\n",
              "      <td>-0.691235</td>\n",
              "      <td>0.054828</td>\n",
              "      <td>0.355866</td>\n",
              "      <td>2.854124</td>\n",
              "      <td>14.384529</td>\n",
              "      <td>0.179071</td>\n",
              "      <td>0.304796</td>\n",
              "      <td>1.157896</td>\n",
              "      <td>-0.746974</td>\n",
              "      <td>-8.700484</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>121</td>\n",
              "      <td>32.697201</td>\n",
              "      <td>-51.028198</td>\n",
              "      <td>4.740330e-09</td>\n",
              "      <td>2.16971</td>\n",
              "      <td>0.03282</td>\n",
              "      <td>62.947647</td>\n",
              "      <td>276.111694</td>\n",
              "      <td>-61.767799</td>\n",
              "      <td>385.059998</td>\n",
              "      <td>5.930000e-11</td>\n",
              "      <td>1.618200e-12</td>\n",
              "      <td>5.107200e-11</td>\n",
              "      <td>1.523330e-12</td>\n",
              "      <td>4.746</td>\n",
              "      <td>2</td>\n",
              "      <td>2.3031</td>\n",
              "      <td>6.790450e-08</td>\n",
              "      <td>3.436410e-09</td>\n",
              "      <td>-3.436410e-09</td>\n",
              "      <td>1.627360e-11</td>\n",
              "      <td>22.070999</td>\n",
              "      <td>1.822940e-08</td>\n",
              "      <td>6.762120e-10</td>\n",
              "      <td>-6.762120e-10</td>\n",
              "      <td>1.217990e-11</td>\n",
              "      <td>38.094002</td>\n",
              "      <td>4.268660e-09</td>\n",
              "      <td>2.105200e-10</td>\n",
              "      <td>-2.105200e-10</td>\n",
              "      <td>9.830760e-12</td>\n",
              "      <td>38.015999</td>\n",
              "      <td>7.063220e-10</td>\n",
              "      <td>4.505460e-12</td>\n",
              "      <td>20.693001</td>\n",
              "      <td>9.707360e-11</td>\n",
              "      <td>1.242470e-12</td>\n",
              "      <td>8.745</td>\n",
              "      <td>1444.283813</td>\n",
              "      <td>27.729914</td>\n",
              "      <td>4.094160e-07</td>\n",
              "      <td>2.493990e-08</td>\n",
              "      <td>54756.121094</td>\n",
              "      <td>2630000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.576720</td>\n",
              "      <td>-0.620531</td>\n",
              "      <td>-0.716050</td>\n",
              "      <td>0.095520</td>\n",
              "      <td>0.043811</td>\n",
              "      <td>2.585528</td>\n",
              "      <td>22.944206</td>\n",
              "      <td>0.075396</td>\n",
              "      <td>0.336402</td>\n",
              "      <td>1.360673</td>\n",
              "      <td>-1.122652</td>\n",
              "      <td>-8.324191</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0          ra        dec  flux_1_100_gev  spectral_index  \\\n",
              "0         117  170.807907 -64.260902    1.268470e-09         2.24084   \n",
              "1         118  202.240402 -56.130901    6.763560e-09         2.21938   \n",
              "2         119  159.735199 -53.186100    1.939800e-09         2.31971   \n",
              "3         120  276.320709 -52.509300    1.993040e-09         2.01742   \n",
              "4         121   32.697201 -51.028198    4.740330e-09         2.16971   \n",
              "\n",
              "   spectral_index_error  detection_significance         lii        bii  \\\n",
              "0               0.10782               14.025361  293.535187  -3.010400   \n",
              "1               0.03290               46.079670  308.176086   6.358300   \n",
              "2               0.07696               24.437548  283.751587   4.679100   \n",
              "3               0.08697               21.572411  342.269989 -17.464701   \n",
              "4               0.03282               62.947647  276.111694 -61.767799   \n",
              "\n",
              "   pivot_energy  flux_density  flux_density_error   energy_flux  \\\n",
              "0    339.359985  6.514200e-11        7.074700e-12  2.629280e-11   \n",
              "1    582.570007  3.501100e-11        1.167800e-12  7.046150e-11   \n",
              "2    327.359985  7.086100e-11        3.901800e-12  3.157340e-11   \n",
              "3    714.700012  5.416700e-12        3.985900e-13  1.649350e-11   \n",
              "4    385.059998  5.930000e-11        1.618200e-12  5.107200e-11   \n",
              "\n",
              "   energy_flux_error  curve_significance  spectrum_type  powerlaw_index  \\\n",
              "0       2.118950e-12               4.524              2          2.6114   \n",
              "1       2.359720e-12               5.998              2          2.3404   \n",
              "2       1.598800e-12               5.330              2          2.6188   \n",
              "3       1.321550e-12               3.863              2          2.2269   \n",
              "4       1.523330e-12               4.746              2          2.3031   \n",
              "\n",
              "   flux_100_300_mev  flux_100_300_mev_pos_err  flux_100_300_mev_neg_err  \\\n",
              "0      5.874740e-08              6.184800e-09             -6.154550e-09   \n",
              "1      9.235340e-08              8.233390e-09             -8.233390e-09   \n",
              "2      5.444920e-08              4.922340e-09             -4.922340e-09   \n",
              "3      1.263060e-08              3.067500e-09             -3.020100e-09   \n",
              "4      6.790450e-08              3.436410e-09             -3.436410e-09   \n",
              "\n",
              "   nufnu_100_300_mev  sqrt_ts_100_300_mev  flux_0p3_1_gev  \\\n",
              "0       1.469530e-11             9.574000    1.306430e-08   \n",
              "1       2.237260e-11            11.952000    2.821480e-08   \n",
              "2       1.305460e-11            11.671000    1.373480e-08   \n",
              "3       3.179230e-12             4.266000    7.096370e-09   \n",
              "4       1.627360e-11            22.070999    1.822940e-08   \n",
              "\n",
              "   flux_0p3_1_gev_pos_err  flux_0p3_1_gev_neg_err  nufnu_0p3_1_gev  \\\n",
              "0            1.428180e-09           -1.413830e-09     8.282390e-12   \n",
              "1            1.211760e-09           -1.211760e-09     1.891400e-11   \n",
              "2            9.768080e-10           -9.768080e-10     8.842370e-12   \n",
              "3            6.141310e-10           -6.141310e-10     4.914550e-12   \n",
              "4            6.762120e-10           -6.762120e-10     1.217990e-11   \n",
              "\n",
              "   sqrt_ts_0p3_1_gev  flux_1_3_gev  flux_1_3_gev_pos_err  \\\n",
              "0           9.663000  1.216690e-09          2.746420e-10   \n",
              "1          27.768000  5.441840e-09          2.938120e-10   \n",
              "2          15.994000  1.892330e-09          2.139180e-10   \n",
              "3          13.680000  1.576740e-09          1.665220e-10   \n",
              "4          38.094002  4.268660e-09          2.105200e-10   \n",
              "\n",
              "   flux_1_3_gev_neg_err  nufnu_1_3_gev  sqrt_ts_1_3_gev  flux_3_10_gev  \\\n",
              "0         -2.659180e-10   2.492240e-12         4.895000   9.437780e-11   \n",
              "1         -2.938120e-10   1.247090e-11        26.823999   9.965950e-10   \n",
              "2         -2.063610e-10   4.104160e-12        11.592000   1.549630e-10   \n",
              "3         -1.591710e-10   3.672690e-12        13.956000   2.878620e-10   \n",
              "4         -2.105200e-10   9.830760e-12        38.015999   7.063220e-10   \n",
              "\n",
              "   nufnu_3_10_gev  sqrt_ts_3_10_gev  flux_10_100_gev  nufnu_10_100_gev  \\\n",
              "0    4.955250e-13          1.854000     2.818170e-11      2.611100e-13   \n",
              "1    6.266210e-12         18.434000     1.578590e-10      1.893100e-12   \n",
              "2    8.952460e-13          4.716000     1.032650e-14      1.008050e-16   \n",
              "3    1.822500e-12          8.611000     6.177760e-11      7.242330e-13   \n",
              "4    4.505460e-12         20.693001     9.707360e-11      1.242470e-12   \n",
              "\n",
              "   sqrt_ts_10_100_gev  variability_index  significance_peak     flux_peak  \\\n",
              "0               2.527         210.782593           9.025927  3.128840e-07   \n",
              "1               9.420         788.263062          16.583115  3.443660e-07   \n",
              "2               0.000         914.668640          22.781076  5.224050e-07   \n",
              "3               5.219         310.308960          13.392247  9.469290e-08   \n",
              "4               8.745        1444.283813          27.729914  4.094160e-07   \n",
              "\n",
              "   flux_peak_error     time_peak  time_peak_interval  source_type  \\\n",
              "0     3.964350e-08  54725.683594             2630000          0.0   \n",
              "1     3.004300e-08  56004.156250             2630000          0.0   \n",
              "2     3.202220e-08  56004.156250             2630000          0.0   \n",
              "3     1.144430e-08  55395.359375             2630000          0.0   \n",
              "4     2.493990e-08  54756.121094             2630000          0.0   \n",
              "\n",
              "   analysis_flags      HR12      HR23      HR34  hard_slope  soft_slope  \\\n",
              "0               0 -0.636151 -0.829607 -0.856029    0.026422    0.193456   \n",
              "1               0 -0.531969 -0.676626 -0.690423    0.013797    0.144657   \n",
              "2               0 -0.597125 -0.757815 -0.848617    0.090802    0.160689   \n",
              "3               0 -0.280541 -0.636407 -0.691235    0.054828    0.355866   \n",
              "4               0 -0.576720 -0.620531 -0.716050    0.095520    0.043811   \n",
              "\n",
              "     P_E_lg        TS_      sig_  gamm_log    Ts_log   sig_log  F100_log  \n",
              "0  2.530661  15.028675  0.322559  0.350411  1.176921 -0.491391 -8.896720  \n",
              "1  2.765348  17.106525  0.130166  0.346232  1.233162 -0.885503 -8.169825  \n",
              "2  2.515026  37.428822  0.218107  0.365434  1.573206 -0.661330 -8.712243  \n",
              "3  2.854124  14.384529  0.179071  0.304796  1.157896 -0.746974 -8.700484  \n",
              "4  2.585528  22.944206  0.075396  0.336402  1.360673 -1.122652 -8.324191  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otOujTgwT3l7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# COMPLETAMOS LA BASE DE DATOS CON LOS NUEVOS OBJETOS CLASIFICADOS.\n",
        "indices=df_UNK['Unnamed: 0'].tolist()\n",
        "cols_=['nombre','source_type','ra','dec','lii','bii']\n",
        "Lista_=pd.DataFrame(columns=cols_)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThIby6Tx35nc",
        "colab_type": "code",
        "outputId": "18cd9070-acd4-4b31-ee6d-b16f081893ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "#CREAMOS DATAFRAME CON LOS OBJETOS CLASIFICADOS Y SUS NOMBRES\n",
        "def salva_AGNs (archivo,df_UNK,df0):\n",
        "    archivo='/content/drive/My Drive/'+archivo\n",
        "    indices=df_UNK['Unnamed: 0'].tolist()\n",
        "\n",
        "    import csv\n",
        "    with open(archivo, \"a\", newline='') as fp:\n",
        "      print('Salvando archivo :', archivo)\n",
        "      cols_=['nombre','source_type','ra','dec','lii','bii']\n",
        "      wr = csv.writer(fp, dialect='excel')\n",
        "      wr.writerow(cols_)\n",
        "\n",
        "      for ind_ in indices:\n",
        "        st_=df_UNK.loc[(df_UNK['Unnamed: 0'] == ind_\t)  , 'source_type']\n",
        "        st_=st_.iloc[0]\n",
        "        name_=df0.iloc[ind_]['name']\n",
        "        ra_=df0.iloc[ind_]['ra']\n",
        "        dec_=df0.iloc[ind_]['dec']\n",
        "        lii_=df0.iloc[ind_]['lii']\n",
        "        bii_=df0.iloc[ind_]['bii']\n",
        "        #row_= [name_, st_, ra_, dec_]\n",
        "        list1 = [name_,st_,ra_,dec_,lii_,bii_]\n",
        "        wr = csv.writer(fp, dialect='excel')\n",
        "        wr.writerow(list1)\n",
        "    \n",
        "\n",
        "salva_AGNs ('nn2_output.csv',df_UNK,df0)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Salvando archivo : /content/drive/My Drive/nn2_output.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRZo5ttidhCJ",
        "colab_type": "code",
        "outputId": "325594ec-2976-4294-bdfd-c7265540d9f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "\n",
        "\n",
        "df = pd.read_csv('/content/drive/My Drive/nn2_output.csv')\n",
        "df.head(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>nombre</th>\n",
              "      <th>source_type</th>\n",
              "      <th>ra</th>\n",
              "      <th>dec</th>\n",
              "      <th>lii</th>\n",
              "      <th>bii</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3FGL J1123.2-6415</td>\n",
              "      <td>0.0</td>\n",
              "      <td>170.8079</td>\n",
              "      <td>-64.2609</td>\n",
              "      <td>293.5352</td>\n",
              "      <td>-3.0104</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3FGL J1328.9-5607</td>\n",
              "      <td>0.0</td>\n",
              "      <td>202.2404</td>\n",
              "      <td>-56.1309</td>\n",
              "      <td>308.1761</td>\n",
              "      <td>6.3583</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3FGL J1038.9-5311</td>\n",
              "      <td>0.0</td>\n",
              "      <td>159.7352</td>\n",
              "      <td>-53.1861</td>\n",
              "      <td>283.7516</td>\n",
              "      <td>4.6791</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3FGL J1825.2-5230</td>\n",
              "      <td>0.0</td>\n",
              "      <td>276.3207</td>\n",
              "      <td>-52.5093</td>\n",
              "      <td>342.2700</td>\n",
              "      <td>-17.4647</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3FGL J0210.7-5101</td>\n",
              "      <td>0.0</td>\n",
              "      <td>32.6972</td>\n",
              "      <td>-51.0282</td>\n",
              "      <td>276.1117</td>\n",
              "      <td>-61.7678</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              nombre  source_type        ra      dec       lii      bii\n",
              "0  3FGL J1123.2-6415          0.0  170.8079 -64.2609  293.5352  -3.0104\n",
              "1  3FGL J1328.9-5607          0.0  202.2404 -56.1309  308.1761   6.3583\n",
              "2  3FGL J1038.9-5311          0.0  159.7352 -53.1861  283.7516   4.6791\n",
              "3  3FGL J1825.2-5230          0.0  276.3207 -52.5093  342.2700 -17.4647\n",
              "4  3FGL J0210.7-5101          0.0   32.6972 -51.0282  276.1117 -61.7678"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkTWoSVAZz6u",
        "colab_type": "code",
        "outputId": "389f8bfe-1f77-4bdc-fb51-123dfcf94bea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        }
      },
      "source": [
        "# RECUENTO DE CLASIFICACION\n",
        "print ('Bl Lac = 1 -- FSRQ = 0 -- UNKNOWN = 2 ')\n",
        "df['source_type'].apply(pd.Series).stack().value_counts()\n",
        "#Ypredict"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Bl Lac = 1 -- FSRQ = 0 -- UNKNOWN = 2 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0            1025\n",
              "0.0             694\n",
              "source_type       2\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    }
  ]
}